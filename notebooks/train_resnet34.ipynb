{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Training Nutrition5K Calorie Predictor - ResNet-34\n",
        "\n",
        "This notebook allows you to quickly train and experiment with different model architectures.\n",
        "\n",
        "**Quick Start:** Just run all cells to train ResNet-34 with best config!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "import os\n",
        "\n",
        "# Add src to path\n",
        "sys.path.insert(0, '../src')\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm.notebook import tqdm\n",
        "import json\n",
        "from datetime import datetime\n",
        "\n",
        "# Import our modules\n",
        "from dataset import Nutrition5KDataset, create_train_val_split\n",
        "from model import build_model, list_available_components\n",
        "from train import MultiTaskLoss, EarlyStopping, Trainer\n",
        "\n",
        "print(\"✓ Imports successful!\")\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# MODEL ARCHITECTURE - Change these to experiment!\n",
        "# ============================================================================\n",
        "ENCODER = 'resnet34'          # Options: 'resnet18', 'resnet34', 'resnet50'\n",
        "FUSION = 'middle'             # Options: 'middle', 'middle_attention', 'additive'\n",
        "REGRESSION_HEAD = 'standard'  # Options: 'standard', 'deep'\n",
        "SEGMENTATION_HEAD = 'standard' # Options: 'standard', 'light'\n",
        "\n",
        "# ============================================================================\n",
        "# HYPERPARAMETERS - Best config from grid search\n",
        "# ============================================================================\n",
        "LEARNING_RATE = 0.0005\n",
        "DROPOUT = 0.4\n",
        "WEIGHT_DECAY = 0.0001\n",
        "BATCH_SIZE = 64\n",
        "CALORIE_WEIGHT = 1.0\n",
        "SEG_WEIGHT = 0.5\n",
        "FUSION_CHANNELS = 512\n",
        "\n",
        "# ============================================================================\n",
        "# TRAINING SETTINGS\n",
        "# ============================================================================\n",
        "NUM_EPOCHS = 50\n",
        "EARLY_STOPPING_PATIENCE = 15\n",
        "VAL_RATIO = 0.15\n",
        "NUM_WORKERS = 4\n",
        "IMG_SIZE = 224\n",
        "\n",
        "# ============================================================================\n",
        "# DATA PATHS\n",
        "# ============================================================================\n",
        "DATA_ROOT = '../Nutrition5K/Nutrition5K/train'\n",
        "CSV_PATH = '../Nutrition5K/Nutrition5K/nutrition5k_train.csv'\n",
        "\n",
        "# ============================================================================\n",
        "# OUTPUT\n",
        "# ============================================================================\n",
        "EXPERIMENT_NAME = f\"{ENCODER}_{FUSION}_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
        "OUTPUT_DIR = f'../outputs/notebook_{EXPERIMENT_NAME}'\n",
        "\n",
        "# Device\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "print(\"Configuration:\")\n",
        "print(f\"  Architecture: {ENCODER} + {FUSION} fusion\")\n",
        "print(f\"  Heads: {REGRESSION_HEAD} (reg), {SEGMENTATION_HEAD} (seg)\")\n",
        "print(f\"  Learning rate: {LEARNING_RATE}\")\n",
        "print(f\"  Batch size: {BATCH_SIZE}\")\n",
        "print(f\"  Device: {DEVICE}\")\n",
        "print(f\"  Output: {OUTPUT_DIR}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Show Available Components\n",
        "\n",
        "See what architectures you can try!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "list_available_components()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Load Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Creating train/val split...\")\n",
        "train_csv, val_csv = create_train_val_split(\n",
        "    CSV_PATH,\n",
        "    val_ratio=VAL_RATIO,\n",
        "    random_seed=42\n",
        ")\n",
        "\n",
        "print(\"\\nLoading datasets...\")\n",
        "train_dataset = Nutrition5KDataset(\n",
        "    csv_path=train_csv,\n",
        "    data_root=DATA_ROOT,\n",
        "    split='train',\n",
        "    augment=True,\n",
        "    img_size=IMG_SIZE\n",
        ")\n",
        "\n",
        "val_dataset = Nutrition5KDataset(\n",
        "    csv_path=val_csv,\n",
        "    data_root=DATA_ROOT,\n",
        "    split='val',\n",
        "    augment=False,\n",
        "    img_size=IMG_SIZE\n",
        ")\n",
        "\n",
        "print(f\"\\n✓ Training samples: {len(train_dataset)}\")\n",
        "print(f\"✓ Validation samples: {len(val_dataset)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Visualize a Sample\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load a sample\n",
        "sample = train_dataset[0]\n",
        "\n",
        "# Denormalize RGB for visualization\n",
        "rgb = sample['rgb'].numpy()\n",
        "mean = np.array([0.485, 0.456, 0.406]).reshape(3, 1, 1)\n",
        "std = np.array([0.229, 0.224, 0.225]).reshape(3, 1, 1)\n",
        "rgb = rgb * std + mean\n",
        "rgb = np.clip(rgb, 0, 1)\n",
        "rgb = np.transpose(rgb, (1, 2, 0))\n",
        "\n",
        "# Plot\n",
        "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
        "\n",
        "axes[0].imshow(rgb)\n",
        "axes[0].set_title('RGB Image')\n",
        "axes[0].axis('off')\n",
        "\n",
        "axes[1].imshow(sample['depth'].numpy()[0], cmap='viridis')\n",
        "axes[1].set_title('Depth Image')\n",
        "axes[1].axis('off')\n",
        "\n",
        "axes[2].imshow(sample['mask'].numpy()[0], cmap='gray')\n",
        "axes[2].set_title('Segmentation Mask')\n",
        "axes[2].axis('off')\n",
        "\n",
        "plt.suptitle(f\"Dish: {sample['dish_id']} | Calories: {sample['calorie'].item():.0f} kcal\", fontsize=14)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Create Data Loaders\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=True,\n",
        "    num_workers=NUM_WORKERS,\n",
        "    pin_memory=True if torch.cuda.is_available() else False,\n",
        "    drop_last=True\n",
        ")\n",
        "\n",
        "val_loader = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=False,\n",
        "    num_workers=NUM_WORKERS,\n",
        "    pin_memory=True if torch.cuda.is_available() else False\n",
        ")\n",
        "\n",
        "print(f\"✓ Train batches: {len(train_loader)}\")\n",
        "print(f\"✓ Val batches: {len(val_loader)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Build Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(f\"Building {ENCODER} model...\")\n",
        "\n",
        "model = build_model(\n",
        "    encoder=ENCODER,\n",
        "    fusion=FUSION,\n",
        "    regression_head=REGRESSION_HEAD,\n",
        "    segmentation_head=SEGMENTATION_HEAD,\n",
        "    pretrained=False,\n",
        "    dropout_rate=DROPOUT,\n",
        "    fusion_channels=FUSION_CHANNELS\n",
        ")\n",
        "\n",
        "model = model.to(DEVICE)\n",
        "\n",
        "print(f\"\\n✓ Model created!\")\n",
        "print(f\"  Total parameters: {model.get_num_parameters():,}\")\n",
        "print(f\"  Configuration: {model.get_config()}\")\n",
        "\n",
        "# Test forward pass\n",
        "rgb_test = torch.randn(2, 3, IMG_SIZE, IMG_SIZE).to(DEVICE)\n",
        "depth_test = torch.randn(2, 1, IMG_SIZE, IMG_SIZE).to(DEVICE)\n",
        "cal_pred, seg_pred = model(rgb_test, depth_test)\n",
        "print(f\"\\n✓ Forward pass test successful!\")\n",
        "print(f\"  Calorie output: {cal_pred.shape}\")\n",
        "print(f\"  Segmentation output: {seg_pred.shape}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Setup Training Components\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Loss function\n",
        "criterion = MultiTaskLoss(\n",
        "    calorie_weight=CALORIE_WEIGHT,\n",
        "    seg_weight=SEG_WEIGHT\n",
        ")\n",
        "\n",
        "# Optimizer\n",
        "optimizer = optim.AdamW(\n",
        "    model.parameters(),\n",
        "    lr=LEARNING_RATE,\n",
        "    weight_decay=WEIGHT_DECAY\n",
        ")\n",
        "\n",
        "# Learning rate scheduler\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
        "    optimizer,\n",
        "    mode='min',\n",
        "    factor=0.5,\n",
        "    patience=5\n",
        ")\n",
        "\n",
        "# Create output directory\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "# Save configuration\n",
        "config = {\n",
        "    'encoder': ENCODER,\n",
        "    'fusion': FUSION,\n",
        "    'regression_head': REGRESSION_HEAD,\n",
        "    'segmentation_head': SEGMENTATION_HEAD,\n",
        "    'learning_rate': LEARNING_RATE,\n",
        "    'dropout': DROPOUT,\n",
        "    'weight_decay': WEIGHT_DECAY,\n",
        "    'batch_size': BATCH_SIZE,\n",
        "    'calorie_weight': CALORIE_WEIGHT,\n",
        "    'seg_weight': SEG_WEIGHT,\n",
        "    'num_epochs': NUM_EPOCHS,\n",
        "    'early_stopping_patience': EARLY_STOPPING_PATIENCE,\n",
        "}\n",
        "\n",
        "with open(os.path.join(OUTPUT_DIR, 'config.json'), 'w') as f:\n",
        "    json.dump(config, f, indent=4)\n",
        "\n",
        "print(\"✓ Training components ready!\")\n",
        "print(f\"  Loss: Multi-task (calorie={CALORIE_WEIGHT}, seg={SEG_WEIGHT})\")\n",
        "print(f\"  Optimizer: AdamW (lr={LEARNING_RATE}, wd={WEIGHT_DECAY})\")\n",
        "print(f\"  Scheduler: ReduceLROnPlateau (patience=5)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Train the Model\n",
        "\n",
        "**This will take some time!** Progress bar will show training status.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    train_loader=train_loader,\n",
        "    val_loader=val_loader,\n",
        "    criterion=criterion,\n",
        "    optimizer=optimizer,\n",
        "    scheduler=scheduler,\n",
        "    device=DEVICE,\n",
        "    output_dir=OUTPUT_DIR,\n",
        "    early_stopping_patience=EARLY_STOPPING_PATIENCE\n",
        ")\n",
        "\n",
        "# Start training\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"STARTING TRAINING\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "trainer.train(NUM_EPOCHS)\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"TRAINING COMPLETE!\")\n",
        "print(\"=\"*80)\n",
        "print(f\"Best validation loss: {trainer.best_val_loss:.4f}\")\n",
        "print(f\"Model saved to: {OUTPUT_DIR}/best_model.pth\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Training Summary\n",
        "\n",
        "View TensorBoard for detailed curves:\n",
        "```bash\n",
        "tensorboard --logdir=../outputs/notebook_*\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\nTraining Summary:\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Architecture: {ENCODER} + {FUSION}\")\n",
        "print(f\"Total Parameters: {model.get_num_parameters():,}\")\n",
        "print(f\"Best Validation Loss: {trainer.best_val_loss:.4f}\")\n",
        "print(f\"Training Samples: {len(train_dataset)}\")\n",
        "print(f\"Validation Samples: {len(val_dataset)}\")\n",
        "print(f\"Output Directory: {OUTPUT_DIR}\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Show best model path\n",
        "best_model_path = os.path.join(OUTPUT_DIR, 'best_model.pth')\n",
        "if os.path.exists(best_model_path):\n",
        "    print(f\"\\n✓ Best model saved at: {best_model_path}\")\n",
        "    print(f\"  File size: {os.path.getsize(best_model_path) / 1e6:.1f} MB\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Next Steps\n",
        "\n",
        "1. **Try different architectures**: Change ENCODER, FUSION, etc. in cell 4 and rerun from there\n",
        "2. **View TensorBoard**: Run `tensorboard --logdir=../outputs/notebook_*`\n",
        "3. **Generate test predictions**: Use `test_inference.py` with your best model\n",
        "4. **Compare models**: Use `compare_architectures.py`\n",
        "\n",
        "### Quick Experiments:\n",
        "```python\n",
        "# Try ResNet-50\n",
        "ENCODER = 'resnet50'\n",
        "\n",
        "# Try attention fusion\n",
        "FUSION = 'middle_attention'\n",
        "\n",
        "# Try deep head\n",
        "REGRESSION_HEAD = 'deep'\n",
        "```\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
