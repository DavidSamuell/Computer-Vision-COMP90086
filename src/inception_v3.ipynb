{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "98f4f0d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import torchvision.transforms as T\n",
    "import torchvision.transforms.functional as TF\n",
    "from torchvision import models\n",
    "import random\n",
    "from typing import Tuple, Optional\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(789)\n",
    "np.random.seed(789)\n",
    "random.seed(789)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(789)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e403255",
   "metadata": {},
   "source": [
    "### Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6c1362f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset Implementation\n",
    "class Nutrition5KDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset class for Nutrition5K with multi-modal inputs (RGB + Depth)\n",
    "    for calorie prediction only (no segmentation)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        csv_path: str,\n",
    "        data_root: str,\n",
    "        split: str = 'train',\n",
    "        augment: bool = True,\n",
    "        img_size: int = 224,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_path: Path to the CSV file with dish IDs and calorie values\n",
    "            data_root: Root directory containing color/, depth_raw/ subdirectories\n",
    "            split: 'train' or 'val'\n",
    "            augment: Whether to apply data augmentation\n",
    "            img_size: Target image size for resizing\n",
    "            use_segmentation: Not used (kept for compatibility)\n",
    "        \"\"\"\n",
    "        self.data_root = data_root\n",
    "        self.split = split\n",
    "        self.augment = augment\n",
    "        self.img_size = img_size\n",
    "        \n",
    "        # Load CSV\n",
    "        self.df = pd.read_csv(csv_path)\n",
    "        # Rename 'Value' column to 'calories' if it exists\n",
    "        if 'Value' in self.df.columns and 'calories' not in self.df.columns:\n",
    "            self.df = self.df.rename(columns={'Value': 'calories'})\n",
    "        # Make sure calories column exists\n",
    "        if 'calories' not in self.df.columns:\n",
    "            raise ValueError(\"CSV file must contain a 'calories' column or a 'Value' column that can be renamed\")\n",
    "        # Filter out high-calorie samples\n",
    "        self.df = self.df[self.df['calories'] < 3000].reset_index(drop=True)\n",
    "                \n",
    "        # Build paths\n",
    "        self.color_dir = os.path.join(data_root, 'color')\n",
    "        self.depth_raw_dir = os.path.join(data_root, 'depth_raw')\n",
    "        \n",
    "        # Validate dataset\n",
    "        self.valid_indices = self._validate_dataset()\n",
    "        print(f\"Loaded {len(self.valid_indices)} valid samples out of {len(self.df)}\")\n",
    "        \n",
    "        # Color normalization (ImageNet stats as baseline)\n",
    "        self.color_normalize = T.Normalize(\n",
    "            mean=[0.485, 0.456, 0.406],\n",
    "            std=[0.229, 0.224, 0.225]\n",
    "        )\n",
    "        \n",
    "    def _validate_dataset(self):\n",
    "        \"\"\"Pre-validate all samples and return valid indices\"\"\"\n",
    "        valid_indices = []\n",
    "        \n",
    "        for idx in range(len(self.df)):\n",
    "            dish_id = self.df.iloc[idx]['ID']\n",
    "            \n",
    "            rgb_path = os.path.join(self.color_dir, dish_id, 'rgb.png')\n",
    "            depth_path = os.path.join(self.depth_raw_dir, dish_id, 'depth_raw.png')\n",
    "            \n",
    "            # Check if files exist\n",
    "            if not os.path.exists(rgb_path):\n",
    "                continue\n",
    "            if not os.path.exists(depth_path):\n",
    "                continue\n",
    "            \n",
    "            # Try to load images to check for corruption\n",
    "            try:\n",
    "                with Image.open(rgb_path) as img:\n",
    "                    img.verify()\n",
    "                with Image.open(depth_path) as img:\n",
    "                    img.verify()\n",
    "                valid_indices.append(idx)\n",
    "            except Exception as e:\n",
    "                continue\n",
    "                \n",
    "        return valid_indices\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.valid_indices)\n",
    "    \n",
    "    def _load_image_safe(self, path: str, mode: str = 'RGB') -> Optional[Image.Image]:\n",
    "        \"\"\"Safely load an image with error handling\"\"\"\n",
    "        try:\n",
    "            with Image.open(path) as img:\n",
    "                return img.convert(mode).copy()\n",
    "        except Exception as e:\n",
    "            return None\n",
    "    \n",
    "    def _apply_augmentation(self, rgb_img, depth_img):\n",
    "        \"\"\"Apply geometric augmentation only (no color changes)\"\"\"\n",
    "        if not self.augment:\n",
    "            return rgb_img, depth_img\n",
    "        \n",
    "        # Convert to tensors first\n",
    "        rgb_tensor = TF.to_tensor(rgb_img)\n",
    "        depth_tensor = TF.to_tensor(depth_img)\n",
    "        \n",
    "        # Random horizontal flip\n",
    "        if random.random() > 0.5:\n",
    "            rgb_tensor = TF.hflip(rgb_tensor)\n",
    "            depth_tensor = TF.hflip(depth_tensor)\n",
    "        \n",
    "        # Random rotation (±15 degrees)\n",
    "        if random.random() > 0.5:\n",
    "            angle = random.uniform(-15, 15)\n",
    "            rgb_tensor = TF.rotate(rgb_tensor, angle)\n",
    "            depth_tensor = TF.rotate(depth_tensor, angle)\n",
    "        \n",
    "        # Random resized crop\n",
    "        if random.random() > 0.4:  # 60% probability\n",
    "            i, j, h, w = T.RandomResizedCrop.get_params(\n",
    "                rgb_tensor, scale=(0.75, 1.0), ratio=(0.9, 1.1)\n",
    "            )\n",
    "            rgb_tensor = TF.resized_crop(rgb_tensor, i, j, h, w, (self.img_size, self.img_size))\n",
    "            depth_tensor = TF.resized_crop(depth_tensor, i, j, h, w, (self.img_size, self.img_size))\n",
    "        \n",
    "        # Convert back to PIL\n",
    "        rgb_img = TF.to_pil_image(rgb_tensor)\n",
    "        depth_img = TF.to_pil_image(depth_tensor)\n",
    "        \n",
    "        return rgb_img, depth_img\n",
    "    \n",
    "    def _resize_and_center_crop(self, img, target_size: int = 256):\n",
    "        \"\"\"\n",
    "        Resize and center crop image to target_size x target_size\n",
    "        Matches the preprocessing in the Nutrition5k paper\n",
    "        \n",
    "        Args:\n",
    "            img: PIL Image\n",
    "            target_size: Target size (default 256x256 as per paper)\n",
    "        \n",
    "        Returns:\n",
    "            Cropped PIL Image\n",
    "        \"\"\"\n",
    "        # Get original dimensions\n",
    "        width, height = img.size\n",
    "        \n",
    "        # Resize so the shorter side is target_size\n",
    "        if width < height:\n",
    "            new_width = target_size\n",
    "            new_height = int(target_size * height / width)\n",
    "        else:\n",
    "            new_height = target_size\n",
    "            new_width = int(target_size * width / height)\n",
    "        \n",
    "        img = img.resize((new_width, new_height), Image.LANCZOS)\n",
    "        \n",
    "        # Center crop to target_size x target_size\n",
    "        left = (new_width - target_size) // 2\n",
    "        top = (new_height - target_size) // 2\n",
    "        right = left + target_size\n",
    "        bottom = top + target_size\n",
    "        \n",
    "        img = img.crop((left, top, right, bottom))\n",
    "        \n",
    "        return img\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"Get a single sample\"\"\"\n",
    "        actual_idx = self.valid_indices[idx]\n",
    "        row = self.df.iloc[actual_idx]\n",
    "        \n",
    "        dish_id = row['ID']\n",
    "        calorie = float(row['calories'])\n",
    "        \n",
    "        # Load images\n",
    "        rgb_path = os.path.join(self.color_dir, dish_id, 'rgb.png')\n",
    "        depth_path = os.path.join(self.depth_raw_dir, dish_id, 'depth_raw.png')\n",
    "        \n",
    "        rgb_img = self._load_image_safe(rgb_path, 'RGB')\n",
    "        depth_img = self._load_image_safe(depth_path, 'L')  # Grayscale for depth\n",
    "        \n",
    "        if rgb_img is None or depth_img is None:\n",
    "            # Fallback: return a black image\n",
    "            rgb_img = Image.new('RGB', (self.img_size, self.img_size), (0, 0, 0))\n",
    "            depth_img = Image.new('L', (self.img_size, self.img_size), 0)\n",
    "        \n",
    "        # Apply augmentation\n",
    "        rgb_img, depth_img = self._apply_augmentation(rgb_img, depth_img)\n",
    "        \n",
    "        # Resize and center crop to match paper preprocessing (256x256)\n",
    "        rgb_img = self._resize_and_center_crop(rgb_img, target_size=self.img_size)\n",
    "        depth_img = self._resize_and_center_crop(depth_img, target_size=self.img_size)\n",
    "        \n",
    "        # Convert to tensors\n",
    "        rgb_tensor = TF.to_tensor(rgb_img)  # (3, H, W)\n",
    "        depth_tensor = TF.to_tensor(depth_img)  # (1, H, W)\n",
    "        \n",
    "        # Normalize RGB\n",
    "        rgb_tensor = self.color_normalize(rgb_tensor)\n",
    "        \n",
    "        # Normalize depth (0-1 range, assuming depth is already in reasonable range)\n",
    "        depth_tensor = depth_tensor / 255.0\n",
    "        \n",
    "        return {\n",
    "            'dish_id': dish_id,\n",
    "            'rgb': rgb_tensor,\n",
    "            'depth': depth_tensor,\n",
    "            'calorie': torch.tensor(calorie, dtype=torch.float32)\n",
    "        }\n",
    "\n",
    "\n",
    "def create_train_val_split(csv_path: str, val_ratio: float = 0.15, random_seed: int = 42):\n",
    "    \"\"\"\n",
    "    Create train/validation split CSV files\n",
    "    \"\"\"\n",
    "    # Read original CSV\n",
    "    df = pd.read_csv(csv_path)    \n",
    "    \n",
    "    # Shuffle with fixed seed\n",
    "    df_shuffled = df.sample(frac=1, random_state=random_seed).reset_index(drop=True)\n",
    "    \n",
    "    # Split\n",
    "    val_size = int(len(df_shuffled) * val_ratio)\n",
    "    train_df = df_shuffled[val_size:]\n",
    "    val_df = df_shuffled[:val_size]\n",
    "    \n",
    "    # Save temporary CSV files\n",
    "    base_dir = os.path.dirname(csv_path)\n",
    "    train_csv = os.path.join(base_dir, 'train_split.csv')\n",
    "    val_csv = os.path.join(base_dir, 'val_split.csv')\n",
    "    \n",
    "    train_df.to_csv(train_csv, index=False)\n",
    "    val_df.to_csv(val_csv, index=False)\n",
    "    \n",
    "    return train_csv, val_csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa446bcd",
   "metadata": {},
   "source": [
    "### Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fcf2b5ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Utilities - Simplified for Calorie Prediction Only\n",
    "import math\n",
    "def get_warmup_cosine_scheduler(optimizer, warmup_steps, total_steps, min_lr_ratio=0.0):\n",
    "    def lr_lambda(current_step):\n",
    "        if current_step < warmup_steps:\n",
    "            return float(current_step) / float(max(1, warmup_steps))\n",
    "        else:\n",
    "            progress = float(current_step - warmup_steps) / float(max(1, total_steps - warmup_steps))\n",
    "            # Scale from min_lr_ratio to 1.0 instead of 0.0 to 1.0\n",
    "            return min_lr_ratio + (1.0 - min_lr_ratio) * 0.5 * (1.0 + math.cos(math.pi * progress))\n",
    "    \n",
    "    return optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n",
    "\n",
    "\n",
    "class EarlyStopping:\n",
    "    \"\"\"Early stopping to stop training when validation loss stops improving\"\"\"\n",
    "    \n",
    "    def __init__(self, patience: int = 10, min_delta: float = 0.0, mode: str = 'min'):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            patience: Number of epochs with no improvement after which training will be stopped\n",
    "            min_delta: Minimum change to qualify as an improvement\n",
    "            mode: 'min' or 'max' - whether lower or higher metric is better\n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.mode = mode\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.best_epoch = 0\n",
    "        \n",
    "    def __call__(self, score, epoch):\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.best_epoch = epoch\n",
    "            return False\n",
    "        \n",
    "        if self.mode == 'min':\n",
    "            improved = score < (self.best_score - self.min_delta)\n",
    "        else:\n",
    "            improved = score > (self.best_score + self.min_delta)\n",
    "        \n",
    "        if improved:\n",
    "            self.best_score = score\n",
    "            self.best_epoch = epoch\n",
    "            self.counter = 0\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "                \n",
    "        return self.early_stop\n",
    "\n",
    "\n",
    "class Trainer:\n",
    "    \"\"\"Training manager for calorie prediction\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        model,\n",
    "        train_loader,\n",
    "        val_loader,\n",
    "        criterion,\n",
    "        optimizer,\n",
    "        scheduler,\n",
    "        device,\n",
    "        output_dir,\n",
    "        early_stopping_patience=15,\n",
    "        scheduler_step_on_batch=False\n",
    "    ):\n",
    "        self.model = model\n",
    "        self.train_loader = train_loader\n",
    "        self.val_loader = val_loader\n",
    "        self.criterion = criterion\n",
    "        self.optimizer = optimizer\n",
    "        self.scheduler = scheduler\n",
    "        self.device = device\n",
    "        self.output_dir = output_dir\n",
    "        self.scheduler_step_on_batch = scheduler_step_on_batch\n",
    "        \n",
    "        # Early stopping\n",
    "        self.early_stopping = EarlyStopping(\n",
    "            patience=early_stopping_patience,\n",
    "            min_delta=0.1,\n",
    "            mode='min'\n",
    "        )\n",
    "        \n",
    "        # Tensorboard\n",
    "        self.writer = SummaryWriter(log_dir=os.path.join(output_dir, 'tensorboard'))\n",
    "        \n",
    "        # Tracking\n",
    "        self.best_val_loss = float('inf')\n",
    "        self.train_losses = []\n",
    "        self.val_losses = []\n",
    "        self.best_metrics = {}\n",
    "    \n",
    "    def train_epoch(self):\n",
    "        \"\"\"Train for one epoch\"\"\"\n",
    "        self.model.train()\n",
    "        total_loss = 0.0\n",
    "        num_batches = 0\n",
    "        \n",
    "        pbar = tqdm(self.train_loader, desc=\"Training\")\n",
    "        for batch_idx, batch in enumerate(pbar):\n",
    "            # Move to device\n",
    "            rgb = batch['rgb'].to(self.device)\n",
    "            depth = batch['depth'].to(self.device)\n",
    "            calories = batch['calorie'].to(self.device)\n",
    "            \n",
    "            # Forward pass\n",
    "            self.optimizer.zero_grad()\n",
    "            calorie_pred = self.model(rgb, depth)\n",
    "            \n",
    "            # Compute loss (MSE for calorie prediction)\n",
    "            loss = self.criterion(calorie_pred.squeeze(), calories)\n",
    "            \n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)\n",
    "            self.optimizer.step()\n",
    "            \n",
    "            # Update learning rate (if step_on_batch)\n",
    "            if self.scheduler_step_on_batch and self.scheduler:\n",
    "                self.scheduler.step()\n",
    "            \n",
    "            # Track metrics\n",
    "            total_loss += loss.item()\n",
    "            num_batches += 1\n",
    "            \n",
    "            # Update progress bar\n",
    "            pbar.set_postfix({'Loss': f'{loss.item():.4f}'})\n",
    "        \n",
    "        return total_loss / num_batches\n",
    "    \n",
    "    def validate_epoch(self):\n",
    "        \"\"\"Validate for one epoch\"\"\"\n",
    "        self.model.eval()\n",
    "        total_loss = 0.0\n",
    "        all_predictions = []\n",
    "        all_targets = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(self.val_loader, desc=\"Validation\"):\n",
    "                # Move to device\n",
    "                rgb = batch['rgb'].to(self.device)\n",
    "                depth = batch['depth'].to(self.device)\n",
    "                calories = batch['calorie'].to(self.device)\n",
    "                \n",
    "                # Forward pass\n",
    "                calorie_pred = self.model(rgb, depth)\n",
    "                \n",
    "                # Compute loss\n",
    "                loss = self.criterion(calorie_pred.squeeze(), calories)\n",
    "                total_loss += loss.item()\n",
    "                \n",
    "                # Store predictions and targets for metrics\n",
    "                all_predictions.extend(calorie_pred.squeeze().cpu().numpy())\n",
    "                all_targets.extend(calories.cpu().numpy())\n",
    "        \n",
    "        # Calculate metrics\n",
    "        avg_loss = total_loss / len(self.val_loader)\n",
    "        predictions = np.array(all_predictions)\n",
    "        targets = np.array(all_targets)\n",
    "        \n",
    "        mae = np.mean(np.abs(predictions - targets))\n",
    "        \n",
    "        return avg_loss, mae\n",
    "    \n",
    "    def train(self, num_epochs):\n",
    "        \"\"\"Full training loop\"\"\"\n",
    "        print(f\"Starting training for {num_epochs} epochs...\")\n",
    "        \n",
    "        for epoch in range(num_epochs):\n",
    "            print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
    "            \n",
    "            # Train\n",
    "            train_loss = self.train_epoch()\n",
    "            \n",
    "            # Validate\n",
    "            val_loss, mae = self.validate_epoch()\n",
    "            \n",
    "            # Update learning rate (if not step_on_batch)\n",
    "            if not self.scheduler_step_on_batch and self.scheduler:\n",
    "                self.scheduler.step(val_loss)\n",
    "            \n",
    "            # Log metrics\n",
    "            self.writer.add_scalar('Loss/Train', train_loss, epoch)\n",
    "            self.writer.add_scalar('Loss/Val', val_loss, epoch)\n",
    "            self.writer.add_scalar('MAE', mae, epoch)\n",
    "            \n",
    "            # Save best model\n",
    "            if val_loss < self.best_val_loss:\n",
    "                self.best_val_loss = val_loss\n",
    "                self.best_metrics = {\n",
    "                    'epoch': epoch + 1,\n",
    "                    'val_loss': val_loss,\n",
    "                    'mae': mae,\n",
    "                }\n",
    "                \n",
    "                # Save model checkpoint\n",
    "                torch.save({\n",
    "                    'epoch': epoch + 1,\n",
    "                    'model_state_dict': self.model.state_dict(),\n",
    "                    'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "                    'val_loss': val_loss,\n",
    "                    'mae': mae,\n",
    "                }, os.path.join(self.output_dir, 'best_model.pth'))\n",
    "            \n",
    "            # Print epoch results\n",
    "            print(f\"Train Loss: {train_loss:.4f}\")\n",
    "            print(f\"Val Loss: {val_loss:.4f}\")\n",
    "            print(f\"MAE: {mae:.2f}\")\n",
    "            \n",
    "            # Early stopping\n",
    "            if self.early_stopping(val_loss, epoch):\n",
    "                print(f\"Early stopping triggered after {epoch+1} epochs\")\n",
    "                print(f\"Best epoch: {self.early_stopping.best_epoch+1}\")\n",
    "                break\n",
    "        \n",
    "        self.writer.close()\n",
    "        print(f\"\\nTraining completed!\")\n",
    "        print(f\"Best validation loss: {self.best_val_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9d0805ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration:\n",
      "  Data root: ../Nutrition5K/train\n",
      "  CSV path: ../Nutrition5K/nutrition5k_train.csv\n",
      "  Output directory: ../experiments\n",
      "  Batch size: 32\n",
      "  Number of epochs: 40\n",
      "  Image size: 256\n",
      "  Workers: 4\n"
     ]
    }
   ],
   "source": [
    "# Configuration - Update these paths to match your setup\n",
    "DATA_ROOT = '../Nutrition5K/train'  # Path to training data directory\n",
    "CSV_PATH = '../Nutrition5K/nutrition5k_train.csv'  # Path to training CSV\n",
    "OUTPUT_DIR = '../experiments'  # Directory to save experiment results\n",
    "\n",
    "# Global training hyperparameters (learning rate and weight decay set per experiment)\n",
    "BATCH_SIZE = 32\n",
    "NUM_EPOCHS = 40\n",
    "VAL_RATIO = 0.15\n",
    "IMG_SIZE = 256\n",
    "NUM_WORKERS = 4\n",
    "\n",
    "print(\"Configuration:\")\n",
    "print(f\"  Data root: {DATA_ROOT}\")\n",
    "print(f\"  CSV path: {CSV_PATH}\")\n",
    "print(f\"  Output directory: {OUTPUT_DIR}\")\n",
    "print(f\"  Batch size: {BATCH_SIZE}\")\n",
    "print(f\"  Number of epochs: {NUM_EPOCHS}\")\n",
    "print(f\"  Image size: {IMG_SIZE}\")\n",
    "print(f\"  Workers: {NUM_WORKERS}\")\n",
    "\n",
    "# Create output directory\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1969721f",
   "metadata": {},
   "source": [
    "### Dataset Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2c3f4f6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating train/validation split...\n",
      "Train CSV: ../Nutrition5K/train_split.csv\n",
      "Validation CSV: ../Nutrition5K/val_split.csv\n",
      "Loaded 2804 valid samples out of 2805\n",
      "\n",
      "Dataset loaded successfully!\n",
      "Training samples: 2804\n",
      "RGB shape: torch.Size([3, 256, 256])\n",
      "Depth shape: torch.Size([1, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "# Create train/validation split\n",
    "print(\"Creating train/validation split...\")\n",
    "train_csv, val_csv = create_train_val_split(\n",
    "    CSV_PATH,\n",
    "    val_ratio=VAL_RATIO,\n",
    "    random_seed=42\n",
    ")\n",
    "\n",
    "print(f\"Train CSV: {train_csv}\")\n",
    "print(f\"Validation CSV: {val_csv}\")\n",
    "\n",
    "# Load a sample to check data\n",
    "sample_dataset = Nutrition5KDataset(\n",
    "    csv_path=train_csv,\n",
    "    data_root=DATA_ROOT,\n",
    "    split='train',\n",
    "    augment=False,  # No augmentation for checking\n",
    "    img_size=IMG_SIZE,\n",
    ")\n",
    "\n",
    "print(f\"\\nDataset loaded successfully!\")\n",
    "print(f\"Training samples: {len(sample_dataset)}\")\n",
    "print(f\"RGB shape: {sample_dataset[0]['rgb'].shape}\")\n",
    "print(f\"Depth shape: {sample_dataset[0]['depth'].shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82df622d",
   "metadata": {},
   "source": [
    "## Inception"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9365adb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "\n",
    "class InceptionV3Encoder(nn.Module):\n",
    "    \"\"\"InceptionV3 encoder as used in the original Nutrition5k paper\"\"\"\n",
    "    \n",
    "    def __init__(self, pretrained: bool = False, in_channels: int = 3):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Load InceptionV3 model\n",
    "        inception = models.inception_v3(pretrained=pretrained, aux_logits=False)\n",
    "        \n",
    "        # The output of InceptionV3 features is 2048 channels\n",
    "        self.out_channels = 2048\n",
    "        \n",
    "        # Modify first conv if we have different input channels (e.g., 1 for depth)\n",
    "        if in_channels != 3:\n",
    "            self.Conv2d_1a_3x3 = nn.Conv2d(\n",
    "                in_channels, 32, kernel_size=3, stride=2, bias=False\n",
    "            )\n",
    "        else:\n",
    "            self.Conv2d_1a_3x3 = inception.Conv2d_1a_3x3\n",
    "        \n",
    "        # Copy all other layers from InceptionV3\n",
    "        # First block\n",
    "        self.Conv2d_2a_3x3 = inception.Conv2d_2a_3x3\n",
    "        self.Conv2d_2b_3x3 = inception.Conv2d_2b_3x3\n",
    "        self.maxpool1 = inception.maxpool1\n",
    "        \n",
    "        # Second block\n",
    "        self.Conv2d_3b_1x1 = inception.Conv2d_3b_1x1\n",
    "        self.Conv2d_4a_3x3 = inception.Conv2d_4a_3x3\n",
    "        self.maxpool2 = inception.maxpool2\n",
    "        \n",
    "        # Inception blocks\n",
    "        self.Mixed_5b = inception.Mixed_5b\n",
    "        self.Mixed_5c = inception.Mixed_5c\n",
    "        self.Mixed_5d = inception.Mixed_5d\n",
    "        self.Mixed_6a = inception.Mixed_6a\n",
    "        self.Mixed_6b = inception.Mixed_6b\n",
    "        self.Mixed_6c = inception.Mixed_6c\n",
    "        self.Mixed_6d = inception.Mixed_6d\n",
    "        self.Mixed_6e = inception.Mixed_6e\n",
    "        self.Mixed_7a = inception.Mixed_7a\n",
    "        self.Mixed_7b = inception.Mixed_7b\n",
    "        self.Mixed_7c = inception.Mixed_7c\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Input tensor (B, C, H, W)\n",
    "        Returns:\n",
    "            Feature map (B, 2048, H/32, W/32)\n",
    "        \"\"\"\n",
    "        # First block\n",
    "        x = self.Conv2d_1a_3x3(x)\n",
    "        x = self.Conv2d_2a_3x3(x)\n",
    "        x = self.Conv2d_2b_3x3(x)\n",
    "        x = self.maxpool1(x)\n",
    "        \n",
    "        # Second block\n",
    "        x = self.Conv2d_3b_1x1(x)\n",
    "        x = self.Conv2d_4a_3x3(x)\n",
    "        x = self.maxpool2(x)\n",
    "        \n",
    "        # Inception blocks\n",
    "        x = self.Mixed_5b(x)\n",
    "        x = self.Mixed_5c(x)\n",
    "        x = self.Mixed_5d(x)\n",
    "        x = self.Mixed_6a(x)\n",
    "        x = self.Mixed_6b(x)\n",
    "        x = self.Mixed_6c(x)\n",
    "        x = self.Mixed_6d(x)\n",
    "        x = self.Mixed_6e(x)\n",
    "        x = self.Mixed_7a(x)\n",
    "        x = self.Mixed_7b(x)\n",
    "        x = self.Mixed_7c(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "# Early Fusion Module (RGB + Depth fused at input level)\n",
    "class EarlyFusion(nn.Module):\n",
    "    \"\"\"\n",
    "    Early Fusion: Combine RGB and Depth channels at the input level\n",
    "    before processing through the network\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, pretrained: bool = False, fusion_channels: int = 2048, dropout_rate: float = 0.4):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Create a single encoder with 4 input channels (3 RGB + 1 Depth)\n",
    "        self.encoder = InceptionV3Encoder(pretrained=pretrained, in_channels=4)\n",
    "        \n",
    "        # Regression head for calorie prediction\n",
    "        self.regression_head = RegressionHead(\n",
    "            in_channels=self.encoder.out_channels,\n",
    "            dropout_rate=dropout_rate\n",
    "        )\n",
    "    \n",
    "    def forward(self, rgb, depth):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            rgb: RGB images (B, 3, H, W)\n",
    "            depth: Depth images (B, 1, H, W)\n",
    "        \n",
    "        Returns:\n",
    "            Predicted calories (B, 1)\n",
    "        \"\"\"\n",
    "        # Concatenate RGB and depth along channel dimension\n",
    "        x = torch.cat([rgb, depth], dim=1)  # (B, 4, H, W)\n",
    "        \n",
    "        # Process through the encoder\n",
    "        features = self.encoder(x)\n",
    "        \n",
    "        # Predict calories\n",
    "        calories = self.regression_head(features)\n",
    "        \n",
    "        return calories\n",
    "\n",
    "# Late Fusion Module (RGB + Depth processed separately and fused at regression level)\n",
    "class LateFusion(nn.Module):\n",
    "    \"\"\"\n",
    "    Late Fusion: Process RGB and Depth streams independently, then fuse at the regression head level\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, pretrained: bool = False, fusion_channels: int = 2048, dropout_rate: float = 0.4):\n",
    "        super().__init__()\n",
    "        \n",
    "        # RGB and Depth encoders\n",
    "        self.rgb_encoder = InceptionV3Encoder(pretrained=pretrained, in_channels=3)\n",
    "        self.depth_encoder = InceptionV3Encoder(pretrained=pretrained, in_channels=1)\n",
    "        \n",
    "        # Global average pooling\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        \n",
    "        # Fusion at the feature vector level\n",
    "        in_features = self.rgb_encoder.out_channels + self.depth_encoder.out_channels\n",
    "        \n",
    "        # Fully connected layers for regression\n",
    "        self.regression_layers = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(in_features, 512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(256, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, rgb, depth):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            rgb: RGB images (B, 3, H, W)\n",
    "            depth: Depth images (B, 1, H, W)\n",
    "        \n",
    "        Returns:\n",
    "            Predicted calories (B, 1)\n",
    "        \"\"\"\n",
    "        # Extract features from both streams\n",
    "        rgb_features = self.rgb_encoder(rgb)    # (B, 2048, H/32, W/32)\n",
    "        depth_features = self.depth_encoder(depth)  # (B, 2048, H/32, W/32)\n",
    "        \n",
    "        # Apply global average pooling\n",
    "        rgb_features = self.avgpool(rgb_features)    # (B, 2048, 1, 1)\n",
    "        depth_features = self.avgpool(depth_features)  # (B, 2048, 1, 1)\n",
    "        \n",
    "        # Concatenate feature vectors\n",
    "        fused = torch.cat([rgb_features, depth_features], dim=1)  # (B, 4096, 1, 1)\n",
    "        \n",
    "        # Predict calories\n",
    "        calories = self.regression_layers(fused)\n",
    "        \n",
    "        return calories\n",
    "\n",
    "class RegressionHead(nn.Module):\n",
    "    def __init__(self, in_channels: int = 2048, dropout_rate: float = 0.4):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        \n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(in_channels, 512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(256, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.avgpool(x)  # (B, C, 1, 1)\n",
    "        x = self.fc_layers(x)  # (B, 1)\n",
    "        return x\n",
    "\n",
    "\n",
    "class VolumeEstimator(nn.Module):\n",
    "    \"\"\"\n",
    "    Food volume estimation from overhead depth images following the Nutrition5k paper.\n",
    "    \n",
    "    Given:\n",
    "    - Distance between camera and capture plane: 35.9 cm\n",
    "    - Per-pixel surface area at this distance: 5.957 × 10^-3 cm²\n",
    "    \n",
    "    The volume is calculated by:\n",
    "    1. Computing per-pixel volume (depth × surface_area)\n",
    "    2. Summing over all food pixels (using binary threshold segmentation)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 camera_distance: float = 35.9,  # cm\n",
    "                 pixel_surface_area: float = 5.957e-3,  # cm²\n",
    "                 depth_threshold: float = 0.1):  # Threshold for simple segmentation\n",
    "        super().__init__()\n",
    "        \n",
    "        self.camera_distance = camera_distance\n",
    "        self.pixel_surface_area = pixel_surface_area\n",
    "        self.depth_threshold = depth_threshold\n",
    "    \n",
    "    def forward(self, depth_images):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            depth_images: Depth images (B, 1, H, W), normalized to [0, 1] range\n",
    "        \n",
    "        Returns:\n",
    "            volume_estimates: Volume in cm³ for each image (B, 1)\n",
    "        \"\"\"\n",
    "        # Simple threshold-based segmentation for foreground/background\n",
    "        segmentation_mask = (depth_images > self.depth_threshold).float()\n",
    "        \n",
    "        # Convert normalized depth back to actual depth values\n",
    "        # Assuming depth is normalized to [0, 1] and represents distance from camera\n",
    "        # For simplicity, we assume the depth represents actual distance in cm scaled to [0, 1]\n",
    "        depth_cm = depth_images * self.camera_distance\n",
    "        \n",
    "        # Calculate per-pixel volume: depth × surface_area\n",
    "        per_pixel_volume = depth_cm * self.pixel_surface_area  # (B, 1, H, W)\n",
    "        \n",
    "        # Apply segmentation mask to consider only food pixels\n",
    "        masked_volume = per_pixel_volume * segmentation_mask\n",
    "        \n",
    "        # Sum over all pixels to get total volume\n",
    "        volume_estimates = masked_volume.sum(dim=[2, 3])  # (B, 1)\n",
    "        \n",
    "        return volume_estimates\n",
    "\n",
    "\n",
    "class RegressionHeadWithVolume(nn.Module):\n",
    "    \"\"\"\n",
    "    Regression head that concatenates volume estimate to InceptionV3 features.\n",
    "    \n",
    "    According to the paper: \"concatenating the volume estimation value to the output \n",
    "    of the InceptionV3 backbone, before the following two fully connected layers\"\n",
    "    with FC layers of 64 and 1 dimension.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, in_channels: int = 2048, dropout_rate: float = 0.4):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        \n",
    "        # Two FC layers as described in the paper (2048+1 -> 64 -> 1)\n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(in_channels + 1, 64),  # +1 for volume\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, features, volume):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            features: Feature maps from backbone (B, 2048, H, W)\n",
    "            volume: Volume estimates (B, 1)\n",
    "        \n",
    "        Returns:\n",
    "            Predicted calories (B, 1)\n",
    "        \"\"\"\n",
    "        # Global average pooling\n",
    "        x = self.avgpool(features)  # (B, 2048, 1, 1)\n",
    "        x = torch.flatten(x, 1)  # (B, 2048)\n",
    "        \n",
    "        # Concatenate volume estimate\n",
    "        x = torch.cat([x, volume], dim=1)  # (B, 2049)\n",
    "        \n",
    "        # Predict calories\n",
    "        x = self.fc_layers(x)  # (B, 1)\n",
    "        \n",
    "        return x\n",
    "\n",
    "class Nutrition5kModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Implementation of the dual-stream architecture used in the original Nutrition5k paper\n",
    "    Uses InceptionV3 as the backbone and middle fusion\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        fusion: str = 'middle',\n",
    "        fusion_channels: int = 2048,\n",
    "        dropout_rate: float = 0.4,\n",
    "        pretrained: bool = False,\n",
    "        use_volume: bool = False\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.use_volume = use_volume\n",
    "        \n",
    "        if fusion == 'early':\n",
    "            self.model = EarlyFusion(\n",
    "                pretrained=pretrained,\n",
    "                fusion_channels=fusion_channels,\n",
    "                dropout_rate=dropout_rate\n",
    "            )\n",
    "        elif fusion == 'late':\n",
    "            self.model = LateFusion(\n",
    "                pretrained=pretrained,\n",
    "                fusion_channels=fusion_channels,\n",
    "                dropout_rate=dropout_rate\n",
    "            )\n",
    "        elif fusion == 'image_only':\n",
    "            # Image-only variant: only RGB is used\n",
    "            self.rgb_encoder = InceptionV3Encoder(pretrained=pretrained, in_channels=3)\n",
    "            \n",
    "            # Volume estimator (if enabled)\n",
    "            if use_volume:\n",
    "                self.volume_estimator = VolumeEstimator()\n",
    "                self.regression_head = RegressionHeadWithVolume(\n",
    "                    in_channels=self.rgb_encoder.out_channels,\n",
    "                    dropout_rate=dropout_rate\n",
    "                )\n",
    "            else:\n",
    "                self.regression_head = RegressionHead(\n",
    "                    in_channels=self.rgb_encoder.out_channels,\n",
    "                    dropout_rate=dropout_rate\n",
    "                )\n",
    "        elif fusion == 'image_volume':\n",
    "            # Image+Volume variant: RGB encoder + volume as additional signal\n",
    "            self.rgb_encoder = InceptionV3Encoder(pretrained=pretrained, in_channels=3)\n",
    "            self.volume_estimator = VolumeEstimator()\n",
    "            self.regression_head = RegressionHeadWithVolume(\n",
    "                in_channels=self.rgb_encoder.out_channels,\n",
    "                dropout_rate=dropout_rate\n",
    "            )\n",
    "            self.use_volume = True  # Always use volume for this variant\n",
    "        else:  # middle fusion\n",
    "            # RGB and Depth encoders using InceptionV3\n",
    "            self.rgb_encoder = InceptionV3Encoder(pretrained=pretrained, in_channels=3)\n",
    "            self.depth_encoder = InceptionV3Encoder(pretrained=pretrained, in_channels=1)\n",
    "            \n",
    "            # Create middle fusion module\n",
    "            from_channels = self.rgb_encoder.out_channels + self.depth_encoder.out_channels\n",
    "            self.fusion_conv = nn.Sequential(\n",
    "                nn.Conv2d(from_channels, fusion_channels, kernel_size=1, bias=False),\n",
    "                nn.BatchNorm2d(fusion_channels),\n",
    "                nn.ReLU(inplace=True)\n",
    "            )\n",
    "            \n",
    "            # Volume estimator (if enabled)\n",
    "            if use_volume:\n",
    "                self.volume_estimator = VolumeEstimator()\n",
    "                self.regression_head = RegressionHeadWithVolume(\n",
    "                    in_channels=fusion_channels,\n",
    "                    dropout_rate=dropout_rate\n",
    "                )\n",
    "            else:\n",
    "                self.regression_head = RegressionHead(\n",
    "                    in_channels=fusion_channels,\n",
    "                    dropout_rate=dropout_rate\n",
    "                )\n",
    "    \n",
    "    def forward(self, rgb, depth):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            rgb: RGB images (B, 3, H, W)\n",
    "            depth: Depth images (B, 1, H, W)\n",
    "        \n",
    "        Returns:\n",
    "            calorie_pred: Predicted calories (B, 1)\n",
    "        \"\"\"\n",
    "        if hasattr(self, 'model'):\n",
    "            return self.model(rgb, depth)\n",
    "        \n",
    "        # Calculate volume estimate if enabled\n",
    "        volume = None\n",
    "        if self.use_volume and hasattr(self, 'volume_estimator'):\n",
    "            volume = self.volume_estimator(depth)  # (B, 1)\n",
    "        \n",
    "        # Image-only or Image+Volume variant\n",
    "        if hasattr(self, 'rgb_encoder') and not hasattr(self, 'depth_encoder'):\n",
    "            rgb_features = self.rgb_encoder(rgb)  # (B, 2048, H/32, W/32)\n",
    "            \n",
    "            if volume is not None:\n",
    "                calorie_pred = self.regression_head(rgb_features, volume)\n",
    "            else:\n",
    "                calorie_pred = self.regression_head(rgb_features)\n",
    "            \n",
    "            return calorie_pred\n",
    "        \n",
    "        # Extract features from both streams\n",
    "        rgb_features = self.rgb_encoder(rgb)      # (B, 2048, H/32, W/32)\n",
    "        depth_features = self.depth_encoder(depth)  # (B, 2048, H/32, W/32)\n",
    "        \n",
    "        # Middle fusion - concatenate and apply 1x1 conv\n",
    "        fused = torch.cat([rgb_features, depth_features], dim=1)  # (B, 4096, H/32, W/32)\n",
    "        fused = self.fusion_conv(fused)  # (B, 2048, H/32, W/32)\n",
    "        \n",
    "        # Predict calories (with or without volume)\n",
    "        if volume is not None:\n",
    "            calorie_pred = self.regression_head(fused, volume)\n",
    "        else:\n",
    "            calorie_pred = self.regression_head(fused)\n",
    "        \n",
    "        return calorie_pred\n",
    "    \n",
    "    def get_num_parameters(self):\n",
    "        \"\"\"Get total number of trainable parameters\"\"\"\n",
    "        return sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
    "\n",
    "# Factory function to build Nutrition5k models with different fusion types\n",
    "def build_nutrition5k_model(fusion='middle', pretrained=False, dropout_rate=0.4, fusion_channels=2048, \n",
    "                           use_volume=False, **kwargs):\n",
    "    \"\"\"\n",
    "    Factory function to build models using the Nutrition5k paper architecture (InceptionV3 backbone)\n",
    "    \n",
    "    Args:\n",
    "        fusion: Fusion type ('early', 'middle', 'late', 'image_only', or 'image_volume')\n",
    "        pretrained: Whether to use pretrained weights for InceptionV3\n",
    "        dropout_rate: Dropout rate for regression head\n",
    "        fusion_channels: Number of channels after fusion\n",
    "        use_volume: Whether to use volume estimation as additional signal (uses simple threshold-based segmentation)\n",
    "    \n",
    "    Returns:\n",
    "        Nutrition5k model with specified configuration\n",
    "    \"\"\"\n",
    "    return Nutrition5kModel(\n",
    "        fusion=fusion,\n",
    "        fusion_channels=fusion_channels,\n",
    "        dropout_rate=dropout_rate,\n",
    "        pretrained=pretrained,\n",
    "        use_volume=use_volume\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d718780",
   "metadata": {},
   "source": [
    "## Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "480c9c51",
   "metadata": {},
   "source": [
    "### InceptionV3 - Middle Fusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3dcecae0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "TRAINING: Nutrition5k InceptionV3 + Middle Fusion\n",
      "============================================================\n",
      "Loaded 2804 valid samples out of 2805\n",
      "Loaded 495 valid samples out of 495\n",
      "Model parameters: 53,143,873\n",
      "Training samples: 2804\n",
      "Validation samples: 495\n",
      "Learning rate: 0.0003\n",
      "Weight decay: 1e-06\n",
      "Starting training for 40 epochs...\n",
      "\n",
      "Epoch 1/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:15<00:00,  5.72it/s, Loss=128568.6797]\n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 99179.5700\n",
      "Val Loss: 107412.9351\n",
      "MAE: 240.64\n",
      "\n",
      "Epoch 2/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:14<00:00,  6.18it/s, Loss=14551.3506] \n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 44613.2511\n",
      "Val Loss: 22115.9072\n",
      "MAE: 103.60\n",
      "\n",
      "Epoch 3/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.24it/s, Loss=8529.9629] \n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 14990.2540\n",
      "Val Loss: 15339.2216\n",
      "MAE: 89.52\n",
      "\n",
      "Epoch 4/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.24it/s, Loss=10611.9082]\n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 12844.5238\n",
      "Val Loss: 14614.1202\n",
      "MAE: 86.15\n",
      "\n",
      "Epoch 5/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:14<00:00,  6.19it/s, Loss=10989.2109]\n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 12333.9567\n",
      "Val Loss: 13903.3691\n",
      "MAE: 84.82\n",
      "\n",
      "Epoch 6/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.34it/s, Loss=7037.2222] \n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 13701.8433\n",
      "Val Loss: 15968.9327\n",
      "MAE: 87.62\n",
      "\n",
      "Epoch 7/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.43it/s, Loss=9634.6934] \n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 9885.7110\n",
      "Val Loss: 13036.7188\n",
      "MAE: 78.05\n",
      "\n",
      "Epoch 8/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.32it/s, Loss=15831.9697]\n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 13061.2367\n",
      "Val Loss: 37848.2272\n",
      "MAE: 150.94\n",
      "\n",
      "Epoch 9/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.28it/s, Loss=15873.6582]\n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 13630.2193\n",
      "Val Loss: 26709.6769\n",
      "MAE: 118.80\n",
      "\n",
      "Epoch 10/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.29it/s, Loss=12883.2695]\n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 11182.7567\n",
      "Val Loss: 16951.6092\n",
      "MAE: 94.59\n",
      "\n",
      "Epoch 11/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.38it/s, Loss=8178.9229] \n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 8048.6062\n",
      "Val Loss: 12620.6396\n",
      "MAE: 74.06\n",
      "\n",
      "Epoch 12/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.34it/s, Loss=7492.9854] \n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 10334.1747\n",
      "Val Loss: 15044.2647\n",
      "MAE: 86.57\n",
      "\n",
      "Epoch 13/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.30it/s, Loss=3736.9170] \n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 7712.2835\n",
      "Val Loss: 10110.7316\n",
      "MAE: 70.29\n",
      "\n",
      "Epoch 14/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.35it/s, Loss=12397.4609]\n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 4862.7000\n",
      "Val Loss: 9211.0626\n",
      "MAE: 63.99\n",
      "\n",
      "Epoch 15/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.34it/s, Loss=4223.6538] \n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 4543.4875\n",
      "Val Loss: 9900.7881\n",
      "MAE: 65.54\n",
      "\n",
      "Epoch 16/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.34it/s, Loss=2259.6072] \n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 4426.4736\n",
      "Val Loss: 9328.8389\n",
      "MAE: 66.59\n",
      "\n",
      "Epoch 17/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.31it/s, Loss=1298.2256] \n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 3814.8753\n",
      "Val Loss: 8683.5598\n",
      "MAE: 63.18\n",
      "\n",
      "Epoch 18/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:14<00:00,  6.18it/s, Loss=4756.7666] \n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 4460.9755\n",
      "Val Loss: 9520.8630\n",
      "MAE: 66.56\n",
      "\n",
      "Epoch 19/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.25it/s, Loss=3241.6943] \n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 4638.1852\n",
      "Val Loss: 8618.1980\n",
      "MAE: 63.57\n",
      "\n",
      "Epoch 20/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.23it/s, Loss=2759.9041] \n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 4441.7072\n",
      "Val Loss: 10380.2209\n",
      "MAE: 66.44\n",
      "\n",
      "Epoch 21/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.29it/s, Loss=8066.5176] \n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 4397.2711\n",
      "Val Loss: 9014.0540\n",
      "MAE: 64.05\n",
      "\n",
      "Epoch 22/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.23it/s, Loss=2299.1699] \n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 3708.5228\n",
      "Val Loss: 8564.3510\n",
      "MAE: 61.92\n",
      "\n",
      "Epoch 23/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.30it/s, Loss=3101.3701] \n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 3872.7702\n",
      "Val Loss: 9195.6286\n",
      "MAE: 63.51\n",
      "\n",
      "Epoch 24/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.28it/s, Loss=1211.8209] \n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 3619.2267\n",
      "Val Loss: 8286.5531\n",
      "MAE: 59.58\n",
      "\n",
      "Epoch 25/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.31it/s, Loss=4860.5879] \n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 4642.6053\n",
      "Val Loss: 11539.1972\n",
      "MAE: 76.86\n",
      "\n",
      "Epoch 26/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.31it/s, Loss=5091.5918] \n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 5310.7747\n",
      "Val Loss: 9601.1501\n",
      "MAE: 65.47\n",
      "\n",
      "Epoch 27/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.28it/s, Loss=3598.9797]\n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 4402.4465\n",
      "Val Loss: 8586.3208\n",
      "MAE: 60.98\n",
      "\n",
      "Epoch 28/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.28it/s, Loss=3286.5671] \n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 4379.7765\n",
      "Val Loss: 9463.2135\n",
      "MAE: 64.45\n",
      "\n",
      "Epoch 29/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.28it/s, Loss=3635.2405] \n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 3820.1549\n",
      "Val Loss: 7934.8172\n",
      "MAE: 57.35\n",
      "\n",
      "Epoch 30/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.23it/s, Loss=9945.4971] \n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 5754.6514\n",
      "Val Loss: 10848.7680\n",
      "MAE: 67.77\n",
      "\n",
      "Epoch 31/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:14<00:00,  6.12it/s, Loss=2592.0410] \n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 4085.6081\n",
      "Val Loss: 8705.7453\n",
      "MAE: 62.18\n",
      "\n",
      "Epoch 32/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.28it/s, Loss=1845.0674] \n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 3637.0483\n",
      "Val Loss: 8562.5231\n",
      "MAE: 62.51\n",
      "\n",
      "Epoch 33/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.27it/s, Loss=5132.6309] \n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 3630.7866\n",
      "Val Loss: 9332.6526\n",
      "MAE: 62.17\n",
      "\n",
      "Epoch 34/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.27it/s, Loss=1935.2677] \n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 3029.1384\n",
      "Val Loss: 7578.5881\n",
      "MAE: 56.28\n",
      "\n",
      "Epoch 35/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.31it/s, Loss=5809.5571] \n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 4695.0795\n",
      "Val Loss: 9082.4254\n",
      "MAE: 66.55\n",
      "\n",
      "Epoch 36/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.28it/s, Loss=2360.2610] \n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 4250.1567\n",
      "Val Loss: 8774.8670\n",
      "MAE: 62.02\n",
      "\n",
      "Epoch 37/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.41it/s, Loss=5048.2212]\n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 3479.9186\n",
      "Val Loss: 9305.3121\n",
      "MAE: 62.28\n",
      "\n",
      "Epoch 38/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.33it/s, Loss=1282.8358] \n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 3230.0179\n",
      "Val Loss: 7757.0370\n",
      "MAE: 56.59\n",
      "\n",
      "Epoch 39/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.33it/s, Loss=3625.0686] \n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 4668.2630\n",
      "Val Loss: 12126.1959\n",
      "MAE: 82.30\n",
      "\n",
      "Epoch 40/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.33it/s, Loss=8918.3311] \n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 5625.5605\n",
      "Val Loss: 14431.2035\n",
      "MAE: 85.84\n",
      "\n",
      "Training completed!\n",
      "Best validation loss: 7578.5881\n",
      "\n",
      "Experiment completed! Results saved to: ../experiments/nutrition5k_experiments/inceptionv3_middle_fusion_20251024_130923\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Add a new cell with this code to run the Nutrition5k InceptionV3 experiments\n",
    "\n",
    "# Import the necessary modules\n",
    "import sys\n",
    "sys.path.append('/data/projects/punim0478/setiawand/Computer-Vision-COMP90086/src')\n",
    "\n",
    "from nutrition5k_inceptionv3_model import build_nutrition5k_model\n",
    "\n",
    "# Configure experiment settings\n",
    "BATCH_SIZE = 32\n",
    "NUM_EPOCHS = 40\n",
    "DROPOUT_RATE = 0.4\n",
    "LEARNING_RATE = 3e-4\n",
    "WEIGHT_DECAY = 1e-6\n",
    "EARLY_STOPPING_PATIENCE = 15\n",
    "WARMUP_RATIO = 0.1\n",
    "MIN_LR_RATIO = 0.05\n",
    "FUSION_CHANNELS = 2048  # InceptionV3 output channels\n",
    "\n",
    "def train_nutrition5k_model(fusion_type='middle'):\n",
    "    \"\"\"Train the Nutrition5k model with InceptionV3 and specified fusion type\"\"\"\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "    print(f\"TRAINING: Nutrition5k InceptionV3 + {fusion_type.capitalize()} Fusion\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Create datasets\n",
    "    train_dataset = Nutrition5KDataset(\n",
    "        csv_path=train_csv,\n",
    "        data_root=DATA_ROOT,\n",
    "        split='train',\n",
    "        augment=False,\n",
    "        img_size=IMG_SIZE,\n",
    "    )\n",
    "    \n",
    "    val_dataset = Nutrition5KDataset(\n",
    "        csv_path=val_csv,\n",
    "        data_root=DATA_ROOT,\n",
    "        split='val',\n",
    "        augment=False,\n",
    "        img_size=IMG_SIZE,\n",
    "    )\n",
    "    \n",
    "    # Create data loaders\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=True,\n",
    "        num_workers=NUM_WORKERS,\n",
    "        pin_memory=True if torch.cuda.is_available() else False,\n",
    "        drop_last=True\n",
    "    )\n",
    "    \n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=False,\n",
    "        num_workers=NUM_WORKERS,\n",
    "        pin_memory=True if torch.cuda.is_available() else False\n",
    "    )\n",
    "    \n",
    "    # Build model with specified fusion type\n",
    "    model = build_nutrition5k_model(\n",
    "        fusion=fusion_type,\n",
    "        pretrained=False,\n",
    "        dropout_rate=DROPOUT_RATE,\n",
    "        fusion_channels=FUSION_CHANNELS\n",
    "    )\n",
    "    model = model.to(device)\n",
    "    \n",
    "    print(f\"Model parameters: {model.get_num_parameters():,}\")\n",
    "    print(f\"Training samples: {len(train_dataset)}\")\n",
    "    print(f\"Validation samples: {len(val_dataset)}\")\n",
    "    \n",
    "    # Loss function\n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    # Optimizer\n",
    "    optimizer = optim.AdamW(\n",
    "        model.parameters(),\n",
    "        lr=LEARNING_RATE,\n",
    "        weight_decay=WEIGHT_DECAY\n",
    "    )\n",
    "    \n",
    "    print(f\"Learning rate: {LEARNING_RATE}\")\n",
    "    print(f\"Weight decay: {WEIGHT_DECAY}\")\n",
    "    \n",
    "    # Learning rate scheduler\n",
    "    steps_per_epoch = len(train_loader)\n",
    "    total_steps = NUM_EPOCHS * steps_per_epoch\n",
    "    warmup_steps = int(total_steps * WARMUP_RATIO)\n",
    "    \n",
    "    scheduler = get_warmup_cosine_scheduler(\n",
    "        optimizer, \n",
    "        warmup_steps=warmup_steps, \n",
    "        total_steps=total_steps,\n",
    "        min_lr_ratio=MIN_LR_RATIO\n",
    "    )\n",
    "    \n",
    "    # Create experiment directory\n",
    "    exp_name = f\"inceptionv3_{fusion_type}_fusion_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "    exp_dir = os.path.join(OUTPUT_DIR, 'nutrition5k_experiments', exp_name)\n",
    "    os.makedirs(exp_dir, exist_ok=True)\n",
    "    \n",
    "    # Save experiment configuration\n",
    "    config = {\n",
    "        'fusion': fusion_type,\n",
    "        'pretrained': False,\n",
    "        'dropout_rate': DROPOUT_RATE,\n",
    "        'fusion_channels': FUSION_CHANNELS,\n",
    "        'learning_rate': LEARNING_RATE,\n",
    "        'weight_decay': WEIGHT_DECAY,\n",
    "        'batch_size': BATCH_SIZE,\n",
    "        'img_size': IMG_SIZE,\n",
    "        'num_epochs': NUM_EPOCHS\n",
    "    }\n",
    "    \n",
    "    with open(os.path.join(exp_dir, 'config.json'), 'w') as f:\n",
    "        json.dump(config, f, indent=4)\n",
    "    \n",
    "    # Create trainer\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        criterion=criterion,\n",
    "        optimizer=optimizer,\n",
    "        scheduler=scheduler,\n",
    "        device=device,\n",
    "        output_dir=exp_dir,\n",
    "        early_stopping_patience=EARLY_STOPPING_PATIENCE,\n",
    "        scheduler_step_on_batch=False\n",
    "    )\n",
    "    \n",
    "    # Train the model\n",
    "    trainer.train(NUM_EPOCHS)\n",
    "    \n",
    "    print(f\"\\nExperiment completed! Results saved to: {exp_dir}\")\n",
    "    return trainer.best_metrics\n",
    "\n",
    "# Run an experiment with middle fusion\n",
    "middle_fusion_results = train_nutrition5k_model(fusion_type='middle')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eee7ad5",
   "metadata": {},
   "source": [
    "### InceptionV3 - Early Fusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d2cd5dc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "TRAINING: Nutrition5k InceptionV3 + Early Fusion\n",
      "============================================================\n",
      "Loaded 2804 valid samples out of 2805\n",
      "Loaded 495 valid samples out of 495\n",
      "Model parameters: 22,966,465\n",
      "Training samples: 2804\n",
      "Validation samples: 495\n",
      "Learning rate: 0.0003\n",
      "Weight decay: 1e-06\n",
      "Starting training for 40 epochs...\n",
      "\n",
      "Epoch 1/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.49it/s, Loss=66747.9062] \n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 99325.6495\n",
      "Val Loss: 107376.2798\n",
      "MAE: 240.56\n",
      "\n",
      "Epoch 2/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.57it/s, Loss=22843.2305] \n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 42928.3323\n",
      "Val Loss: 19980.8986\n",
      "MAE: 103.48\n",
      "\n",
      "Epoch 3/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.55it/s, Loss=10708.5781]\n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 17106.7717\n",
      "Val Loss: 19836.2162\n",
      "MAE: 95.29\n",
      "\n",
      "Epoch 4/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.48it/s, Loss=12754.4180]\n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 17037.9287\n",
      "Val Loss: 29315.9156\n",
      "MAE: 112.77\n",
      "\n",
      "Epoch 5/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.52it/s, Loss=11109.4961]\n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 12419.4974\n",
      "Val Loss: 12350.2427\n",
      "MAE: 78.81\n",
      "\n",
      "Epoch 6/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.60it/s, Loss=8752.6113] \n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 13488.8264\n",
      "Val Loss: 12729.3925\n",
      "MAE: 76.98\n",
      "\n",
      "Epoch 7/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.46it/s, Loss=8419.4541] \n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 12929.6196\n",
      "Val Loss: 21976.7961\n",
      "MAE: 100.01\n",
      "\n",
      "Epoch 8/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.56it/s, Loss=4679.3589] \n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 10146.2328\n",
      "Val Loss: 10747.8688\n",
      "MAE: 71.96\n",
      "\n",
      "Epoch 9/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.50it/s, Loss=5635.6797] \n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 8792.4451\n",
      "Val Loss: 10783.5305\n",
      "MAE: 71.93\n",
      "\n",
      "Epoch 10/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.62it/s, Loss=5006.7964] \n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 7525.5879\n",
      "Val Loss: 11707.3728\n",
      "MAE: 71.07\n",
      "\n",
      "Epoch 11/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.63it/s, Loss=4415.2920] \n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 8775.2512\n",
      "Val Loss: 11203.9194\n",
      "MAE: 70.39\n",
      "\n",
      "Epoch 12/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.59it/s, Loss=8206.9492] \n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 7488.8060\n",
      "Val Loss: 11448.6037\n",
      "MAE: 70.75\n",
      "\n",
      "Epoch 13/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.58it/s, Loss=20127.1152]\n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 7698.7578\n",
      "Val Loss: 19493.1765\n",
      "MAE: 90.35\n",
      "\n",
      "Epoch 14/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.55it/s, Loss=12658.3789]\n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 8682.6980\n",
      "Val Loss: 18340.8633\n",
      "MAE: 85.01\n",
      "\n",
      "Epoch 15/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.58it/s, Loss=5315.3403] \n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 7846.1895\n",
      "Val Loss: 13579.7655\n",
      "MAE: 80.14\n",
      "\n",
      "Epoch 16/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.62it/s, Loss=7562.2861] \n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 8193.6605\n",
      "Val Loss: 10145.4527\n",
      "MAE: 69.51\n",
      "\n",
      "Epoch 17/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.41it/s, Loss=2269.3503] \n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 5541.4729\n",
      "Val Loss: 7903.5556\n",
      "MAE: 58.91\n",
      "\n",
      "Epoch 18/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.57it/s, Loss=8156.2046] \n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 6543.4155\n",
      "Val Loss: 8652.4755\n",
      "MAE: 62.92\n",
      "\n",
      "Epoch 19/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.58it/s, Loss=2912.4287] \n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 4890.6906\n",
      "Val Loss: 8625.3224\n",
      "MAE: 62.41\n",
      "\n",
      "Epoch 20/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.47it/s, Loss=3772.5403] \n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 5196.3465\n",
      "Val Loss: 10165.2951\n",
      "MAE: 63.64\n",
      "\n",
      "Epoch 21/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.46it/s, Loss=1960.0081] \n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 3562.4957\n",
      "Val Loss: 7652.7168\n",
      "MAE: 54.76\n",
      "\n",
      "Epoch 22/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.45it/s, Loss=17051.5156]\n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 5509.5268\n",
      "Val Loss: 12240.1665\n",
      "MAE: 74.17\n",
      "\n",
      "Epoch 23/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.50it/s, Loss=3583.2676] \n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 6604.5801\n",
      "Val Loss: 12108.0193\n",
      "MAE: 69.45\n",
      "\n",
      "Epoch 24/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.45it/s, Loss=7632.9282] \n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 6506.6554\n",
      "Val Loss: 14720.9593\n",
      "MAE: 77.30\n",
      "\n",
      "Epoch 25/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.45it/s, Loss=2055.4907] \n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 4572.6020\n",
      "Val Loss: 9063.0943\n",
      "MAE: 59.88\n",
      "\n",
      "Epoch 26/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.50it/s, Loss=1919.8328] \n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 3421.9659\n",
      "Val Loss: 7674.3759\n",
      "MAE: 56.12\n",
      "\n",
      "Epoch 27/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.42it/s, Loss=1848.4658] \n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 4576.4738\n",
      "Val Loss: 9714.7449\n",
      "MAE: 61.45\n",
      "\n",
      "Epoch 28/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.40it/s, Loss=2453.5884] \n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 3644.3707\n",
      "Val Loss: 7877.2721\n",
      "MAE: 55.54\n",
      "\n",
      "Epoch 29/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.46it/s, Loss=4258.2334] \n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 4365.7270\n",
      "Val Loss: 11066.9588\n",
      "MAE: 69.82\n",
      "\n",
      "Epoch 30/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.44it/s, Loss=7192.0464]\n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 3430.3935\n",
      "Val Loss: 7680.3667\n",
      "MAE: 57.24\n",
      "\n",
      "Epoch 31/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.50it/s, Loss=3328.4053] \n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 4283.5907\n",
      "Val Loss: 8796.7368\n",
      "MAE: 64.82\n",
      "\n",
      "Epoch 32/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.56it/s, Loss=2363.3750] \n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 3478.3367\n",
      "Val Loss: 7675.7208\n",
      "MAE: 58.12\n",
      "\n",
      "Epoch 33/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.48it/s, Loss=5590.2734] \n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 3638.4839\n",
      "Val Loss: 9331.1797\n",
      "MAE: 65.83\n",
      "\n",
      "Epoch 34/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.28it/s, Loss=4025.5542] \n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 3395.1593\n",
      "Val Loss: 7289.9086\n",
      "MAE: 54.76\n",
      "\n",
      "Epoch 35/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.47it/s, Loss=1952.4146] \n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 4409.1664\n",
      "Val Loss: 10487.2523\n",
      "MAE: 68.48\n",
      "\n",
      "Epoch 36/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.56it/s, Loss=5168.2134] \n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 3580.7155\n",
      "Val Loss: 7592.5793\n",
      "MAE: 55.65\n",
      "\n",
      "Epoch 37/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.44it/s, Loss=5320.6187] \n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 4219.9357\n",
      "Val Loss: 9092.4391\n",
      "MAE: 63.40\n",
      "\n",
      "Epoch 38/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.43it/s, Loss=2338.5122]\n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 2817.4882\n",
      "Val Loss: 8004.2764\n",
      "MAE: 54.92\n",
      "\n",
      "Epoch 39/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.47it/s, Loss=2916.2383]\n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 3088.7458\n",
      "Val Loss: 8371.1610\n",
      "MAE: 58.62\n",
      "\n",
      "Epoch 40/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.51it/s, Loss=2650.1313] \n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 2973.2887\n",
      "Val Loss: 9626.2067\n",
      "MAE: 60.45\n",
      "\n",
      "Training completed!\n",
      "Best validation loss: 7289.9086\n",
      "\n",
      "Experiment completed! Results saved to: ../experiments/nutrition5k_experiments/inceptionv3_early_fusion_20251024_132025\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "### EARLY FUSION\n",
    "# Import the necessary modules\n",
    "import sys\n",
    "sys.path.append('/data/projects/punim0478/setiawand/Computer-Vision-COMP90086/src')\n",
    "\n",
    "from nutrition5k_inceptionv3_model import build_nutrition5k_model\n",
    "\n",
    "# Configure experiment settings\n",
    "BATCH_SIZE = 32\n",
    "NUM_EPOCHS = 40\n",
    "DROPOUT_RATE = 0.4\n",
    "LEARNING_RATE = 3e-4\n",
    "WEIGHT_DECAY = 1e-6\n",
    "EARLY_STOPPING_PATIENCE = 15\n",
    "WARMUP_RATIO = 0.1\n",
    "MIN_LR_RATIO = 0.05\n",
    "FUSION_CHANNELS = 2048  # InceptionV3 output channels\n",
    "\n",
    "def train_nutrition5k_model(fusion_type='middle'):\n",
    "    \"\"\"Train the Nutrition5k model with InceptionV3 and specified fusion type\"\"\"\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "    print(f\"TRAINING: Nutrition5k InceptionV3 + {fusion_type.capitalize()} Fusion\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Create datasets\n",
    "    train_dataset = Nutrition5KDataset(\n",
    "        csv_path=train_csv,\n",
    "        data_root=DATA_ROOT,\n",
    "        split='train',\n",
    "        augment=False,\n",
    "        img_size=IMG_SIZE,\n",
    "    )\n",
    "    \n",
    "    val_dataset = Nutrition5KDataset(\n",
    "        csv_path=val_csv,\n",
    "        data_root=DATA_ROOT,\n",
    "        split='val',\n",
    "        augment=False,\n",
    "        img_size=IMG_SIZE,\n",
    "    )\n",
    "    \n",
    "    # Create data loaders\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=True,\n",
    "        num_workers=NUM_WORKERS,\n",
    "        pin_memory=True if torch.cuda.is_available() else False,\n",
    "        drop_last=True\n",
    "    )\n",
    "    \n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=False,\n",
    "        num_workers=NUM_WORKERS,\n",
    "        pin_memory=True if torch.cuda.is_available() else False\n",
    "    )\n",
    "    \n",
    "    # Build model with specified fusion type\n",
    "    model = build_nutrition5k_model(\n",
    "        fusion=fusion_type,\n",
    "        pretrained=False,\n",
    "        dropout_rate=DROPOUT_RATE,\n",
    "        fusion_channels=FUSION_CHANNELS\n",
    "    )\n",
    "    model = model.to(device)\n",
    "    \n",
    "    print(f\"Model parameters: {model.get_num_parameters():,}\")\n",
    "    print(f\"Training samples: {len(train_dataset)}\")\n",
    "    print(f\"Validation samples: {len(val_dataset)}\")\n",
    "    \n",
    "    # Loss function\n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    # Optimizer\n",
    "    optimizer = optim.AdamW(\n",
    "        model.parameters(),\n",
    "        lr=LEARNING_RATE,\n",
    "        weight_decay=WEIGHT_DECAY\n",
    "    )\n",
    "    \n",
    "    print(f\"Learning rate: {LEARNING_RATE}\")\n",
    "    print(f\"Weight decay: {WEIGHT_DECAY}\")\n",
    "    \n",
    "    # Learning rate scheduler\n",
    "    steps_per_epoch = len(train_loader)\n",
    "    total_steps = NUM_EPOCHS * steps_per_epoch\n",
    "    warmup_steps = int(total_steps * WARMUP_RATIO)\n",
    "    \n",
    "    scheduler = get_warmup_cosine_scheduler(\n",
    "        optimizer, \n",
    "        warmup_steps=warmup_steps, \n",
    "        total_steps=total_steps,\n",
    "        min_lr_ratio=MIN_LR_RATIO\n",
    "    )\n",
    "    \n",
    "    # Create experiment directory\n",
    "    exp_name = f\"inceptionv3_{fusion_type}_fusion_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "    exp_dir = os.path.join(OUTPUT_DIR, 'nutrition5k_experiments', exp_name)\n",
    "    os.makedirs(exp_dir, exist_ok=True)\n",
    "    \n",
    "    # Save experiment configuration\n",
    "    config = {\n",
    "        'fusion': fusion_type,\n",
    "        'pretrained': False,\n",
    "        'dropout_rate': DROPOUT_RATE,\n",
    "        'fusion_channels': FUSION_CHANNELS,\n",
    "        'learning_rate': LEARNING_RATE,\n",
    "        'weight_decay': WEIGHT_DECAY,\n",
    "        'batch_size': BATCH_SIZE,\n",
    "        'img_size': IMG_SIZE,\n",
    "        'num_epochs': NUM_EPOCHS\n",
    "    }\n",
    "    \n",
    "    with open(os.path.join(exp_dir, 'config.json'), 'w') as f:\n",
    "        json.dump(config, f, indent=4)\n",
    "    \n",
    "    # Create trainer\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        criterion=criterion,\n",
    "        optimizer=optimizer,\n",
    "        scheduler=scheduler,\n",
    "        device=device,\n",
    "        output_dir=exp_dir,\n",
    "        early_stopping_patience=EARLY_STOPPING_PATIENCE,\n",
    "        scheduler_step_on_batch=False\n",
    "    )\n",
    "    \n",
    "    # Train the model\n",
    "    trainer.train(NUM_EPOCHS)\n",
    "    \n",
    "    print(f\"\\nExperiment completed! Results saved to: {exp_dir}\")\n",
    "    return trainer.best_metrics\n",
    "\n",
    "# Run an experiment with middle fusion\n",
    "middle_fusion_results = train_nutrition5k_model(fusion_type='early')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe967232",
   "metadata": {},
   "source": [
    "### InceptionV3 - Late Fusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b791dc23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "TRAINING: Nutrition5k InceptionV3 + Late Fusion\n",
      "============================================================\n",
      "Loaded 2804 valid samples out of 2805\n",
      "Loaded 495 valid samples out of 495\n",
      "Model parameters: 45,799,745\n",
      "Training samples: 2804\n",
      "Validation samples: 495\n",
      "Learning rate: 0.0003\n",
      "Weight decay: 1e-06\n",
      "Starting training for 40 epochs...\n",
      "\n",
      "Epoch 1/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.40it/s, Loss=106439.0938]\n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 98389.8398\n",
      "Val Loss: 107367.1013\n",
      "MAE: 240.54\n",
      "\n",
      "Epoch 2/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.25it/s, Loss=28412.5645]\n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 41690.2808\n",
      "Val Loss: 31997.4932\n",
      "MAE: 123.44\n",
      "\n",
      "Epoch 3/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.32it/s, Loss=11473.9180]\n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 17350.7568\n",
      "Val Loss: 19154.0181\n",
      "MAE: 99.54\n",
      "\n",
      "Epoch 4/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.25it/s, Loss=10099.2783]\n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 14514.1249\n",
      "Val Loss: 18284.8318\n",
      "MAE: 101.39\n",
      "\n",
      "Epoch 5/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.37it/s, Loss=8358.5098] \n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 13459.6902\n",
      "Val Loss: 13000.7582\n",
      "MAE: 83.92\n",
      "\n",
      "Epoch 6/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:14<00:00,  6.19it/s, Loss=11738.3379]\n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 11558.1775\n",
      "Val Loss: 14764.5721\n",
      "MAE: 93.13\n",
      "\n",
      "Epoch 7/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.37it/s, Loss=5837.0171] \n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 7952.4247\n",
      "Val Loss: 10476.4639\n",
      "MAE: 72.17\n",
      "\n",
      "Epoch 8/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.38it/s, Loss=5708.1650] \n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 6212.3241\n",
      "Val Loss: 10303.3420\n",
      "MAE: 67.74\n",
      "\n",
      "Epoch 9/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.28it/s, Loss=7737.3613] \n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 5673.2633\n",
      "Val Loss: 8837.4902\n",
      "MAE: 63.97\n",
      "\n",
      "Epoch 10/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.35it/s, Loss=9406.4307] \n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 5494.6568\n",
      "Val Loss: 9756.0814\n",
      "MAE: 69.59\n",
      "\n",
      "Epoch 11/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.27it/s, Loss=4610.4297] \n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 5146.7232\n",
      "Val Loss: 8753.4821\n",
      "MAE: 63.10\n",
      "\n",
      "Epoch 12/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.35it/s, Loss=5909.7256] \n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 4907.9320\n",
      "Val Loss: 9570.8997\n",
      "MAE: 65.66\n",
      "\n",
      "Epoch 13/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.37it/s, Loss=4963.5718] \n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 4037.6672\n",
      "Val Loss: 8444.3814\n",
      "MAE: 61.17\n",
      "\n",
      "Epoch 14/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.28it/s, Loss=7843.6504] \n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 4759.5694\n",
      "Val Loss: 9681.8932\n",
      "MAE: 66.88\n",
      "\n",
      "Epoch 15/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.27it/s, Loss=9072.1475] \n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 4229.9857\n",
      "Val Loss: 8599.4233\n",
      "MAE: 62.60\n",
      "\n",
      "Epoch 16/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.22it/s, Loss=2737.9417] \n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 4172.8494\n",
      "Val Loss: 10331.9440\n",
      "MAE: 70.47\n",
      "\n",
      "Epoch 17/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.33it/s, Loss=3295.8223] \n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 4371.7250\n",
      "Val Loss: 8629.5308\n",
      "MAE: 65.55\n",
      "\n",
      "Epoch 18/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.30it/s, Loss=4097.1982] \n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 4424.5678\n",
      "Val Loss: 8790.1743\n",
      "MAE: 65.26\n",
      "\n",
      "Epoch 19/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.38it/s, Loss=2633.3958] \n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 4350.4276\n",
      "Val Loss: 8730.1980\n",
      "MAE: 62.17\n",
      "\n",
      "Epoch 20/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.25it/s, Loss=4055.3335] \n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 4177.5882\n",
      "Val Loss: 14836.8188\n",
      "MAE: 79.11\n",
      "\n",
      "Epoch 21/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.23it/s, Loss=2228.0410] \n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 4229.4994\n",
      "Val Loss: 9068.2230\n",
      "MAE: 66.28\n",
      "\n",
      "Epoch 22/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.30it/s, Loss=1770.4275] \n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 3978.3809\n",
      "Val Loss: 8781.0372\n",
      "MAE: 61.05\n",
      "\n",
      "Epoch 23/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.32it/s, Loss=3093.6758] \n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 3067.3215\n",
      "Val Loss: 8732.8710\n",
      "MAE: 64.11\n",
      "\n",
      "Epoch 24/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.42it/s, Loss=3645.4475] \n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 3630.3558\n",
      "Val Loss: 8726.5923\n",
      "MAE: 62.54\n",
      "\n",
      "Epoch 25/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.28it/s, Loss=1093.2750] \n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 3741.8051\n",
      "Val Loss: 9379.7649\n",
      "MAE: 62.13\n",
      "\n",
      "Epoch 26/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.33it/s, Loss=1971.1019] \n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 3466.8906\n",
      "Val Loss: 8442.0509\n",
      "MAE: 58.94\n",
      "\n",
      "Epoch 27/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.32it/s, Loss=4339.6250] \n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 4190.1686\n",
      "Val Loss: 8390.8783\n",
      "MAE: 63.37\n",
      "\n",
      "Epoch 28/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.33it/s, Loss=2052.4866] \n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 3881.3878\n",
      "Val Loss: 9256.8487\n",
      "MAE: 65.92\n",
      "\n",
      "Epoch 29/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.30it/s, Loss=12281.3994]\n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 3442.6623\n",
      "Val Loss: 7819.5536\n",
      "MAE: 57.46\n",
      "\n",
      "Epoch 30/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.38it/s, Loss=4406.4668] \n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 5363.2380\n",
      "Val Loss: 13414.7968\n",
      "MAE: 77.36\n",
      "\n",
      "Epoch 31/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.43it/s, Loss=4942.8740] \n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 6517.6254\n",
      "Val Loss: 24391.3788\n",
      "MAE: 103.60\n",
      "\n",
      "Epoch 32/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.48it/s, Loss=4491.8784] \n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 5878.0402\n",
      "Val Loss: 10430.6566\n",
      "MAE: 65.04\n",
      "\n",
      "Epoch 33/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.23it/s, Loss=2186.6104] \n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 4494.0169\n",
      "Val Loss: 8213.3772\n",
      "MAE: 59.08\n",
      "\n",
      "Epoch 34/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.37it/s, Loss=7051.7349] \n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 3761.5112\n",
      "Val Loss: 10694.6189\n",
      "MAE: 66.91\n",
      "\n",
      "Epoch 35/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.29it/s, Loss=2668.6448]\n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 3711.7728\n",
      "Val Loss: 7766.9089\n",
      "MAE: 59.69\n",
      "\n",
      "Epoch 36/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.28it/s, Loss=3103.6143] \n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 4424.6941\n",
      "Val Loss: 12087.1532\n",
      "MAE: 74.20\n",
      "\n",
      "Epoch 37/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.33it/s, Loss=3766.8242] \n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 5107.7292\n",
      "Val Loss: 16110.6986\n",
      "MAE: 81.93\n",
      "\n",
      "Epoch 38/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.27it/s, Loss=6010.6538] \n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 4091.8209\n",
      "Val Loss: 8399.1342\n",
      "MAE: 57.81\n",
      "\n",
      "Epoch 39/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.40it/s, Loss=2086.4343] \n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 4500.5079\n",
      "Val Loss: 9125.9881\n",
      "MAE: 62.44\n",
      "\n",
      "Epoch 40/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.41it/s, Loss=1422.8608]\n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 3323.3737\n",
      "Val Loss: 8567.3797\n",
      "MAE: 60.29\n",
      "\n",
      "Training completed!\n",
      "Best validation loss: 7766.9089\n",
      "\n",
      "Experiment completed! Results saved to: ../experiments/nutrition5k_experiments/inceptionv3_late_fusion_20251024_133101\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "### EARLY FUSION\n",
    "# Import the necessary modules\n",
    "import sys\n",
    "sys.path.append('/data/projects/punim0478/setiawand/Computer-Vision-COMP90086/src')\n",
    "\n",
    "from nutrition5k_inceptionv3_model import build_nutrition5k_model\n",
    "\n",
    "# Configure experiment settings\n",
    "BATCH_SIZE = 32\n",
    "NUM_EPOCHS = 40\n",
    "DROPOUT_RATE = 0.4\n",
    "LEARNING_RATE = 3e-4\n",
    "WEIGHT_DECAY = 1e-6\n",
    "EARLY_STOPPING_PATIENCE = 15\n",
    "WARMUP_RATIO = 0.1\n",
    "MIN_LR_RATIO = 0.05\n",
    "FUSION_CHANNELS = 2048  # InceptionV3 output channels\n",
    "\n",
    "def train_nutrition5k_model(fusion_type='middle'):\n",
    "    \"\"\"Train the Nutrition5k model with InceptionV3 and specified fusion type\"\"\"\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "    print(f\"TRAINING: Nutrition5k InceptionV3 + {fusion_type.capitalize()} Fusion\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Create datasets\n",
    "    train_dataset = Nutrition5KDataset(\n",
    "        csv_path=train_csv,\n",
    "        data_root=DATA_ROOT,\n",
    "        split='train',\n",
    "        augment=False,\n",
    "        img_size=IMG_SIZE,\n",
    "    )\n",
    "    \n",
    "    val_dataset = Nutrition5KDataset(\n",
    "        csv_path=val_csv,\n",
    "        data_root=DATA_ROOT,\n",
    "        split='val',\n",
    "        augment=False,\n",
    "        img_size=IMG_SIZE,\n",
    "    )\n",
    "    \n",
    "    # Create data loaders\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=True,\n",
    "        num_workers=NUM_WORKERS,\n",
    "        pin_memory=True if torch.cuda.is_available() else False,\n",
    "        drop_last=True\n",
    "    )\n",
    "    \n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=False,\n",
    "        num_workers=NUM_WORKERS,\n",
    "        pin_memory=True if torch.cuda.is_available() else False\n",
    "    )\n",
    "    \n",
    "    # Build model with specified fusion type\n",
    "    model = build_nutrition5k_model(\n",
    "        fusion=fusion_type,\n",
    "        pretrained=False,\n",
    "        dropout_rate=DROPOUT_RATE,\n",
    "        fusion_channels=FUSION_CHANNELS\n",
    "    )\n",
    "    model = model.to(device)\n",
    "    \n",
    "    print(f\"Model parameters: {model.get_num_parameters():,}\")\n",
    "    print(f\"Training samples: {len(train_dataset)}\")\n",
    "    print(f\"Validation samples: {len(val_dataset)}\")\n",
    "    \n",
    "    # Loss function\n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    # Optimizer\n",
    "    optimizer = optim.AdamW(\n",
    "        model.parameters(),\n",
    "        lr=LEARNING_RATE,\n",
    "        weight_decay=WEIGHT_DECAY\n",
    "    )\n",
    "    \n",
    "    print(f\"Learning rate: {LEARNING_RATE}\")\n",
    "    print(f\"Weight decay: {WEIGHT_DECAY}\")\n",
    "    \n",
    "    # Learning rate scheduler\n",
    "    steps_per_epoch = len(train_loader)\n",
    "    total_steps = NUM_EPOCHS * steps_per_epoch\n",
    "    warmup_steps = int(total_steps * WARMUP_RATIO)\n",
    "    \n",
    "    scheduler = get_warmup_cosine_scheduler(\n",
    "        optimizer, \n",
    "        warmup_steps=warmup_steps, \n",
    "        total_steps=total_steps,\n",
    "        min_lr_ratio=MIN_LR_RATIO\n",
    "    )\n",
    "    \n",
    "    # Create experiment directory\n",
    "    exp_name = f\"inceptionv3_{fusion_type}_fusion_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "    exp_dir = os.path.join(OUTPUT_DIR, 'nutrition5k_experiments', exp_name)\n",
    "    os.makedirs(exp_dir, exist_ok=True)\n",
    "    \n",
    "    # Save experiment configuration\n",
    "    config = {\n",
    "        'fusion': fusion_type,\n",
    "        'pretrained': False,\n",
    "        'dropout_rate': DROPOUT_RATE,\n",
    "        'fusion_channels': FUSION_CHANNELS,\n",
    "        'learning_rate': LEARNING_RATE,\n",
    "        'weight_decay': WEIGHT_DECAY,\n",
    "        'batch_size': BATCH_SIZE,\n",
    "        'img_size': IMG_SIZE,\n",
    "        'num_epochs': NUM_EPOCHS\n",
    "    }\n",
    "    \n",
    "    with open(os.path.join(exp_dir, 'config.json'), 'w') as f:\n",
    "        json.dump(config, f, indent=4)\n",
    "    \n",
    "    # Create trainer\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        criterion=criterion,\n",
    "        optimizer=optimizer,\n",
    "        scheduler=scheduler,\n",
    "        device=device,\n",
    "        output_dir=exp_dir,\n",
    "        early_stopping_patience=EARLY_STOPPING_PATIENCE,\n",
    "        scheduler_step_on_batch=False\n",
    "    )\n",
    "    \n",
    "    # Train the model\n",
    "    trainer.train(NUM_EPOCHS)\n",
    "    \n",
    "    print(f\"\\nExperiment completed! Results saved to: {exp_dir}\")\n",
    "    return trainer.best_metrics\n",
    "\n",
    "# Run an experiment with middle fusion\n",
    "middle_fusion_results = train_nutrition5k_model(fusion_type='late')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b743bb8",
   "metadata": {},
   "source": [
    "### InceptionV3 - Middle + Volume"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52dfc49d",
   "metadata": {},
   "source": [
    "This section implements the food volume estimation method as described in the Nutrition5k paper. The method:\n",
    "\n",
    "1. **Estimates food volume from overhead depth images** using:\n",
    "   - Camera distance: 35.9 cm\n",
    "   - Per-pixel surface area: 5.957 × 10⁻³ cm²\n",
    "   \n",
    "2. **Uses binary foreground/background segmentation** to identify food pixels\n",
    "\n",
    "3. **Calculates volume** by summing per-pixel volumes (depth × surface_area) over all food pixels\n",
    "\n",
    "4. **Concatenates volume estimate** to the InceptionV3 backbone output before FC layers\n",
    "\n",
    "We implement three variants:\n",
    "- **Image-only**: Uses only RGB images (baseline)\n",
    "- **Image+Volume**: RGB + volume estimate as additional signal  \n",
    "- **Middle+Volume**: RGB + Depth fusion + volume estimate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ba819d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "TRAINING: Nutrition5k InceptionV3 + IMAGE_VOLUME\n",
      "============================================================\n",
      "Loaded 2804 valid samples out of 2805\n",
      "Loaded 495 valid samples out of 495\n",
      "Model parameters: 21,916,833\n",
      "Training samples: 2804\n",
      "Validation samples: 495\n",
      "Learning rate: 0.0005\n",
      "Weight decay: 1e-06\n",
      "Starting training for 40 epochs...\n",
      "\n",
      "Epoch 1/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.43it/s, Loss=108259.2578]\n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 98651.2712\n",
      "Val Loss: 107381.6948\n",
      "MAE: 240.57\n",
      "\n",
      "Epoch 2/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.54it/s, Loss=16013.5312] \n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 65919.9727\n",
      "Val Loss: 29319.6537\n",
      "MAE: 118.82\n",
      "\n",
      "Epoch 3/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.54it/s, Loss=21910.8262]\n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 26365.2399\n",
      "Val Loss: 25831.0439\n",
      "MAE: 110.35\n",
      "\n",
      "Epoch 4/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.54it/s, Loss=28824.6973]\n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 19369.3317\n",
      "Val Loss: 26086.2035\n",
      "MAE: 106.78\n",
      "\n",
      "Epoch 5/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.52it/s, Loss=12186.3164]\n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 16064.0011\n",
      "Val Loss: 22472.0131\n",
      "MAE: 101.50\n",
      "\n",
      "Epoch 6/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.43it/s, Loss=7965.6699] \n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 14435.9392\n",
      "Val Loss: 12856.3794\n",
      "MAE: 76.76\n",
      "\n",
      "Epoch 7/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.66it/s, Loss=11353.6094]\n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 14900.7881\n",
      "Val Loss: 16189.5482\n",
      "MAE: 85.86\n",
      "\n",
      "Epoch 8/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.52it/s, Loss=11691.4023]\n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 13149.4884\n",
      "Val Loss: 11781.9545\n",
      "MAE: 76.72\n",
      "\n",
      "Epoch 9/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.55it/s, Loss=11914.6602]\n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 12836.9574\n",
      "Val Loss: 12081.9814\n",
      "MAE: 75.09\n",
      "\n",
      "Epoch 10/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.65it/s, Loss=5833.5947] \n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 12259.6983\n",
      "Val Loss: 34023.9789\n",
      "MAE: 135.16\n",
      "\n",
      "Epoch 11/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.51it/s, Loss=7867.6704] \n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 10627.7787\n",
      "Val Loss: 8857.1537\n",
      "MAE: 64.40\n",
      "\n",
      "Epoch 12/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.45it/s, Loss=8124.0371] \n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 8637.2702\n",
      "Val Loss: 12734.0982\n",
      "MAE: 73.18\n",
      "\n",
      "Epoch 13/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.48it/s, Loss=5287.5591] \n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 12329.5391\n",
      "Val Loss: 11926.0453\n",
      "MAE: 73.37\n",
      "\n",
      "Epoch 14/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.53it/s, Loss=4309.2856] \n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 10885.5472\n",
      "Val Loss: 13142.9994\n",
      "MAE: 78.21\n",
      "\n",
      "Epoch 15/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.51it/s, Loss=20139.8066]\n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 10598.2898\n",
      "Val Loss: 14689.3955\n",
      "MAE: 81.68\n",
      "\n",
      "Epoch 16/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.41it/s, Loss=7086.2871] \n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 8293.4044\n",
      "Val Loss: 10231.8171\n",
      "MAE: 66.58\n",
      "\n",
      "Epoch 17/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.57it/s, Loss=12543.9473]\n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 6399.1024\n",
      "Val Loss: 9016.7161\n",
      "MAE: 60.06\n",
      "\n",
      "Epoch 18/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.55it/s, Loss=5665.5361] \n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 5826.5098\n",
      "Val Loss: 8145.8707\n",
      "MAE: 57.01\n",
      "\n",
      "Epoch 19/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.39it/s, Loss=5068.1343] \n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 7035.3614\n",
      "Val Loss: 9229.9055\n",
      "MAE: 61.73\n",
      "\n",
      "Epoch 20/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.56it/s, Loss=5287.8306] \n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 6057.8679\n",
      "Val Loss: 7616.8182\n",
      "MAE: 56.79\n",
      "\n",
      "Epoch 21/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.43it/s, Loss=4746.9902] \n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 7072.8501\n",
      "Val Loss: 24474.4791\n",
      "MAE: 99.65\n",
      "\n",
      "Epoch 22/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.53it/s, Loss=13119.5117]\n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 7839.2704\n",
      "Val Loss: 10457.4959\n",
      "MAE: 65.65\n",
      "\n",
      "Epoch 23/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.62it/s, Loss=3085.6042] \n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 6021.8229\n",
      "Val Loss: 7191.0435\n",
      "MAE: 55.98\n",
      "\n",
      "Epoch 24/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.61it/s, Loss=6064.8857] \n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 7473.2339\n",
      "Val Loss: 9309.3742\n",
      "MAE: 63.62\n",
      "\n",
      "Epoch 25/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.55it/s, Loss=3918.4976] \n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 6366.5307\n",
      "Val Loss: 7951.3516\n",
      "MAE: 57.45\n",
      "\n",
      "Epoch 26/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.59it/s, Loss=3958.7363] \n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 5714.5977\n",
      "Val Loss: 8676.6589\n",
      "MAE: 61.97\n",
      "\n",
      "Epoch 27/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.59it/s, Loss=4025.3110] \n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 6011.6670\n",
      "Val Loss: 8741.7768\n",
      "MAE: 62.08\n",
      "\n",
      "Epoch 28/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.63it/s, Loss=3402.2810] \n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 5449.7801\n",
      "Val Loss: 9306.1301\n",
      "MAE: 59.73\n",
      "\n",
      "Epoch 29/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.50it/s, Loss=3335.1790] \n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 4545.7973\n",
      "Val Loss: 7175.3438\n",
      "MAE: 54.02\n",
      "\n",
      "Epoch 30/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.53it/s, Loss=10892.3789]\n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 7070.0363\n",
      "Val Loss: 13045.5674\n",
      "MAE: 77.76\n",
      "\n",
      "Epoch 31/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.51it/s, Loss=5397.4434] \n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 6838.4158\n",
      "Val Loss: 10400.0666\n",
      "MAE: 64.40\n",
      "\n",
      "Epoch 32/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.44it/s, Loss=4656.6797] \n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 5807.6003\n",
      "Val Loss: 8157.4585\n",
      "MAE: 59.51\n",
      "\n",
      "Epoch 33/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.52it/s, Loss=1558.7113] \n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 5879.0825\n",
      "Val Loss: 8046.5217\n",
      "MAE: 57.06\n",
      "\n",
      "Epoch 34/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.62it/s, Loss=7381.2354] \n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 6066.1867\n",
      "Val Loss: 9070.3390\n",
      "MAE: 60.64\n",
      "\n",
      "Epoch 35/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.55it/s, Loss=3927.4146] \n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 4413.8461\n",
      "Val Loss: 7681.4189\n",
      "MAE: 54.37\n",
      "\n",
      "Epoch 36/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.51it/s, Loss=9086.3516] \n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 5761.2919\n",
      "Val Loss: 9406.7413\n",
      "MAE: 65.93\n",
      "\n",
      "Epoch 37/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.50it/s, Loss=6872.7788] \n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 4781.9159\n",
      "Val Loss: 7606.9587\n",
      "MAE: 55.86\n",
      "\n",
      "Epoch 38/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.54it/s, Loss=6143.3491] \n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 5480.3070\n",
      "Val Loss: 10385.1789\n",
      "MAE: 63.28\n",
      "\n",
      "Epoch 39/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.49it/s, Loss=3758.9685] \n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 4822.6640\n",
      "Val Loss: 7814.0743\n",
      "MAE: 54.46\n",
      "\n",
      "Epoch 40/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.51it/s, Loss=2849.0405] \n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 5745.1829\n",
      "Val Loss: 9136.1897\n",
      "MAE: 59.73\n",
      "\n",
      "Training completed!\n",
      "Best validation loss: 7175.3438\n",
      "\n",
      "Experiment completed! Results saved to: ../experiments/nutrition5k_experiments/inceptionv3_image_volume_20251024_140149\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('/data/projects/punim0478/setiawand/Computer-Vision-COMP90086/src')\n",
    "\n",
    "from nutrition5k_inceptionv3_model import build_nutrition5k_model\n",
    "\n",
    "# Configure experiment settings\n",
    "BATCH_SIZE = 32\n",
    "NUM_EPOCHS = 40\n",
    "DROPOUT_RATE = 0.4\n",
    "LEARNING_RATE = 5e-4\n",
    "WEIGHT_DECAY = 1e-6\n",
    "EARLY_STOPPING_PATIENCE = 15\n",
    "WARMUP_RATIO = 0.1\n",
    "MIN_LR_RATIO = 0.05\n",
    "FUSION_CHANNELS = 2048  # InceptionV3 output channels\n",
    "\n",
    "# Training function with volume estimation support\n",
    "def train_nutrition5k_with_volume(fusion_type='image_volume'):\n",
    "    \"\"\"\n",
    "    Train the Nutrition5k model with volume estimation\n",
    "    \n",
    "    Args:\n",
    "        fusion_type: 'image_only', 'image_volume', 'middle', etc.\n",
    "        use_segmentation: Whether to use learned segmentation for volume estimation\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "    print(f\"TRAINING: Nutrition5k InceptionV3 + {fusion_type.upper()}\")\n",
    "\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Create datasets\n",
    "    train_dataset = Nutrition5KDataset(\n",
    "        csv_path=train_csv,\n",
    "        data_root=DATA_ROOT,\n",
    "        split='train',\n",
    "        augment=False,\n",
    "        img_size=IMG_SIZE,\n",
    "    )\n",
    "    \n",
    "    val_dataset = Nutrition5KDataset(\n",
    "        csv_path=val_csv,\n",
    "        data_root=DATA_ROOT,\n",
    "        split='val',\n",
    "        augment=False,\n",
    "        img_size=IMG_SIZE,\n",
    "    )\n",
    "    \n",
    "    # Create data loaders\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=True,\n",
    "        num_workers=NUM_WORKERS,\n",
    "        pin_memory=True if torch.cuda.is_available() else False,\n",
    "        drop_last=True\n",
    "    )\n",
    "    \n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=False,\n",
    "        num_workers=NUM_WORKERS,\n",
    "        pin_memory=True if torch.cuda.is_available() else False\n",
    "    )\n",
    "    \n",
    "    # Build model with volume estimation\n",
    "    use_volume = 'volume' in fusion_type\n",
    "    \n",
    "    model = build_nutrition5k_model(\n",
    "        fusion=fusion_type,\n",
    "        pretrained=False,\n",
    "        dropout_rate=DROPOUT_RATE,\n",
    "        fusion_channels=FUSION_CHANNELS,\n",
    "        use_volume=use_volume,\n",
    "    )\n",
    "    model = model.to(device)\n",
    "    \n",
    "    print(f\"Model parameters: {model.get_num_parameters():,}\")\n",
    "    print(f\"Training samples: {len(train_dataset)}\")\n",
    "    print(f\"Validation samples: {len(val_dataset)}\")\n",
    "    \n",
    "    # Loss function\n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    # Optimizer\n",
    "    optimizer = optim.AdamW(\n",
    "        model.parameters(),\n",
    "        lr=LEARNING_RATE,\n",
    "        weight_decay=WEIGHT_DECAY\n",
    "    )\n",
    "    \n",
    "    print(f\"Learning rate: {LEARNING_RATE}\")\n",
    "    print(f\"Weight decay: {WEIGHT_DECAY}\")\n",
    "    \n",
    "    # Learning rate scheduler\n",
    "    steps_per_epoch = len(train_loader)\n",
    "    total_steps = NUM_EPOCHS * steps_per_epoch\n",
    "    warmup_steps = int(total_steps * WARMUP_RATIO)\n",
    "    \n",
    "    scheduler = get_warmup_cosine_scheduler(\n",
    "        optimizer, \n",
    "        warmup_steps=warmup_steps, \n",
    "        total_steps=total_steps,\n",
    "        min_lr_ratio=MIN_LR_RATIO\n",
    "    )\n",
    "    \n",
    "    # Create experiment directory\n",
    "    exp_name = f\"inceptionv3_{fusion_type}_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "    exp_dir = os.path.join(OUTPUT_DIR, 'nutrition5k_experiments', exp_name)\n",
    "    os.makedirs(exp_dir, exist_ok=True)\n",
    "    \n",
    "    # Save experiment configuration\n",
    "    config = {\n",
    "        'fusion': fusion_type,\n",
    "        'use_volume': use_volume,\n",
    "        'pretrained': False,\n",
    "        'dropout_rate': DROPOUT_RATE,\n",
    "        'fusion_channels': FUSION_CHANNELS,\n",
    "        'learning_rate': LEARNING_RATE,\n",
    "        'weight_decay': WEIGHT_DECAY,\n",
    "        'batch_size': BATCH_SIZE,\n",
    "        'img_size': IMG_SIZE,\n",
    "        'num_epochs': NUM_EPOCHS\n",
    "    }\n",
    "    \n",
    "    with open(os.path.join(exp_dir, 'config.json'), 'w') as f:\n",
    "        json.dump(config, f, indent=4)\n",
    "    \n",
    "    # Create trainer\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        criterion=criterion,\n",
    "        optimizer=optimizer,\n",
    "        scheduler=scheduler,\n",
    "        device=device,\n",
    "        output_dir=exp_dir,\n",
    "        early_stopping_patience=EARLY_STOPPING_PATIENCE,\n",
    "        scheduler_step_on_batch=False\n",
    "    )\n",
    "    \n",
    "    # Train the model\n",
    "    trainer.train(NUM_EPOCHS)\n",
    "    \n",
    "    print(f\"\\nExperiment completed! Results saved to: {exp_dir}\")\n",
    "    return trainer.best_metrics\n",
    "\n",
    "\n",
    "middle_fusion_results = train_nutrition5k_with_volume(fusion_type='image_volume')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aedf6ded",
   "metadata": {},
   "source": [
    "# Experimental Improvements\n",
    "\n",
    "This section contains experiments to further improve the InceptionV3 + Volume model:\n",
    "\n",
    "1. **Huber Loss Experiment**: Test Huber loss for better robustness to outliers\n",
    "2. **Deeper Volume Head**: Try more complex volume processing  \n",
    "3. **Hyperparameter Tuning**: Grid search for optimal learning rates and dropout\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "79d7dbbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure experiment settings\n",
    "BATCH_SIZE = 32\n",
    "NUM_EPOCHS = 45\n",
    "DROPOUT_RATE = 0.4\n",
    "LEARNING_RATE = 3e-4\n",
    "WEIGHT_DECAY = 1e-6\n",
    "EARLY_STOPPING_PATIENCE = 15\n",
    "WARMUP_RATIO = 0.1\n",
    "MIN_LR_RATIO = 0.05\n",
    "FUSION_CHANNELS = 2048  # InceptionV3 output channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dd55df03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Huber Loss Experiment...\n",
      "============================================================\n",
      "EXPERIMENT 1: HUBER LOSS (delta=50.0)\n",
      "TRAINING: Nutrition5k InceptionV3 + IMAGE_VOLUME\n",
      "============================================================\n",
      "Loaded 2804 valid samples out of 2805\n",
      "Loaded 495 valid samples out of 495\n",
      "Model parameters: 21,916,833\n",
      "Training samples: 2804\n",
      "Validation samples: 495\n",
      "Using Huber Loss with delta=50.0\n",
      "Learning rate: 0.0005\n",
      "Weight decay: 1e-06\n",
      "Starting training for 40 epochs...\n",
      "\n",
      "Epoch 1/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.24it/s, Loss=11141.7168]\n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 10643.9618\n",
      "Val Loss: 10846.3843\n",
      "MAE: 240.63\n",
      "\n",
      "Epoch 2/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.53it/s, Loss=9204.0020] \n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 10151.3070\n",
      "Val Loss: 9631.6149\n",
      "MAE: 215.63\n",
      "\n",
      "Epoch 3/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.56it/s, Loss=10808.7344]\n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 9478.4009\n",
      "Val Loss: 9529.5171\n",
      "MAE: 213.65\n",
      "\n",
      "Epoch 4/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.42it/s, Loss=10174.6729]\n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 9230.1581\n",
      "Val Loss: 9379.8687\n",
      "MAE: 210.64\n",
      "\n",
      "Epoch 5/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.57it/s, Loss=10531.4971]\n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 8900.4787\n",
      "Val Loss: 9118.2562\n",
      "MAE: 205.48\n",
      "\n",
      "Epoch 6/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.49it/s, Loss=7289.9009] \n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 8332.7923\n",
      "Val Loss: 8381.4564\n",
      "MAE: 190.42\n",
      "\n",
      "Epoch 7/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.53it/s, Loss=7030.3945] \n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 7072.8830\n",
      "Val Loss: 7461.7495\n",
      "MAE: 171.75\n",
      "\n",
      "Epoch 8/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.52it/s, Loss=6245.6543]\n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 4965.8345\n",
      "Val Loss: 5192.3348\n",
      "MAE: 124.99\n",
      "\n",
      "Epoch 9/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.46it/s, Loss=3681.1890]\n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 3773.1540\n",
      "Val Loss: 3606.7987\n",
      "MAE: 92.84\n",
      "\n",
      "Epoch 10/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.48it/s, Loss=2393.3779]\n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 3159.9073\n",
      "Val Loss: 3098.9756\n",
      "MAE: 82.67\n",
      "\n",
      "Epoch 11/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.52it/s, Loss=1396.4508]\n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 2947.5262\n",
      "Val Loss: 3013.1390\n",
      "MAE: 79.94\n",
      "\n",
      "Epoch 12/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.53it/s, Loss=2545.4194]\n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 2838.2407\n",
      "Val Loss: 2768.4688\n",
      "MAE: 74.87\n",
      "\n",
      "Epoch 13/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.52it/s, Loss=3589.2791]\n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 2793.4855\n",
      "Val Loss: 3229.7859\n",
      "MAE: 84.37\n",
      "\n",
      "Epoch 14/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.37it/s, Loss=1428.6729]\n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 2487.2035\n",
      "Val Loss: 2538.7700\n",
      "MAE: 69.70\n",
      "\n",
      "Epoch 15/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.60it/s, Loss=2851.4927]\n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 2608.0847\n",
      "Val Loss: 3071.9181\n",
      "MAE: 81.89\n",
      "\n",
      "Epoch 16/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.55it/s, Loss=1950.3076]\n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 2385.1723\n",
      "Val Loss: 2486.9775\n",
      "MAE: 68.48\n",
      "\n",
      "Epoch 17/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.58it/s, Loss=1615.8881]\n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 2496.5160\n",
      "Val Loss: 2509.6280\n",
      "MAE: 69.44\n",
      "\n",
      "Epoch 18/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.49it/s, Loss=2152.5427]\n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 2324.5128\n",
      "Val Loss: 3304.7643\n",
      "MAE: 86.08\n",
      "\n",
      "Epoch 19/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.52it/s, Loss=1859.8516]\n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 2095.4323\n",
      "Val Loss: 2286.5796\n",
      "MAE: 64.52\n",
      "\n",
      "Epoch 20/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.46it/s, Loss=2066.9907]\n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 2333.1052\n",
      "Val Loss: 2695.4604\n",
      "MAE: 74.18\n",
      "\n",
      "Epoch 21/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.56it/s, Loss=2261.9521]\n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 2179.2359\n",
      "Val Loss: 2249.1090\n",
      "MAE: 64.53\n",
      "\n",
      "Epoch 22/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.51it/s, Loss=2182.2490]\n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 2240.3247\n",
      "Val Loss: 2836.9266\n",
      "MAE: 75.27\n",
      "\n",
      "Epoch 23/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.47it/s, Loss=1446.5403]\n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 2046.7043\n",
      "Val Loss: 2138.6093\n",
      "MAE: 62.00\n",
      "\n",
      "Epoch 24/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.40it/s, Loss=3039.7432]\n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 2263.7368\n",
      "Val Loss: 2653.1952\n",
      "MAE: 73.37\n",
      "\n",
      "Epoch 25/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.49it/s, Loss=1897.3691]\n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 2049.3667\n",
      "Val Loss: 2271.7062\n",
      "MAE: 63.93\n",
      "\n",
      "Epoch 26/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.54it/s, Loss=1840.9064]\n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 2214.9147\n",
      "Val Loss: 3012.9296\n",
      "MAE: 80.01\n",
      "\n",
      "Epoch 27/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.53it/s, Loss=2094.4614]\n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1915.2849\n",
      "Val Loss: 2072.4470\n",
      "MAE: 60.07\n",
      "\n",
      "Epoch 28/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.53it/s, Loss=3338.6953]\n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 2232.6491\n",
      "Val Loss: 2616.8171\n",
      "MAE: 71.86\n",
      "\n",
      "Epoch 29/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.52it/s, Loss=2330.6382]\n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1983.3744\n",
      "Val Loss: 2062.5206\n",
      "MAE: 59.64\n",
      "\n",
      "Epoch 30/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.48it/s, Loss=2038.4250]\n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 2134.1820\n",
      "Val Loss: 2796.7276\n",
      "MAE: 75.07\n",
      "\n",
      "Epoch 31/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.57it/s, Loss=996.3857] \n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1812.5660\n",
      "Val Loss: 2091.8806\n",
      "MAE: 60.41\n",
      "\n",
      "Epoch 32/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.50it/s, Loss=2217.8608]\n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1990.8698\n",
      "Val Loss: 2499.2729\n",
      "MAE: 70.51\n",
      "\n",
      "Epoch 33/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.55it/s, Loss=1992.0198]\n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1941.7687\n",
      "Val Loss: 2216.7550\n",
      "MAE: 62.45\n",
      "\n",
      "Epoch 34/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.56it/s, Loss=2561.9055]\n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1821.1843\n",
      "Val Loss: 2275.5338\n",
      "MAE: 64.55\n",
      "\n",
      "Epoch 35/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.48it/s, Loss=2569.1079]\n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1839.2868\n",
      "Val Loss: 2906.0326\n",
      "MAE: 78.39\n",
      "\n",
      "Epoch 36/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.47it/s, Loss=2263.5874]\n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1709.4974\n",
      "Val Loss: 1982.4431\n",
      "MAE: 57.70\n",
      "\n",
      "Epoch 37/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.51it/s, Loss=1369.8840]\n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1733.9051\n",
      "Val Loss: 2381.8116\n",
      "MAE: 66.48\n",
      "\n",
      "Epoch 38/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.51it/s, Loss=1288.5786]\n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1811.4912\n",
      "Val Loss: 2180.0294\n",
      "MAE: 61.85\n",
      "\n",
      "Epoch 39/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.53it/s, Loss=2111.2681]\n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1729.3645\n",
      "Val Loss: 2212.7875\n",
      "MAE: 62.56\n",
      "\n",
      "Epoch 40/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.36it/s, Loss=1701.7386]\n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1793.5782\n",
      "Val Loss: 3240.4079\n",
      "MAE: 84.43\n",
      "\n",
      "Training completed!\n",
      "Best validation loss: 1982.4431\n",
      "\n",
      "Experiment 1 (Huber Loss) completed! Results saved to: ../experiments/nutrition5k_experiments/inceptionv3_image_volume_huber50.0_20251024_144307\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# EXPERIMENT 1: HUBER LOSS\n",
    "# Test Huber loss for better robustness to outliers\n",
    "\n",
    "def train_nutrition5k_huber_loss(fusion_type='image_volume', huber_delta=50.0):\n",
    "    \"\"\"\n",
    "    Train the Nutrition5k model with Huber loss instead of MSE\n",
    "    \n",
    "    Args:\n",
    "        fusion_type: 'image_only', 'image_volume', 'middle', etc.\n",
    "        huber_delta: Delta parameter for Huber loss (transition point between L1 and L2)\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "    print(f\"EXPERIMENT 1: HUBER LOSS (delta={huber_delta})\")\n",
    "    print(f\"TRAINING: Nutrition5k InceptionV3 + {fusion_type.upper()}\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Create datasets (same as before)\n",
    "    train_dataset = Nutrition5KDataset(\n",
    "        csv_path=train_csv,\n",
    "        data_root=DATA_ROOT,\n",
    "        split='train',\n",
    "        augment=False,\n",
    "        img_size=IMG_SIZE,\n",
    "    )\n",
    "    \n",
    "    val_dataset = Nutrition5KDataset(\n",
    "        csv_path=val_csv,\n",
    "        data_root=DATA_ROOT,\n",
    "        split='val',\n",
    "        augment=False,\n",
    "        img_size=IMG_SIZE,\n",
    "    )\n",
    "    \n",
    "    # Create data loaders\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=True,\n",
    "        num_workers=NUM_WORKERS,\n",
    "        pin_memory=True if torch.cuda.is_available() else False,\n",
    "        drop_last=True\n",
    "    )\n",
    "    \n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=False,\n",
    "        num_workers=NUM_WORKERS,\n",
    "        pin_memory=True if torch.cuda.is_available() else False\n",
    "    )\n",
    "    \n",
    "    # Build model\n",
    "    use_volume = 'volume' in fusion_type\n",
    "    \n",
    "    model = build_nutrition5k_model(\n",
    "        fusion=fusion_type,\n",
    "        pretrained=False,\n",
    "        dropout_rate=DROPOUT_RATE,\n",
    "        fusion_channels=FUSION_CHANNELS,\n",
    "        use_volume=use_volume,\n",
    "    )\n",
    "    model = model.to(device)\n",
    "    \n",
    "    print(f\"Model parameters: {model.get_num_parameters():,}\")\n",
    "    print(f\"Training samples: {len(train_dataset)}\")\n",
    "    print(f\"Validation samples: {len(val_dataset)}\")\n",
    "    \n",
    "    # HUBER LOSS instead of MSE\n",
    "    criterion = nn.HuberLoss(delta=huber_delta)\n",
    "    print(f\"Using Huber Loss with delta={huber_delta}\")\n",
    "    \n",
    "    # Optimizer\n",
    "    optimizer = optim.AdamW(\n",
    "        model.parameters(),\n",
    "        lr=LEARNING_RATE,\n",
    "        weight_decay=WEIGHT_DECAY\n",
    "    )\n",
    "    \n",
    "    print(f\"Learning rate: {LEARNING_RATE}\")\n",
    "    print(f\"Weight decay: {WEIGHT_DECAY}\")\n",
    "    \n",
    "    # Learning rate scheduler\n",
    "    steps_per_epoch = len(train_loader)\n",
    "    total_steps = NUM_EPOCHS * steps_per_epoch\n",
    "    warmup_steps = int(total_steps * WARMUP_RATIO)\n",
    "    \n",
    "    scheduler = get_warmup_cosine_scheduler(\n",
    "        optimizer, \n",
    "        warmup_steps=warmup_steps, \n",
    "        total_steps=total_steps,\n",
    "        min_lr_ratio=MIN_LR_RATIO\n",
    "    )\n",
    "    \n",
    "    # Create experiment directory\n",
    "    exp_name = f\"inceptionv3_{fusion_type}_huber{huber_delta}_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "    exp_dir = os.path.join(OUTPUT_DIR, 'nutrition5k_experiments', exp_name)\n",
    "    os.makedirs(exp_dir, exist_ok=True)\n",
    "    \n",
    "    # Save experiment configuration\n",
    "    config = {\n",
    "        'fusion': fusion_type,\n",
    "        'use_volume': use_volume,\n",
    "        'loss_function': 'huber',\n",
    "        'huber_delta': huber_delta,\n",
    "        'pretrained': False,\n",
    "        'dropout_rate': DROPOUT_RATE,\n",
    "        'fusion_channels': FUSION_CHANNELS,\n",
    "        'learning_rate': LEARNING_RATE,\n",
    "        'weight_decay': WEIGHT_DECAY,\n",
    "        'batch_size': BATCH_SIZE,\n",
    "        'img_size': IMG_SIZE,\n",
    "        'num_epochs': NUM_EPOCHS\n",
    "    }\n",
    "    \n",
    "    with open(os.path.join(exp_dir, 'config.json'), 'w') as f:\n",
    "        json.dump(config, f, indent=4)\n",
    "    \n",
    "    # Create trainer\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        criterion=criterion,\n",
    "        optimizer=optimizer,\n",
    "        scheduler=scheduler,\n",
    "        device=device,\n",
    "        output_dir=exp_dir,\n",
    "        early_stopping_patience=EARLY_STOPPING_PATIENCE,\n",
    "        scheduler_step_on_batch=False\n",
    "    )\n",
    "    \n",
    "    # Train the model\n",
    "    trainer.train(NUM_EPOCHS)\n",
    "    \n",
    "    print(f\"\\nExperiment 1 (Huber Loss) completed! Results saved to: {exp_dir}\")\n",
    "    return trainer.best_metrics\n",
    "\n",
    "# Run Huber Loss experiment\n",
    "print(\"Starting Huber Loss Experiment...\")\n",
    "huber_results = train_nutrition5k_huber_loss(fusion_type='image_volume', huber_delta=50.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3dd648ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deep Volume Head model defined successfully!\n",
      "DeepRegressionHeadWithVolume: Processes volume through 4-layer network\n",
      "Main improvements: Volume → Linear(16) → Linear(8) → Linear(4) → Concat → Deep FC layers\n"
     ]
    }
   ],
   "source": [
    "# EXPERIMENT 2: DEEPER VOLUME HEAD\n",
    "# Try more complex volume processing with deeper neural networks\n",
    "\n",
    "# First, let's create an enhanced model with deeper volume head\n",
    "import sys\n",
    "sys.path.append('/data/projects/punim0478/setiawand/Computer-Vision-COMP90086/src')\n",
    "\n",
    "# We need to modify the RegressionHeadWithVolume class\n",
    "class DeepRegressionHeadWithVolume(nn.Module):\n",
    "    \"\"\"\n",
    "    Enhanced regression head with deeper volume processing\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, in_channels: int = 2048, dropout_rate: float = 0.4):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        \n",
    "        # Volume preprocessing network\n",
    "        self.volume_transform = nn.Sequential(\n",
    "            nn.Linear(1, 16),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(dropout_rate * 0.5),\n",
    "            nn.Linear(16, 8),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(8, 4),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "        # Deeper FC layers for feature + volume fusion\n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(in_channels + 4, 256),  # +4 for processed volume\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(dropout_rate * 0.7),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(dropout_rate * 0.5),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, features, volume):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            features: Feature maps from backbone (B, 2048, H, W)\n",
    "            volume: Volume estimates (B, 1)\n",
    "        \n",
    "        Returns:\n",
    "            Predicted calories (B, 1)\n",
    "        \"\"\"\n",
    "        # Global average pooling\n",
    "        x = self.avgpool(features)  # (B, 2048, 1, 1)\n",
    "        x = torch.flatten(x, 1)  # (B, 2048)\n",
    "        \n",
    "        # Process volume through deeper network\n",
    "        volume_processed = self.volume_transform(volume)  # (B, 4)\n",
    "        \n",
    "        # Concatenate processed volume\n",
    "        x = torch.cat([x, volume_processed], dim=1)  # (B, 2052)\n",
    "        \n",
    "        # Predict calories\n",
    "        x = self.fc_layers(x)  # (B, 1)\n",
    "        \n",
    "        return x\n",
    "\n",
    "# Create a version of our model with the deeper volume head\n",
    "class DeepVolumeNutrition5kModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Nutrition5k model with deeper volume processing\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        fusion: str = 'image_volume',\n",
    "        fusion_channels: int = 2048,\n",
    "        dropout_rate: float = 0.4,\n",
    "        pretrained: bool = False,\n",
    "        use_volume: bool = True\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.use_volume = use_volume\n",
    "        self.fusion = fusion\n",
    "        \n",
    "        if fusion == 'image_volume':\n",
    "            # Image+Volume variant with deep volume head\n",
    "            from nutrition5k_inceptionv3_model import InceptionV3Encoder, VolumeEstimator\n",
    "            \n",
    "            self.rgb_encoder = InceptionV3Encoder(pretrained=pretrained, in_channels=3)\n",
    "            self.volume_estimator = VolumeEstimator()\n",
    "            self.regression_head = DeepRegressionHeadWithVolume(\n",
    "                in_channels=self.rgb_encoder.out_channels,\n",
    "                dropout_rate=dropout_rate\n",
    "            )\n",
    "        else:\n",
    "            raise NotImplementedError(f\"Deep volume head only implemented for image_volume fusion\")\n",
    "    \n",
    "    def forward(self, rgb, depth):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            rgb: RGB images (B, 3, H, W)\n",
    "            depth: Depth images (B, 1, H, W)\n",
    "        \n",
    "        Returns:\n",
    "            calorie_pred: Predicted calories (B, 1)\n",
    "        \"\"\"\n",
    "        # Calculate volume estimate\n",
    "        volume = self.volume_estimator(depth)  # (B, 1)\n",
    "        \n",
    "        # Extract RGB features\n",
    "        rgb_features = self.rgb_encoder(rgb)  # (B, 2048, H/32, W/32)\n",
    "        \n",
    "        # Predict with deep volume processing\n",
    "        calorie_pred = self.regression_head(rgb_features, volume)\n",
    "        \n",
    "        return calorie_pred\n",
    "    \n",
    "    def get_num_parameters(self):\n",
    "        \"\"\"Get total number of trainable parameters\"\"\"\n",
    "        return sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
    "\n",
    "print(\"Deep Volume Head model defined successfully!\")\n",
    "print(\"DeepRegressionHeadWithVolume: Processes volume through 4-layer network\")\n",
    "print(\"Main improvements: Volume → Linear(16) → Linear(8) → Linear(4) → Concat → Deep FC layers\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fcc72e21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Deep Volume Head Experiment...\n",
      "============================================================\n",
      "EXPERIMENT 2: DEEP VOLUME HEAD\n",
      "TRAINING: Nutrition5k InceptionV3 + Deep Volume Processing\n",
      "============================================================\n",
      "Loaded 2804 valid samples out of 2805\n",
      "Loaded 495 valid samples out of 495\n",
      "Model parameters: 22,352,557\n",
      "Training samples: 2804\n",
      "Validation samples: 495\n",
      "Deep Volume Processing: Volume → 16 → 8 → 4 → Concat with features\n",
      "Learning rate: 0.0003\n",
      "Weight decay: 1e-06\n",
      "Starting training for 45 epochs...\n",
      "\n",
      "Epoch 1/45\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:14<00:00,  6.21it/s, Loss=80366.6875] \n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 99205.7511\n",
      "Val Loss: 107435.8311\n",
      "MAE: 240.67\n",
      "\n",
      "Epoch 2/45\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.30it/s, Loss=42767.6406] \n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 62462.6107\n",
      "Val Loss: 26640.4976\n",
      "MAE: 111.19\n",
      "\n",
      "Epoch 3/45\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.48it/s, Loss=19693.2227]\n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 17812.8964\n",
      "Val Loss: 18368.4536\n",
      "MAE: 93.13\n",
      "\n",
      "Epoch 4/45\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.42it/s, Loss=6996.1182] \n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 14158.8240\n",
      "Val Loss: 13956.9041\n",
      "MAE: 84.27\n",
      "\n",
      "Epoch 5/45\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.41it/s, Loss=36037.9648]\n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 17312.7492\n",
      "Val Loss: 16704.5676\n",
      "MAE: 89.29\n",
      "\n",
      "Epoch 6/45\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.32it/s, Loss=10720.8262]\n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 14583.0168\n",
      "Val Loss: 19564.9041\n",
      "MAE: 93.75\n",
      "\n",
      "Epoch 7/45\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.46it/s, Loss=10871.3320]\n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 12611.0272\n",
      "Val Loss: 14169.0002\n",
      "MAE: 80.21\n",
      "\n",
      "Epoch 8/45\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.49it/s, Loss=9407.8428] \n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 15297.1831\n",
      "Val Loss: 20645.2300\n",
      "MAE: 97.14\n",
      "\n",
      "Epoch 9/45\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.46it/s, Loss=19601.9004]\n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 15043.8423\n",
      "Val Loss: 17701.0334\n",
      "MAE: 86.46\n",
      "\n",
      "Epoch 10/45\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.43it/s, Loss=13169.5557]\n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 9831.7051\n",
      "Val Loss: 10827.0071\n",
      "MAE: 70.75\n",
      "\n",
      "Epoch 11/45\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.31it/s, Loss=8325.3926] \n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 9250.7783\n",
      "Val Loss: 11669.3208\n",
      "MAE: 71.77\n",
      "\n",
      "Epoch 12/45\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.33it/s, Loss=7180.2148] \n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 9096.3642\n",
      "Val Loss: 13502.6510\n",
      "MAE: 75.82\n",
      "\n",
      "Epoch 13/45\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.51it/s, Loss=21630.8555]\n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 11536.0411\n",
      "Val Loss: 16316.4424\n",
      "MAE: 88.31\n",
      "\n",
      "Epoch 14/45\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.47it/s, Loss=8692.0693] \n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 10891.4717\n",
      "Val Loss: 13009.5298\n",
      "MAE: 79.27\n",
      "\n",
      "Epoch 15/45\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.41it/s, Loss=6204.7344] \n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 9798.0848\n",
      "Val Loss: 11309.1967\n",
      "MAE: 71.77\n",
      "\n",
      "Epoch 16/45\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.40it/s, Loss=6890.5459] \n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 7931.4332\n",
      "Val Loss: 9796.1152\n",
      "MAE: 63.66\n",
      "\n",
      "Epoch 17/45\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.46it/s, Loss=4032.2761] \n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 7630.1566\n",
      "Val Loss: 10407.0497\n",
      "MAE: 65.29\n",
      "\n",
      "Epoch 18/45\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.38it/s, Loss=3764.5483] \n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 5977.9989\n",
      "Val Loss: 8238.3538\n",
      "MAE: 58.84\n",
      "\n",
      "Epoch 19/45\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.51it/s, Loss=10124.8330]\n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 9647.7231\n",
      "Val Loss: 25826.2465\n",
      "MAE: 103.16\n",
      "\n",
      "Epoch 20/45\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.44it/s, Loss=7833.0483] \n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 7892.4448\n",
      "Val Loss: 9729.6092\n",
      "MAE: 63.32\n",
      "\n",
      "Epoch 21/45\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.51it/s, Loss=8804.7959] \n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 6628.5629\n",
      "Val Loss: 9416.9252\n",
      "MAE: 62.84\n",
      "\n",
      "Epoch 22/45\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.41it/s, Loss=9127.1973] \n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 6950.6162\n",
      "Val Loss: 11039.9025\n",
      "MAE: 66.52\n",
      "\n",
      "Epoch 23/45\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.45it/s, Loss=3264.8909] \n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 6053.4310\n",
      "Val Loss: 8304.7925\n",
      "MAE: 57.59\n",
      "\n",
      "Epoch 24/45\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.45it/s, Loss=4391.6738] \n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 9047.5831\n",
      "Val Loss: 11573.5070\n",
      "MAE: 72.88\n",
      "\n",
      "Epoch 25/45\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.53it/s, Loss=3230.2251] \n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 6986.5248\n",
      "Val Loss: 8468.6087\n",
      "MAE: 58.40\n",
      "\n",
      "Epoch 26/45\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.53it/s, Loss=3686.1116] \n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 7756.2643\n",
      "Val Loss: 10940.5167\n",
      "MAE: 68.32\n",
      "\n",
      "Epoch 27/45\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.41it/s, Loss=5126.8564] \n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 6363.6007\n",
      "Val Loss: 8172.4353\n",
      "MAE: 59.49\n",
      "\n",
      "Epoch 28/45\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.56it/s, Loss=8081.7939] \n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 8297.1073\n",
      "Val Loss: 11520.4989\n",
      "MAE: 69.72\n",
      "\n",
      "Epoch 29/45\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.65it/s, Loss=5672.3999] \n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 6875.8716\n",
      "Val Loss: 8196.6327\n",
      "MAE: 57.72\n",
      "\n",
      "Epoch 30/45\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.59it/s, Loss=3094.8096] \n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 7093.3065\n",
      "Val Loss: 11581.6989\n",
      "MAE: 69.27\n",
      "\n",
      "Epoch 31/45\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.35it/s, Loss=5388.1133] \n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 6088.3377\n",
      "Val Loss: 8285.9584\n",
      "MAE: 58.98\n",
      "\n",
      "Epoch 32/45\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.37it/s, Loss=4404.3076] \n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 7843.6039\n",
      "Val Loss: 9856.5440\n",
      "MAE: 65.92\n",
      "\n",
      "Epoch 33/45\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.41it/s, Loss=7536.9897] \n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 6096.0412\n",
      "Val Loss: 7832.8084\n",
      "MAE: 57.39\n",
      "\n",
      "Epoch 34/45\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.32it/s, Loss=9356.6445] \n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 7243.3942\n",
      "Val Loss: 15842.8336\n",
      "MAE: 79.96\n",
      "\n",
      "Epoch 35/45\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.47it/s, Loss=6985.7808] \n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 6834.2551\n",
      "Val Loss: 9833.3923\n",
      "MAE: 66.18\n",
      "\n",
      "Epoch 36/45\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.38it/s, Loss=7727.5576] \n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 6154.0865\n",
      "Val Loss: 10103.0405\n",
      "MAE: 62.84\n",
      "\n",
      "Epoch 37/45\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.56it/s, Loss=3225.5237] \n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 4738.9881\n",
      "Val Loss: 8308.3131\n",
      "MAE: 57.40\n",
      "\n",
      "Epoch 38/45\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.45it/s, Loss=4491.5337] \n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 6319.1628\n",
      "Val Loss: 19218.7817\n",
      "MAE: 99.64\n",
      "\n",
      "Epoch 39/45\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.55it/s, Loss=2597.2593] \n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 5711.8545\n",
      "Val Loss: 7717.9986\n",
      "MAE: 57.72\n",
      "\n",
      "Epoch 40/45\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.54it/s, Loss=13540.6094]\n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 7226.7803\n",
      "Val Loss: 13784.5108\n",
      "MAE: 75.41\n",
      "\n",
      "Epoch 41/45\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.61it/s, Loss=7317.3413] \n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 6857.2523\n",
      "Val Loss: 11458.6540\n",
      "MAE: 69.23\n",
      "\n",
      "Epoch 42/45\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.45it/s, Loss=1913.9406] \n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 5378.5506\n",
      "Val Loss: 8467.2728\n",
      "MAE: 57.27\n",
      "\n",
      "Epoch 43/45\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.44it/s, Loss=6208.5449] \n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 5861.9913\n",
      "Val Loss: 9621.0563\n",
      "MAE: 63.83\n",
      "\n",
      "Epoch 44/45\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.30it/s, Loss=5605.9072] \n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 5118.7139\n",
      "Val Loss: 7799.6044\n",
      "MAE: 55.56\n",
      "\n",
      "Epoch 45/45\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.46it/s, Loss=7294.8413] \n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 6115.0432\n",
      "Val Loss: 8104.9309\n",
      "MAE: 58.43\n",
      "\n",
      "Training completed!\n",
      "Best validation loss: 7717.9986\n",
      "\n",
      "Experiment 2 (Deep Volume Head) completed! Results saved to: ../experiments/nutrition5k_experiments/inceptionv3_image_volume_deepvolume_20251024_152918\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def train_nutrition5k_deep_volume(fusion_type='image_volume'):\n",
    "    \"\"\"\n",
    "    Train the Nutrition5k model with deeper volume head processing\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "    print(\"EXPERIMENT 2: DEEP VOLUME HEAD\")\n",
    "    print(f\"TRAINING: Nutrition5k InceptionV3 + Deep Volume Processing\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Create datasets\n",
    "    train_dataset = Nutrition5KDataset(\n",
    "        csv_path=train_csv,\n",
    "        data_root=DATA_ROOT,\n",
    "        split='train',\n",
    "        augment=False,\n",
    "        img_size=IMG_SIZE,\n",
    "    )\n",
    "    \n",
    "    val_dataset = Nutrition5KDataset(\n",
    "        csv_path=val_csv,\n",
    "        data_root=DATA_ROOT,\n",
    "        split='val',\n",
    "        augment=False,\n",
    "        img_size=IMG_SIZE,\n",
    "    )\n",
    "    \n",
    "    # Create data loaders\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=True,\n",
    "        num_workers=NUM_WORKERS,\n",
    "        pin_memory=True if torch.cuda.is_available() else False,\n",
    "        drop_last=True\n",
    "    )\n",
    "    \n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=False,\n",
    "        num_workers=NUM_WORKERS,\n",
    "        pin_memory=True if torch.cuda.is_available() else False\n",
    "    )\n",
    "    \n",
    "    # Build model with deep volume head\n",
    "    model = DeepVolumeNutrition5kModel(\n",
    "        fusion=fusion_type,\n",
    "        fusion_channels=FUSION_CHANNELS,\n",
    "        dropout_rate=DROPOUT_RATE,\n",
    "        pretrained=False,\n",
    "        use_volume=True\n",
    "    )\n",
    "    model = model.to(device)\n",
    "    \n",
    "    print(f\"Model parameters: {model.get_num_parameters():,}\")\n",
    "    print(f\"Training samples: {len(train_dataset)}\")\n",
    "    print(f\"Validation samples: {len(val_dataset)}\")\n",
    "    print(\"Deep Volume Processing: Volume → 16 → 8 → 4 → Concat with features\")\n",
    "    \n",
    "    # Loss function (using MSE for comparison)\n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    # Optimizer\n",
    "    optimizer = optim.AdamW(\n",
    "        model.parameters(),\n",
    "        lr=LEARNING_RATE,\n",
    "        weight_decay=WEIGHT_DECAY\n",
    "    )\n",
    "    \n",
    "    print(f\"Learning rate: {LEARNING_RATE}\")\n",
    "    print(f\"Weight decay: {WEIGHT_DECAY}\")\n",
    "    \n",
    "    # Learning rate scheduler\n",
    "    steps_per_epoch = len(train_loader)\n",
    "    total_steps = NUM_EPOCHS * steps_per_epoch\n",
    "    warmup_steps = int(total_steps * WARMUP_RATIO)\n",
    "    \n",
    "    scheduler = get_warmup_cosine_scheduler(\n",
    "        optimizer, \n",
    "        warmup_steps=warmup_steps, \n",
    "        total_steps=total_steps,\n",
    "        min_lr_ratio=MIN_LR_RATIO\n",
    "    )\n",
    "    \n",
    "    # Create experiment directory\n",
    "    exp_name = f\"inceptionv3_{fusion_type}_deepvolume_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "    exp_dir = os.path.join(OUTPUT_DIR, 'nutrition5k_experiments', exp_name)\n",
    "    os.makedirs(exp_dir, exist_ok=True)\n",
    "    \n",
    "    # Save experiment configuration\n",
    "    config = {\n",
    "        'fusion': fusion_type,\n",
    "        'use_volume': True,\n",
    "        'volume_head': 'deep',\n",
    "        'volume_processing': 'Volume -> Linear(16) -> Linear(8) -> Linear(4)',\n",
    "        'pretrained': False,\n",
    "        'dropout_rate': DROPOUT_RATE,\n",
    "        'fusion_channels': FUSION_CHANNELS,\n",
    "        'learning_rate': LEARNING_RATE,\n",
    "        'weight_decay': WEIGHT_DECAY,\n",
    "        'batch_size': BATCH_SIZE,\n",
    "        'img_size': IMG_SIZE,\n",
    "        'num_epochs': NUM_EPOCHS\n",
    "    }\n",
    "    \n",
    "    with open(os.path.join(exp_dir, 'config.json'), 'w') as f:\n",
    "        json.dump(config, f, indent=4)\n",
    "    \n",
    "    # Create trainer\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        criterion=criterion,\n",
    "        optimizer=optimizer,\n",
    "        scheduler=scheduler,\n",
    "        device=device,\n",
    "        output_dir=exp_dir,\n",
    "        early_stopping_patience=EARLY_STOPPING_PATIENCE,\n",
    "        scheduler_step_on_batch=False\n",
    "    )\n",
    "    \n",
    "    # Train the model\n",
    "    trainer.train(NUM_EPOCHS)\n",
    "    \n",
    "    print(f\"\\nExperiment 2 (Deep Volume Head) completed! Results saved to: {exp_dir}\")\n",
    "    return trainer.best_metrics\n",
    "\n",
    "# Run Deep Volume Head experiment\n",
    "print(\"Starting Deep Volume Head Experiment...\")\n",
    "deep_volume_results = train_nutrition5k_deep_volume(fusion_type='image_volume')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ec2229",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Hyperparameter Tuning Experiment...\n",
      "This will train 6 different configurations - it will take a while!\n",
      "============================================================\n",
      "EXPERIMENT 3: HYPERPARAMETER TUNING\n",
      "Testing different LR, Dropout, and Weight Decay combinations\n",
      "============================================================\n",
      "\n",
      "==================================================\n",
      "HYPERPARAMETER CONFIG 1/6\n",
      "Description: Lower LR + Lower Dropout + Higher WD\n",
      "Learning Rate: 0.0001\n",
      "Dropout Rate: 0.3\n",
      "Weight Decay: 1e-05\n",
      "==================================================\n",
      "Loaded 2804 valid samples out of 2805\n",
      "Loaded 495 valid samples out of 495\n",
      "Model parameters: 21,916,833\n",
      "Starting training for 45 epochs...\n",
      "\n",
      "Epoch 1/45\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.44it/s, Loss=102409.4922]\n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 99224.4542\n",
      "Val Loss: 107400.1443\n",
      "MAE: 240.62\n",
      "\n",
      "Epoch 2/45\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.45it/s, Loss=121750.7969]\n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 96177.4820\n",
      "Val Loss: 95536.2358\n",
      "MAE: 225.45\n",
      "\n",
      "Epoch 3/45\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.48it/s, Loss=125471.6641]\n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 91769.7808\n",
      "Val Loss: 98484.9629\n",
      "MAE: 229.50\n",
      "\n",
      "Epoch 4/45\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.46it/s, Loss=57732.0547] \n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 83916.4638\n",
      "Val Loss: 84710.8484\n",
      "MAE: 215.08\n",
      "\n",
      "Epoch 5/45\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.51it/s, Loss=67963.1016] \n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.57it/s]"
     ]
    }
   ],
   "source": [
    "# EXPERIMENT 3: HYPERPARAMETER TUNING\n",
    "# Grid search for optimal learning rates, dropout, and weight decay\n",
    "\n",
    "def train_nutrition5k_hyperparam_tuning():\n",
    "    \"\"\"\n",
    "    Run hyperparameter tuning experiments with different configurations\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "    print(\"EXPERIMENT 3: HYPERPARAMETER TUNING\")\n",
    "    print(\"Testing different LR, Dropout, and Weight Decay combinations\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Hyperparameter grid\n",
    "    hyperparam_configs = [\n",
    "        # [learning_rate, dropout_rate, weight_decay, description]\n",
    "        [1e-4, 0.3, 1e-5, \"Lower LR + Lower Dropout + Higher WD\"],\n",
    "        [1e-4, 0.4, 1e-6, \"Lower LR + Current Dropout + Current WD\"],\n",
    "        [1e-4, 0.5, 1e-7, \"Lower LR + Higher Dropout + Lower WD\"],\n",
    "        [3e-4, 0.3, 1e-5, \"Current LR + Lower Dropout + Higher WD\"],\n",
    "        [3e-4, 0.5, 1e-7, \"Current LR + Higher Dropout + Lower WD\"],\n",
    "        [7e-4, 0.4, 1e-6, \"Higher LR + Current Dropout + Current WD\"],\n",
    "    ]\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for i, (lr, dropout, wd, desc) in enumerate(hyperparam_configs):\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"HYPERPARAMETER CONFIG {i+1}/{len(hyperparam_configs)}\")\n",
    "        print(f\"Description: {desc}\")\n",
    "        print(f\"Learning Rate: {lr}\")\n",
    "        print(f\"Dropout Rate: {dropout}\")\n",
    "        print(f\"Weight Decay: {wd}\")\n",
    "        print(f\"{'='*50}\")\n",
    "        \n",
    "        # Create datasets\n",
    "        train_dataset = Nutrition5KDataset(\n",
    "            csv_path=train_csv,\n",
    "            data_root=DATA_ROOT,\n",
    "            split='train',\n",
    "            augment=False,\n",
    "            img_size=IMG_SIZE,\n",
    "        )\n",
    "        \n",
    "        val_dataset = Nutrition5KDataset(\n",
    "            csv_path=val_csv,\n",
    "            data_root=DATA_ROOT,\n",
    "            split='val',\n",
    "            augment=False,\n",
    "            img_size=IMG_SIZE,\n",
    "        )\n",
    "        \n",
    "        # Create data loaders\n",
    "        train_loader = DataLoader(\n",
    "            train_dataset,\n",
    "            batch_size=BATCH_SIZE,\n",
    "            shuffle=True,\n",
    "            num_workers=NUM_WORKERS,\n",
    "            pin_memory=True if torch.cuda.is_available() else False,\n",
    "            drop_last=True\n",
    "        )\n",
    "        \n",
    "        val_loader = DataLoader(\n",
    "            val_dataset,\n",
    "            batch_size=BATCH_SIZE,\n",
    "            shuffle=False,\n",
    "            num_workers=NUM_WORKERS,\n",
    "            pin_memory=True if torch.cuda.is_available() else False\n",
    "        )\n",
    "        \n",
    "        # Build model\n",
    "        model = build_nutrition5k_model(\n",
    "            fusion='image_volume',\n",
    "            pretrained=False,\n",
    "            dropout_rate=dropout,\n",
    "            fusion_channels=FUSION_CHANNELS,\n",
    "            use_volume=True,\n",
    "        )\n",
    "        model = model.to(device)\n",
    "        \n",
    "        print(f\"Model parameters: {model.get_num_parameters():,}\")\n",
    "        \n",
    "        # Loss function\n",
    "        criterion = nn.MSELoss()\n",
    "        \n",
    "        # Optimizer with tuned parameters\n",
    "        optimizer = optim.AdamW(\n",
    "            model.parameters(),\n",
    "            lr=lr,\n",
    "            weight_decay=wd\n",
    "        )\n",
    "        \n",
    "        # Learning rate scheduler\n",
    "        steps_per_epoch = len(train_loader)\n",
    "        total_steps = NUM_EPOCHS * steps_per_epoch\n",
    "        warmup_steps = int(total_steps * WARMUP_RATIO)\n",
    "        \n",
    "        scheduler = get_warmup_cosine_scheduler(\n",
    "            optimizer, \n",
    "            warmup_steps=warmup_steps, \n",
    "            total_steps=total_steps,\n",
    "            min_lr_ratio=MIN_LR_RATIO\n",
    "        )\n",
    "        \n",
    "        # Create experiment directory\n",
    "        exp_name = f\"inceptionv3_hyperparam_lr{lr}_dr{dropout}_wd{wd}_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "        exp_dir = os.path.join(OUTPUT_DIR, 'nutrition5k_experiments', exp_name)\n",
    "        os.makedirs(exp_dir, exist_ok=True)\n",
    "        \n",
    "        # Save experiment configuration\n",
    "        config = {\n",
    "            'experiment': 'hyperparameter_tuning',\n",
    "            'config_index': i + 1,\n",
    "            'description': desc,\n",
    "            'fusion': 'image_volume',\n",
    "            'use_volume': True,\n",
    "            'pretrained': False,\n",
    "            'dropout_rate': dropout,\n",
    "            'fusion_channels': FUSION_CHANNELS,\n",
    "            'learning_rate': lr,\n",
    "            'weight_decay': wd,\n",
    "            'batch_size': BATCH_SIZE,\n",
    "            'img_size': IMG_SIZE,\n",
    "            'num_epochs': NUM_EPOCHS\n",
    "        }\n",
    "        \n",
    "        with open(os.path.join(exp_dir, 'config.json'), 'w') as f:\n",
    "            json.dump(config, f, indent=4)\n",
    "        \n",
    "        # Create trainer\n",
    "        trainer = Trainer(\n",
    "            model=model,\n",
    "            train_loader=train_loader,\n",
    "            val_loader=val_loader,\n",
    "            criterion=criterion,\n",
    "            optimizer=optimizer,\n",
    "            scheduler=scheduler,\n",
    "            device=device,\n",
    "            output_dir=exp_dir,\n",
    "            early_stopping_patience=EARLY_STOPPING_PATIENCE,\n",
    "            scheduler_step_on_batch=False\n",
    "        )\n",
    "        \n",
    "        # Train the model\n",
    "        trainer.train(NUM_EPOCHS)\n",
    "        \n",
    "        # Store results\n",
    "        result = {\n",
    "            'config_index': i + 1,\n",
    "            'description': desc,\n",
    "            'learning_rate': lr,\n",
    "            'dropout_rate': dropout,\n",
    "            'weight_decay': wd,\n",
    "            'best_metrics': trainer.best_metrics,\n",
    "            'experiment_dir': exp_dir\n",
    "        }\n",
    "        results.append(result)\n",
    "        \n",
    "        print(f\"Config {i+1} completed! Best VAL MAE: {trainer.best_metrics.get('val_mae', 'N/A')}\")\n",
    "    \n",
    "    # Print summary of all results\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"HYPERPARAMETER TUNING RESULTS SUMMARY\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Sort results by validation MAE\n",
    "    results_sorted = sorted(results, key=lambda x: x['best_metrics'].get('val_mae', float('inf')))\n",
    "    \n",
    "    for i, result in enumerate(results_sorted):\n",
    "        print(f\"{i+1}. {result['description']}\")\n",
    "        print(f\"   LR: {result['learning_rate']}, Dropout: {result['dropout_rate']}, WD: {result['weight_decay']}\")\n",
    "        print(f\"   Best VAL MAE: {result['best_metrics'].get('val_mae', 'N/A')}\")\n",
    "        print(f\"   Directory: {result['experiment_dir']}\")\n",
    "        print()\n",
    "    \n",
    "    print(f\"BEST CONFIGURATION: {results_sorted[0]['description']}\")\n",
    "    print(f\"Best VAL MAE: {results_sorted[0]['best_metrics'].get('val_mae', 'N/A')}\")\n",
    "    \n",
    "    return results_sorted\n",
    "\n",
    "# Run hyperparameter tuning\n",
    "print(\"Starting Hyperparameter Tuning Experiment...\")\n",
    "print(\"This will train 6 different configurations - it will take a while!\")\n",
    "hyperparam_results = train_nutrition5k_hyperparam_tuning()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "571bd639",
   "metadata": {},
   "source": [
    "## 🧪 How to Run the Experiments\n",
    "\n",
    "To run any of the experiments above, simply uncomment the appropriate line:\n",
    "\n",
    "### Experiment 1: Huber Loss\n",
    "```python\n",
    "huber_results = train_nutrition5k_huber_loss(fusion_type='image_volume', huber_delta=50.0)\n",
    "```\n",
    "- **Expected improvement**: Better robustness to outliers\n",
    "- **Runtime**: ~40 epochs x 13s = ~8.5 minutes\n",
    "\n",
    "### Experiment 2: Deep Volume Head  \n",
    "```python\n",
    "deep_volume_results = train_nutrition5k_deep_volume(fusion_type='image_volume')\n",
    "```\n",
    "- **Expected improvement**: Better volume signal processing\n",
    "- **Runtime**: Similar to baseline (~8.5 minutes)\n",
    "\n",
    "### Experiment 3: Hyperparameter Tuning\n",
    "```python  \n",
    "hyperparam_results = train_nutrition5k_hyperparam_tuning()\n",
    "```\n",
    "- **Expected improvement**: Optimal LR/dropout/weight_decay combination\n",
    "- **Runtime**: 6 configurations x 8.5 minutes = ~51 minutes\n",
    "\n",
    "### 📊 Expected Results\n",
    "Based on your current **MAE of 54.02**, these experiments should achieve:\n",
    "- **Huber Loss**: 52-54 MAE (better stability)\n",
    "- **Deep Volume**: 51-53 MAE (better volume processing)  \n",
    "- **Hyperparameter Tuning**: 49-52 MAE (optimal settings)\n",
    "\n",
    "### 🎯 Next Steps After Experiments\n",
    "1. Compare results from all experiments\n",
    "2. Use the best configuration for ensemble training\n",
    "3. Apply test-time augmentation for final submission\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "compvis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
