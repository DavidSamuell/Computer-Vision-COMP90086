{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "04511270",
   "metadata": {},
   "source": [
    "# Nutrition5k InceptionV2 Implementation\n",
    "\n",
    "This notebook implements the InceptionV2-based model architecture as described in the Nutrition5k paper. It includes:\n",
    "\n",
    "- Data loading and preprocessing\n",
    "- InceptionV2 model architecture\n",
    "- Different fusion methods (early, middle, late)\n",
    "- Training and validation loops\n",
    "- Memory optimization to prevent OOM errors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b4905f50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Standard imports\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "import gc\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# PyTorch imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# Data processing imports\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import torchvision.transforms as T\n",
    "import torchvision.transforms.functional as TF\n",
    "from collections import OrderedDict\n",
    "import random\n",
    "from typing import Tuple, Optional\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "# Device info function\n",
    "def print_device_info():\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "        print(f\"CUDA version: {torch.version.cuda}\")\n",
    "        print(f\"GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dde3e76b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration (based on Nutrition5k paper):\n",
      "  Data root: ../Nutrition5K/train\n",
      "  CSV path: ../Nutrition5K/nutrition5k_train.csv\n",
      "  Output directory: ../experiments\n",
      "  Batch size: 16\n",
      "  Number of epochs: 40\n",
      "  Image size: 256x256\n",
      "  Workers: 4\n",
      "  Learning rate: 0.001\n",
      "  Optimizer: RMSProp (momentum=0.9, decay=0.9, epsilon=1e-08)\n",
      "  FC dimensions: 1024\n"
     ]
    }
   ],
   "source": [
    "# Configuration - Update these paths to match your setup\n",
    "DATA_ROOT = '../Nutrition5K/train'  # Path to training data directory\n",
    "CSV_PATH = '../Nutrition5K/nutrition5k_train.csv'  # Path to training CSV\n",
    "OUTPUT_DIR = '../experiments'  # Directory to save experiment results\n",
    "\n",
    "# Global training hyperparameters (based on the Nutrition5k paper)\n",
    "BATCH_SIZE = 16  # Reduced batch size to prevent OOM errors\n",
    "NUM_EPOCHS = 40\n",
    "VAL_RATIO = 0.15\n",
    "IMG_SIZE = 256  # Paper specifies 256x256 input\n",
    "NUM_WORKERS = 4  # Reduced workers to prevent memory issues\n",
    "DROPOUT_RATE = 0.4\n",
    "LEARNING_RATE = 1e-3  # As specified in the paper (RMSProp with lr=1e-4)\n",
    "MOMENTUM = 0.9  # As specified in the paper\n",
    "DECAY = 0.9  # As specified in the paper\n",
    "EPSILON = 1e-8  # As specified in the paper\n",
    "FEATURE_DIM = 2048  # Feature map dimension from InceptionV2\n",
    "FC_DIM = 1024  # Using smaller FC dimensions since we're training from scratch (original paper used 4096)\n",
    "\n",
    "print(\"Configuration (based on Nutrition5k paper):\")\n",
    "print(f\"  Data root: {DATA_ROOT}\")\n",
    "print(f\"  CSV path: {CSV_PATH}\")\n",
    "print(f\"  Output directory: {OUTPUT_DIR}\")\n",
    "print(f\"  Batch size: {BATCH_SIZE}\")\n",
    "print(f\"  Number of epochs: {NUM_EPOCHS}\")\n",
    "print(f\"  Image size: {IMG_SIZE}x{IMG_SIZE}\")\n",
    "print(f\"  Workers: {NUM_WORKERS}\")\n",
    "print(f\"  Learning rate: {LEARNING_RATE}\")\n",
    "print(f\"  Optimizer: RMSProp (momentum={MOMENTUM}, decay={DECAY}, epsilon={EPSILON})\")\n",
    "print(f\"  FC dimensions: {FC_DIM}\")\n",
    "\n",
    "# Create output directory\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "70eabc30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nutrition5k Dataset Class\n",
    "class Nutrition5KDataset(Dataset):\n",
    "    \"\"\"Dataset class for Nutrition5K with preprocessing as described in the paper\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        csv_path: str,\n",
    "        data_root: str,\n",
    "        split: str = 'train',\n",
    "        augment: bool = True,\n",
    "        img_size: int = 256  # Updated to 256 per paper\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_path: Path to the CSV file with dish IDs and calorie values\n",
    "            data_root: Root directory containing color/, depth_raw/ subdirectories\n",
    "            split: 'train' or 'val'\n",
    "            augment: Whether to apply data augmentation\n",
    "            img_size: Size to resize images (256x256 as specified in paper)\n",
    "        \"\"\"\n",
    "        self.data_root = data_root\n",
    "        self.split = split\n",
    "        self.img_size = img_size\n",
    "        self.augment = augment and split == 'train'\n",
    "        \n",
    "        # Load CSV\n",
    "        self.df = pd.read_csv(csv_path)\n",
    "        \n",
    "        # Rename 'Value' column to 'calories' if it exists\n",
    "        if 'Value' in self.df.columns and 'calories' not in self.df.columns:\n",
    "            self.df = self.df.rename(columns={'Value': 'calories'})\n",
    "        \n",
    "        # Make sure calories column exists\n",
    "        if 'calories' not in self.df.columns:\n",
    "            raise ValueError(\"CSV file must contain a 'calories' column or a 'Value' column\")\n",
    "        \n",
    "        # Filter out high-calorie samples (as mentioned in the paper)\n",
    "        self.df = self.df[self.df['calories'] < 3000].reset_index(drop=True)\n",
    "        \n",
    "        # Build paths\n",
    "        self.color_dir = os.path.join(data_root, 'color')\n",
    "        self.depth_raw_dir = os.path.join(data_root, 'depth_raw')\n",
    "        \n",
    "        # Validate dataset\n",
    "        self.valid_indices = self._validate_dataset()\n",
    "        print(f\"Loaded {len(self.valid_indices)} valid samples out of {len(self.df)}\")\n",
    "        \n",
    "        # Normalization values from the paper\n",
    "        self.color_normalize = T.Normalize(\n",
    "            mean=[0.485, 0.456, 0.406],\n",
    "            std=[0.229, 0.224, 0.225]\n",
    "        )\n",
    "    \n",
    "    def _validate_dataset(self):\n",
    "        \"\"\"Pre-validate all samples and return valid indices\"\"\"\n",
    "        valid_indices = []\n",
    "        \n",
    "        for idx in range(len(self.df)):\n",
    "            dish_id = self.df.iloc[idx]['ID']\n",
    "            \n",
    "            rgb_path = os.path.join(self.color_dir, dish_id, 'rgb.png')\n",
    "            depth_path = os.path.join(self.depth_raw_dir, dish_id, 'depth_raw.png')\n",
    "            \n",
    "            # Check if files exist\n",
    "            if not os.path.exists(rgb_path):\n",
    "                warnings.warn(f\"Missing RGB image: {rgb_path}\")\n",
    "                continue\n",
    "            if not os.path.exists(depth_path):\n",
    "                warnings.warn(f\"Missing depth image: {depth_path}\")\n",
    "                continue\n",
    "            \n",
    "            # Try to load images to check for corruption\n",
    "            try:\n",
    "                with Image.open(rgb_path) as img:\n",
    "                    img.verify()\n",
    "                with Image.open(depth_path) as img:\n",
    "                    img.verify()\n",
    "                valid_indices.append(idx)\n",
    "            except Exception as e:\n",
    "                warnings.warn(f\"Corrupt image for {dish_id}: {e}\")\n",
    "                continue\n",
    "                \n",
    "        return valid_indices\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.valid_indices)\n",
    "    \n",
    "    def _load_image_safe(self, path: str, mode: str = 'RGB') -> Optional[Image.Image]:\n",
    "        \"\"\"Safely load an image with error handling\"\"\"\n",
    "        try:\n",
    "            with Image.open(path) as img:\n",
    "                return img.convert(mode).copy()\n",
    "        except Exception as e:\n",
    "            warnings.warn(f\"Failed to load image {path}: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def _apply_preprocessing(self, rgb_img, depth_img):\n",
    "        \"\"\"Apply preprocessing as described in the paper\"\"\"\n",
    "        # Resize to target size\n",
    "        rgb_img = TF.resize(rgb_img, (self.img_size, self.img_size))\n",
    "        depth_img = TF.resize(depth_img, (self.img_size, self.img_size))\n",
    "        \n",
    "        # Apply data augmentation for training\n",
    "        if self.split == 'train' and self.augment:\n",
    "            # Random horizontal flip (50% probability)\n",
    "            if random.random() > 0.5:\n",
    "                rgb_img = TF.hflip(rgb_img)\n",
    "                depth_img = TF.hflip(depth_img)\n",
    "            \n",
    "            # Random rotation\n",
    "            if random.random() > 0.5:\n",
    "                angle = random.uniform(-10, 10)\n",
    "                rgb_img = TF.rotate(rgb_img, angle)\n",
    "                depth_img = TF.rotate(depth_img, angle)\n",
    "        \n",
    "        return rgb_img, depth_img\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"Get a single sample\"\"\"\n",
    "        actual_idx = self.valid_indices[idx]\n",
    "        row = self.df.iloc[actual_idx]\n",
    "        \n",
    "        dish_id = row['ID']\n",
    "        calorie = float(row['calories'])\n",
    "        \n",
    "        # Load images\n",
    "        rgb_path = os.path.join(self.color_dir, dish_id, 'rgb.png')\n",
    "        depth_path = os.path.join(self.depth_raw_dir, dish_id, 'depth_raw.png')\n",
    "        \n",
    "        rgb_img = self._load_image_safe(rgb_path, 'RGB')\n",
    "        depth_img = self._load_image_safe(depth_path, 'L')  # Grayscale for depth\n",
    "        \n",
    "        if rgb_img is None or depth_img is None:\n",
    "            # Fallback: return a black image\n",
    "            rgb_img = Image.new('RGB', (self.img_size, self.img_size), (0, 0, 0))\n",
    "            depth_img = Image.new('L', (self.img_size, self.img_size), 0)\n",
    "        \n",
    "        # Apply preprocessing\n",
    "        rgb_img, depth_img = self._apply_preprocessing(rgb_img, depth_img)\n",
    "        \n",
    "        # Convert to tensors\n",
    "        rgb_tensor = TF.to_tensor(rgb_img)  # (3, H, W)\n",
    "        depth_tensor = TF.to_tensor(depth_img)  # (1, H, W)\n",
    "        \n",
    "        # Normalize RGB\n",
    "        rgb_tensor = self.color_normalize(rgb_tensor)\n",
    "        \n",
    "        # Normalize depth (map to [0,1])\n",
    "        depth_tensor = depth_tensor / 255.0\n",
    "        \n",
    "        return {\n",
    "            'dish_id': dish_id,\n",
    "            'rgb': rgb_tensor,\n",
    "            'depth': depth_tensor,\n",
    "            'calorie': torch.tensor(calorie, dtype=torch.float32)\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "adcd8961",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating train/validation split...\n",
      "Train CSV: ../Nutrition5K/train_split.csv\n",
      "Validation CSV: ../Nutrition5K/val_split.csv\n",
      "Loaded 2804 valid samples out of 2805\n",
      "\n",
      "Dataset loaded successfully!\n",
      "Training samples: 2804\n",
      "RGB shape: torch.Size([3, 256, 256])\n",
      "Depth shape: torch.Size([1, 256, 256])\n",
      "Calorie value: 88.5\n"
     ]
    }
   ],
   "source": [
    "# Function to create train/validation split\n",
    "def create_train_val_split(csv_path: str, val_ratio: float = 0.15, random_seed: int = 42):\n",
    "    \"\"\"\n",
    "    Create train/validation split CSV files\n",
    "    \n",
    "    Args:\n",
    "        csv_path: Path to the original CSV file\n",
    "        val_ratio: Ratio of validation samples\n",
    "        random_seed: Random seed for reproducibility\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (train_csv_path, val_csv_path)\n",
    "    \"\"\"\n",
    "    # Read original CSV\n",
    "    df = pd.read_csv(csv_path)    \n",
    "    \n",
    "    # Shuffle with fixed seed\n",
    "    df_shuffled = df.sample(frac=1, random_state=random_seed).reset_index(drop=True)\n",
    "    \n",
    "    # Split\n",
    "    val_size = int(len(df_shuffled) * val_ratio)\n",
    "    train_df = df_shuffled[val_size:]\n",
    "    val_df = df_shuffled[:val_size]\n",
    "    \n",
    "    # Save temporary CSV files\n",
    "    base_dir = os.path.dirname(csv_path)\n",
    "    train_csv = os.path.join(base_dir, 'train_split.csv')\n",
    "    val_csv = os.path.join(base_dir, 'val_split.csv')\n",
    "    \n",
    "    train_df.to_csv(train_csv, index=False)\n",
    "    val_df.to_csv(val_csv, index=False)\n",
    "    \n",
    "    return train_csv, val_csv\n",
    "\n",
    "# Create train/validation split\n",
    "print(\"Creating train/validation split...\")\n",
    "train_csv, val_csv = create_train_val_split(\n",
    "    CSV_PATH,\n",
    "    val_ratio=VAL_RATIO,\n",
    "    random_seed=SEED\n",
    ")\n",
    "\n",
    "print(f\"Train CSV: {train_csv}\")\n",
    "print(f\"Validation CSV: {val_csv}\")\n",
    "\n",
    "# Check if we can load a sample from the dataset\n",
    "sample_dataset = Nutrition5KDataset(\n",
    "    csv_path=train_csv,\n",
    "    data_root=DATA_ROOT,\n",
    "    split='train',\n",
    "    augment=False,\n",
    "    img_size=IMG_SIZE\n",
    ")\n",
    "\n",
    "print(f\"\\nDataset loaded successfully!\")\n",
    "print(f\"Training samples: {len(sample_dataset)}\")\n",
    "if len(sample_dataset) > 0:\n",
    "    sample = sample_dataset[0]\n",
    "    print(f\"RGB shape: {sample['rgb'].shape}\")\n",
    "    print(f\"Depth shape: {sample['depth'].shape}\")\n",
    "    print(f\"Calorie value: {sample['calorie'].item():.1f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "34d1cac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# InceptionV2 Model Implementation\n",
    "# First, let's implement the basic building blocks\n",
    "\n",
    "class BasicConv2d(nn.Module):\n",
    "    \"\"\"Basic convolution module for InceptionV2: Conv2d + BatchNorm + ReLU\"\"\"\n",
    "    \n",
    "    def __init__(self, in_channels, out_channels, **kwargs):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, bias=False, **kwargs)\n",
    "        self.bn = nn.BatchNorm2d(out_channels, eps=0.001)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.bn(x)\n",
    "        return F.relu(x, inplace=True)\n",
    "\n",
    "\n",
    "class InceptionModule(nn.Module):\n",
    "    \"\"\"InceptionV2 module with BatchNorm\"\"\"\n",
    "    \n",
    "    def __init__(self, in_channels, ch1x1, ch3x3red, ch3x3, ch5x5red, ch5x5, pool_proj):\n",
    "        super().__init__()\n",
    "        \n",
    "        # 1x1 branch\n",
    "        self.branch1 = BasicConv2d(in_channels, ch1x1, kernel_size=1)\n",
    "        \n",
    "        # 3x3 branch\n",
    "        self.branch2 = nn.Sequential(\n",
    "            BasicConv2d(in_channels, ch3x3red, kernel_size=1),\n",
    "            BasicConv2d(ch3x3red, ch3x3, kernel_size=3, padding=1)\n",
    "        )\n",
    "        \n",
    "        # 5x5 branch\n",
    "        self.branch3 = nn.Sequential(\n",
    "            BasicConv2d(in_channels, ch5x5red, kernel_size=1),\n",
    "            BasicConv2d(ch5x5red, ch5x5, kernel_size=5, padding=2)\n",
    "        )\n",
    "        \n",
    "        # Pool branch\n",
    "        self.branch4 = nn.Sequential(\n",
    "            nn.MaxPool2d(kernel_size=3, stride=1, padding=1),\n",
    "            BasicConv2d(in_channels, pool_proj, kernel_size=1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        branch1 = self.branch1(x)\n",
    "        branch2 = self.branch2(x)\n",
    "        branch3 = self.branch3(x)\n",
    "        branch4 = self.branch4(x)\n",
    "        \n",
    "        return torch.cat([branch1, branch2, branch3, branch4], dim=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d8ec9f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# InceptionV2 Encoder Implementation\n",
    "\n",
    "class InceptionV2Encoder(nn.Module):\n",
    "    \"\"\"InceptionV2 encoder as used in the original Nutrition5k paper\"\"\"\n",
    "    \n",
    "    def __init__(self, pretrained: bool = False, in_channels: int = 3):\n",
    "        super().__init__()\n",
    "        \n",
    "        # The output of InceptionV2 features is 2048 channels (as shown in paper)\n",
    "        self.out_channels = 2048\n",
    "        \n",
    "        # Initial layers\n",
    "        self.conv1 = nn.Sequential(\n",
    "            BasicConv2d(in_channels, 64, kernel_size=7, stride=2, padding=3),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n",
    "            BasicConv2d(64, 64, kernel_size=1),\n",
    "            BasicConv2d(64, 192, kernel_size=3, padding=1),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        )\n",
    "        \n",
    "        # Inception blocks\n",
    "        self.inception3a = InceptionModule(192, 64, 96, 128, 16, 32, 32)\n",
    "        self.inception3b = InceptionModule(256, 128, 128, 192, 32, 96, 64)\n",
    "        \n",
    "        # Max pooling\n",
    "        self.maxpool3 = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        \n",
    "        # More Inception blocks\n",
    "        self.inception4a = InceptionModule(480, 192, 96, 208, 16, 48, 64)\n",
    "        self.inception4b = InceptionModule(512, 160, 112, 224, 24, 64, 64)\n",
    "        self.inception4c = InceptionModule(512, 128, 128, 256, 24, 64, 64)\n",
    "        self.inception4d = InceptionModule(512, 112, 144, 288, 32, 64, 64)\n",
    "        self.inception4e = InceptionModule(528, 256, 160, 320, 32, 128, 128)\n",
    "        \n",
    "        # Max pooling\n",
    "        self.maxpool4 = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        \n",
    "        # Final Inception blocks\n",
    "        self.inception5a = InceptionModule(832, 256, 160, 320, 32, 128, 128)\n",
    "# Modify these parameters to double the output channels\n",
    "        self.inception5b = InceptionModule(832, 768, 192, 768, 48, 256, 256)        \n",
    "        # Initialize weights\n",
    "        self._initialize_weights()\n",
    "    \n",
    "    def _initialize_weights(self):\n",
    "        \"\"\"Initialize weights with the recommended scheme\"\"\"\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Input tensor (B, C, H, W)\n",
    "        Returns:\n",
    "            Feature map (B, 1024, H/32, W/32)\n",
    "        \"\"\"\n",
    "        x = self.conv1(x)\n",
    "        \n",
    "        x = self.inception3a(x)\n",
    "        x = self.inception3b(x)\n",
    "        x = self.maxpool3(x)\n",
    "        \n",
    "        x = self.inception4a(x)\n",
    "        x = self.inception4b(x)\n",
    "        x = self.inception4c(x)\n",
    "        x = self.inception4d(x)\n",
    "        x = self.inception4e(x)\n",
    "        x = self.maxpool4(x)\n",
    "        \n",
    "        x = self.inception5a(x)\n",
    "        x = self.inception5b(x)\n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4b7d4074",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regression Head for calorie prediction\n",
    "class RegressionHead(nn.Module):\n",
    "    \"\"\"\n",
    "    Regression head for calorie prediction\n",
    "    Modified architecture with smaller FC layers for non-pretrained model\n",
    "    Original paper used (4096→4096→4096→1) with pretrained model\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, in_channels: int = 2048, fc_dim: int = 1024, dropout_rate: float = 0.4):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Global average pooling\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        \n",
    "        # Fully connected layers with reduced dimensions for non-pretrained model\n",
    "        # Uses progressive structure: 2048→1024→1024→1\n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            \n",
    "            # First FC layer: in_channels → fc_dim (2048 → 1024)\n",
    "            nn.Linear(in_channels, fc_dim),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            \n",
    "            # Second FC layer: fc_dim → fc_dim (1024 → 1024)\n",
    "            nn.Linear(fc_dim, fc_dim),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            \n",
    "            # Third FC layer: fc_dim → fc_dim/2 (1024 → 512)\n",
    "            nn.Linear(fc_dim, fc_dim // 2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            \n",
    "            # Final output layer: fc_dim/2 → 1 (512 → 1)\n",
    "            nn.Linear(fc_dim // 2, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.avgpool(x)  # (B, C, 1, 1)\n",
    "        x = self.fc_layers(x)  # (B, 1)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "223869b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full InceptionV2 Model for Calorie Prediction\n",
    "class CalorieInceptionV2Model(nn.Module):\n",
    "    \"\"\"\n",
    "    Complete InceptionV2 model for calorie prediction\n",
    "    Based on the architecture in the Nutrition5k paper but with smaller FC layers\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, pretrained=False, dropout_rate=0.4, fc_dim=1024):\n",
    "        super().__init__()\n",
    "        \n",
    "        # InceptionV2 encoder\n",
    "        self.encoder = InceptionV2Encoder(pretrained=pretrained, in_channels=3)\n",
    "        \n",
    "        # Regression head for calorie prediction\n",
    "        self.regression_head = RegressionHead(\n",
    "            in_channels=self.encoder.out_channels,\n",
    "            fc_dim=fc_dim,\n",
    "            dropout_rate=dropout_rate\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass through the model\n",
    "        \n",
    "        Args:\n",
    "            x: Input RGB image tensor (B, 3, H, W)\n",
    "            \n",
    "        Returns:\n",
    "            Calorie prediction (B, 1)\n",
    "        \"\"\"\n",
    "        # Extract features with InceptionV2 encoder\n",
    "        features = self.encoder(x)\n",
    "        \n",
    "        # Predict calories\n",
    "        calories = self.regression_head(features)\n",
    "        \n",
    "        return calories\n",
    "    \n",
    "    def get_num_parameters(self):\n",
    "        \"\"\"Get total number of trainable parameters\"\"\"\n",
    "        return sum(p.numel() for p in self.parameters() if p.requires_grad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e0b204fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement RMSProp optimizer with paper parameters\n",
    "def get_optimizer(model_params, lr=1e-4, momentum=0.9, weight_decay=0.0, epsilon=1.0):\n",
    "    \"\"\"\n",
    "    Create RMSProp optimizer with parameters from the paper\n",
    "    \n",
    "    Args:\n",
    "        model_params: Model parameters to optimize\n",
    "        lr: Learning rate (paper uses 1e-4)\n",
    "        momentum: Momentum factor (paper uses 0.9)\n",
    "        weight_decay: Weight decay for L2 regularization\n",
    "        epsilon: Epsilon value to prevent division by zero (paper uses 1.0)\n",
    "        \n",
    "    Returns:\n",
    "        Configured optimizer\n",
    "    \"\"\"\n",
    "    return torch.optim.RMSprop(\n",
    "        model_params,\n",
    "        lr=lr,\n",
    "        momentum=momentum,\n",
    "        alpha=DECAY,  # alpha is the decay factor in RMSProp (paper uses 0.9)\n",
    "        eps=epsilon,\n",
    "        weight_decay=weight_decay\n",
    "    )\n",
    "\n",
    "# Training and validation loss function\n",
    "def get_loss_fn():\n",
    "    \"\"\"\n",
    "    Loss function for calorie prediction\n",
    "    Using Mean Squared Error (MSE) instead of MAE\n",
    "    \"\"\"\n",
    "    return nn.MSELoss()  # MSE loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d6125038",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Early Stopping Helper\n",
    "class EarlyStopping:\n",
    "    \"\"\"Early stopping utility to prevent overfitting\"\"\"\n",
    "    \n",
    "    def __init__(self, patience=15, min_delta=0.1, mode='min'):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            patience: Number of epochs to wait after min has been hit\n",
    "            min_delta: Minimum change to qualify as an improvement\n",
    "            mode: 'min' for loss, 'max' for metrics like accuracy\n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.mode = mode\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.best_epoch = 0\n",
    "        self.early_stop = False\n",
    "    \n",
    "    def __call__(self, score, epoch):\n",
    "        \"\"\"Return True if training should stop\"\"\"\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.best_epoch = epoch\n",
    "            return False\n",
    "        \n",
    "        if self.mode == 'min':\n",
    "            improved = score < (self.best_score - self.min_delta)\n",
    "        else:\n",
    "            improved = score > (self.best_score + self.min_delta)\n",
    "        \n",
    "        if improved:\n",
    "            self.best_score = score\n",
    "            self.best_epoch = epoch\n",
    "            self.counter = 0\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "                \n",
    "        return self.early_stop\n",
    "\n",
    "\n",
    "# Trainer class for InceptionV2 model\n",
    "class Trainer:\n",
    "    \"\"\"Training manager for the calorie prediction model\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        model,\n",
    "        train_loader,\n",
    "        val_loader,\n",
    "        criterion,\n",
    "        optimizer,\n",
    "        device,\n",
    "        output_dir,\n",
    "        early_stopping_patience=15\n",
    "    ):\n",
    "        self.model = model\n",
    "        self.train_loader = train_loader\n",
    "        self.val_loader = val_loader\n",
    "        self.criterion = criterion\n",
    "        self.optimizer = optimizer\n",
    "        self.device = device\n",
    "        self.output_dir = output_dir\n",
    "        \n",
    "        # Early stopping\n",
    "        self.early_stopping = EarlyStopping(\n",
    "            patience=early_stopping_patience,\n",
    "            min_delta=0.1,\n",
    "            mode='min'\n",
    "        )\n",
    "        \n",
    "        # Tensorboard\n",
    "        self.writer = SummaryWriter(log_dir=os.path.join(output_dir, 'tensorboard'))\n",
    "        \n",
    "        # Tracking\n",
    "        self.best_val_loss = float('inf')\n",
    "        self.best_metrics = {}\n",
    "    \n",
    "    def train_epoch(self):\n",
    "        \"\"\"Train for one epoch\"\"\"\n",
    "        self.model.train()\n",
    "        total_loss = 0.0\n",
    "        \n",
    "        # Progress bar\n",
    "        pbar = tqdm(self.train_loader, desc=\"Training\")\n",
    "        for batch in pbar:\n",
    "            # Move to device\n",
    "            rgb = batch['rgb'].to(self.device)\n",
    "            calorie = batch['calorie'].to(self.device)\n",
    "            \n",
    "            # Forward pass\n",
    "            self.optimizer.zero_grad()\n",
    "            calorie_pred = self.model(rgb)\n",
    "            \n",
    "            # Compute loss\n",
    "            loss = self.criterion(calorie_pred.squeeze(), calorie)\n",
    "            \n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            \n",
    "            # Gradient clipping for stability\n",
    "            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)\n",
    "            \n",
    "            self.optimizer.step()\n",
    "            \n",
    "            # Track metrics\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            # Update progress bar\n",
    "            pbar.set_postfix({'Loss': f'{loss.item():.4f}'})\n",
    "        \n",
    "        return total_loss / len(self.train_loader)\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def validate_epoch(self):\n",
    "        \"\"\"Validate for one epoch\"\"\"\n",
    "        self.model.eval()\n",
    "        total_loss = 0.0\n",
    "        all_predictions = []\n",
    "        all_targets = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # Progress bar\n",
    "            pbar = tqdm(self.val_loader, desc=\"Validation\")\n",
    "            for batch in pbar:\n",
    "                # Move to device\n",
    "                rgb = batch['rgb'].to(self.device)\n",
    "                calorie = batch['calorie'].to(self.device)\n",
    "                \n",
    "                # Forward pass\n",
    "                calorie_pred = self.model(rgb)\n",
    "                \n",
    "                # Compute loss\n",
    "                loss = self.criterion(calorie_pred.squeeze(), calorie)\n",
    "                total_loss += loss.item()\n",
    "                \n",
    "                # Store predictions and targets for metrics\n",
    "                all_predictions.extend(calorie_pred.squeeze().cpu().numpy())\n",
    "                all_targets.extend(calorie.cpu().numpy())\n",
    "                \n",
    "                # Update progress bar\n",
    "                pbar.set_postfix({'Loss': f'{loss.item():.4f}'})\n",
    "            \n",
    "            # Calculate metrics\n",
    "            val_loss = total_loss / len(self.val_loader)\n",
    "            mae = np.mean(np.abs(np.array(all_predictions) - np.array(all_targets)))\n",
    "            mse = np.mean(np.square(np.array(all_predictions) - np.array(all_targets)))\n",
    "            \n",
    "            return val_loss, mae, mse\n",
    "    \n",
    "    def train(self, num_epochs):\n",
    "        \"\"\"Full training loop\"\"\"\n",
    "        print(f\"Starting training for {num_epochs} epochs...\")\n",
    "        \n",
    "        # Create experiment directory if it doesn't exist\n",
    "        os.makedirs(self.output_dir, exist_ok=True)\n",
    "        \n",
    "        # Print model parameters\n",
    "        print(f\"Model has {self.model.get_num_parameters():,} trainable parameters\")\n",
    "        \n",
    "        for epoch in range(num_epochs):\n",
    "            print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
    "            \n",
    "            # Train\n",
    "            train_loss = self.train_epoch()\n",
    "            \n",
    "            # Validate\n",
    "            val_loss, mae, mse= self.validate_epoch()\n",
    "            \n",
    "            # Log metrics\n",
    "            self.writer.add_scalar('Loss/Train', train_loss, epoch)\n",
    "            self.writer.add_scalar('Loss/Val', val_loss, epoch)\n",
    "            self.writer.add_scalar('MAE', mae, epoch)\n",
    "            self.writer.add_scalar('MSE', mse, epoch)  # Changed from MAE to MSE\n",
    "\n",
    "            \n",
    "            # Print epoch results\n",
    "            print(f\"Train Loss: {train_loss:.4f}\")\n",
    "            print(f\"Val Loss: {val_loss:.4f}\")\n",
    "            print(f\"MAE: {mae:.2f}\")\n",
    "            \n",
    "            # Save best model\n",
    "            if val_loss < self.best_val_loss:\n",
    "                self.best_val_loss = val_loss\n",
    "                self.best_metrics = {\n",
    "                    'epoch': epoch + 1,\n",
    "                    'val_loss': val_loss,\n",
    "                    'mae': mae,\n",
    "                }\n",
    "                \n",
    "                # Save model checkpoint\n",
    "                self._save_checkpoint(epoch)\n",
    "                print(f\"✓ New best model saved! (Val Loss: {val_loss:.4f})\")\n",
    "            \n",
    "            # Early stopping\n",
    "            if self.early_stopping(val_loss, epoch):\n",
    "                print(f\"Early stopping triggered after {epoch+1} epochs\")\n",
    "                print(f\"Best epoch: {self.early_stopping.best_epoch+1}\")\n",
    "                break\n",
    "        \n",
    "        self.writer.close()\n",
    "        print(f\"\\nTraining completed!\")\n",
    "        print(f\"Best validation loss: {self.best_val_loss:.4f}\")\n",
    "    \n",
    "    def _save_checkpoint(self, epoch):\n",
    "        \"\"\"Save model checkpoint\"\"\"\n",
    "        checkpoint = {\n",
    "            'epoch': epoch + 1,\n",
    "            'model_state_dict': self.model.state_dict(),\n",
    "            'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "            'val_loss': self.best_val_loss,\n",
    "        }\n",
    "        \n",
    "        checkpoint_path = os.path.join(self.output_dir, 'best_model.pth')\n",
    "        torch.save(checkpoint, checkpoint_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02c19fea",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Main experiment runner for training InceptionV2 model on Nutrition5k\n",
    "def train_inceptionv2_model(\n",
    "    data_root=DATA_ROOT,\n",
    "    train_csv=None,\n",
    "    val_csv=None,\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    num_epochs=NUM_EPOCHS,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    momentum=MOMENTUM,\n",
    "    decay=DECAY,\n",
    "    epsilon=EPSILON,\n",
    "    dropout_rate=DROPOUT_RATE,\n",
    "    fc_dim=FC_DIM,\n",
    "    img_size=IMG_SIZE,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    early_stopping_patience=15,\n",
    "    experiment_name=None,\n",
    "    pretrained=False\n",
    "):\n",
    "    \"\"\"\n",
    "    Train the InceptionV2 model for calorie prediction\n",
    "    \n",
    "    Args:\n",
    "        data_root: Path to the dataset directory\n",
    "        train_csv/val_csv: Paths to train/val CSV files\n",
    "        output_dir: Directory to save results\n",
    "        batch_size: Batch size for training\n",
    "        num_epochs: Number of training epochs\n",
    "        learning_rate: Learning rate for optimizer\n",
    "        momentum: Momentum for RMSProp\n",
    "        decay: Decay rate for RMSProp\n",
    "        epsilon: Epsilon value for RMSProp\n",
    "        dropout_rate: Dropout rate for regularization\n",
    "        fc_dim: Dimension of fully connected layers\n",
    "        img_size: Input image size\n",
    "        num_workers: Number of data loading workers\n",
    "        early_stopping_patience: Patience for early stopping\n",
    "        experiment_name: Name for this experiment run\n",
    "        pretrained: Whether to use pretrained weights\n",
    "        \n",
    "    Returns:\n",
    "        Best metrics from training\n",
    "    \"\"\"\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    # Generate experiment name if not provided\n",
    "    if experiment_name is None:\n",
    "        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "        experiment_name = f\"inceptionv2_calorie_{timestamp}\"\n",
    "    \n",
    "    # Create experiment directory\n",
    "    exp_dir = os.path.join(output_dir, experiment_name)\n",
    "    os.makedirs(exp_dir, exist_ok=True)\n",
    "    \n",
    "    # Create train/val splits if not provided\n",
    "    if train_csv is None or val_csv is None:\n",
    "        print(\"Creating train/validation split...\")\n",
    "        train_csv, val_csv = create_train_val_split(\n",
    "            CSV_PATH,\n",
    "            val_ratio=VAL_RATIO,\n",
    "            random_seed=SEED\n",
    "        )\n",
    "    \n",
    "    print(f\"Train CSV: {train_csv}\")\n",
    "    print(f\"Validation CSV: {val_csv}\")\n",
    "    \n",
    "    # Create datasets\n",
    "    print(\"\\nLoading datasets...\")\n",
    "    train_dataset = Nutrition5KDataset(\n",
    "        csv_path=train_csv,\n",
    "        data_root=data_root,\n",
    "        split='train',\n",
    "        augment=True,\n",
    "        img_size=img_size\n",
    "    )\n",
    "    \n",
    "    val_dataset = Nutrition5KDataset(\n",
    "        csv_path=val_csv,\n",
    "        data_root=data_root,\n",
    "        split='val',\n",
    "        augment=False,\n",
    "        img_size=img_size\n",
    "    )\n",
    "    \n",
    "    # Create data loaders\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True if torch.cuda.is_available() else False,\n",
    "        drop_last=True\n",
    "    )\n",
    "    \n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True if torch.cuda.is_available() else False\n",
    "    )\n",
    "    \n",
    "    # Build the model\n",
    "    print(\"\\nBuilding InceptionV2 model...\")\n",
    "    model = CalorieInceptionV2Model(\n",
    "        pretrained=pretrained,\n",
    "        dropout_rate=dropout_rate,\n",
    "        fc_dim=fc_dim\n",
    "    )\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Initialize optimizer and loss function\n",
    "    optimizer = get_optimizer(\n",
    "        model.parameters(),\n",
    "        lr=learning_rate,\n",
    "        momentum=momentum,\n",
    "        weight_decay=1e-6,  # Small weight decay for regularization\n",
    "        epsilon=epsilon\n",
    "    )\n",
    "    \n",
    "    criterion = get_loss_fn()\n",
    "    \n",
    "    # Save configuration\n",
    "    config = {\n",
    "        'batch_size': batch_size,\n",
    "        'num_epochs': num_epochs,\n",
    "        'learning_rate': learning_rate,\n",
    "        'momentum': momentum,\n",
    "        'decay': decay,\n",
    "        'epsilon': epsilon,\n",
    "        'dropout_rate': dropout_rate,\n",
    "        'fc_dim': fc_dim,\n",
    "        'img_size': img_size,\n",
    "        'pretrained': pretrained,\n",
    "        'num_workers': num_workers,\n",
    "        'early_stopping_patience': early_stopping_patience\n",
    "    }\n",
    "    \n",
    "    with open(os.path.join(exp_dir, 'config.json'), 'w') as f:\n",
    "        json.dump(config, f, indent=4)\n",
    "    \n",
    "    # Print training information\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(f\"TRAINING: InceptionV2 Calorie Prediction Model\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Model parameters: {model.get_num_parameters():,}\")\n",
    "    print(f\"Training samples: {len(train_dataset)}\")\n",
    "    print(f\"Validation samples: {len(val_dataset)}\")\n",
    "    print(f\"Learning rate: {learning_rate}\")\n",
    "    print(f\"RMSProp parameters: momentum={momentum}, decay={decay}, epsilon={epsilon}\")\n",
    "    print(f\"FC dimensions: {fc_dim}\")\n",
    "    print(f\"Output directory: {exp_dir}\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Create trainer\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        criterion=criterion,\n",
    "        optimizer=optimizer,\n",
    "        device=device,\n",
    "        output_dir=exp_dir,\n",
    "        early_stopping_patience=early_stopping_patience\n",
    "    )\n",
    "    \n",
    "    # Train the model\n",
    "    trainer.train(num_epochs)\n",
    "    \n",
    "    print(f\"\\nExperiment completed! Results saved to: {exp_dir}\")\n",
    "    return trainer.best_metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c0dbd45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model parameter count: 10,899,105\n",
      "\n",
      "Training configuration:\n",
      "  Batch size: 16\n",
      "  FC dimensions: 1024\n",
      "  Learning rate: 0.001\n",
      "  Image size: 256x256\n",
      "  Number of workers: 4\n",
      "\n",
      "GPU available: NVIDIA H100 80GB HBM3\n",
      "GPU memory: 84.93 GB\n",
      "\n",
      "Starting training...\n",
      "Creating train/validation split...\n",
      "Train CSV: ../Nutrition5K/train_split.csv\n",
      "Validation CSV: ../Nutrition5K/val_split.csv\n",
      "\n",
      "Loading datasets...\n",
      "Loaded 2804 valid samples out of 2805\n",
      "Loaded 495 valid samples out of 495\n",
      "\n",
      "Building InceptionV2 model...\n",
      "\n",
      "============================================================\n",
      "TRAINING: InceptionV2 Calorie Prediction Model\n",
      "============================================================\n",
      "Model parameters: 10,899,105\n",
      "Training samples: 2804\n",
      "Validation samples: 495\n",
      "Learning rate: 0.001\n",
      "RMSProp parameters: momentum=0.9, decay=0.9, epsilon=1e-08\n",
      "FC dimensions: 1024\n",
      "Output directory: ../experiments/exp_inceptionv2_calorie_20251024_013408\n",
      "============================================================\n",
      "Starting training for 40 epochs...\n",
      "Model has 10,899,105 trainable parameters\n",
      "\n",
      "Epoch 1/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 175/175 [00:12<00:00, 13.61it/s, Loss=68227.3750] \n",
      "Validation: 100%|██████████| 31/31 [00:02<00:00, 13.46it/s, Loss=76793.5234] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 60590.8357\n",
      "Val Loss: 79799.3223\n",
      "MAE: 231.87\n",
      "✓ New best model saved! (Val Loss: 79799.3223)\n",
      "\n",
      "Epoch 2/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 175/175 [00:15<00:00, 11.60it/s, Loss=174895.9375]\n",
      "Validation: 100%|██████████| 31/31 [00:02<00:00, 12.95it/s, Loss=33989.1562]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 52333.2610\n",
      "Val Loss: 37045.2835\n",
      "MAE: 149.19\n",
      "✓ New best model saved! (Val Loss: 37045.2835)\n",
      "\n",
      "Epoch 3/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 175/175 [00:12<00:00, 13.51it/s, Loss=59037.8711] \n",
      "Validation: 100%|██████████| 31/31 [00:02<00:00, 12.46it/s, Loss=18268.6973]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 41021.6421\n",
      "Val Loss: 27750.2133\n",
      "MAE: 130.20\n",
      "✓ New best model saved! (Val Loss: 27750.2133)\n",
      "\n",
      "Epoch 4/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 175/175 [00:12<00:00, 14.37it/s, Loss=44151.4766] \n",
      "Validation: 100%|██████████| 31/31 [00:02<00:00, 13.95it/s, Loss=403822.0312]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 38051.5855\n",
      "Val Loss: 489878.2319\n",
      "MAE: 633.32\n",
      "\n",
      "Epoch 5/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 175/175 [00:12<00:00, 14.55it/s, Loss=43070.8164] \n",
      "Validation: 100%|██████████| 31/31 [00:02<00:00, 14.02it/s, Loss=42383.3477] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 34272.2087\n",
      "Val Loss: 54472.6598\n",
      "MAE: 188.58\n",
      "\n",
      "Epoch 6/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 175/175 [00:12<00:00, 14.31it/s, Loss=11864.2705] \n",
      "Validation: 100%|██████████| 31/31 [00:03<00:00, 10.29it/s, Loss=14228.9395] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 33957.9909\n",
      "Val Loss: 32353.5599\n",
      "MAE: 119.49\n",
      "\n",
      "Epoch 7/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 175/175 [00:14<00:00, 12.12it/s, Loss=78010.6875] \n",
      "Validation: 100%|██████████| 31/31 [00:02<00:00, 12.97it/s, Loss=41916.4688] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 31511.1000\n",
      "Val Loss: 67346.4707\n",
      "MAE: 181.93\n",
      "\n",
      "Epoch 8/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 175/175 [00:15<00:00, 11.52it/s, Loss=67764.5938]\n",
      "Validation: 100%|██████████| 31/31 [00:02<00:00, 12.86it/s, Loss=122949.8672]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 29892.3315\n",
      "Val Loss: 183503.3233\n",
      "MAE: 351.11\n",
      "\n",
      "Epoch 9/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 175/175 [00:12<00:00, 13.65it/s, Loss=15350.0020] \n",
      "Validation: 100%|██████████| 31/31 [00:02<00:00, 12.89it/s, Loss=29600.4648] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 44860.7932\n",
      "Val Loss: 65140.8335\n",
      "MAE: 176.76\n",
      "\n",
      "Epoch 10/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 175/175 [00:12<00:00, 14.36it/s, Loss=35037.4883] \n",
      "Validation: 100%|██████████| 31/31 [00:02<00:00, 14.02it/s, Loss=5828.2930] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 28742.7750\n",
      "Val Loss: 21591.0838\n",
      "MAE: 100.05\n",
      "✓ New best model saved! (Val Loss: 21591.0838)\n",
      "\n",
      "Epoch 11/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 175/175 [00:12<00:00, 14.36it/s, Loss=39142.5742] \n",
      "Validation: 100%|██████████| 31/31 [00:02<00:00, 14.19it/s, Loss=46806.8594] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 57493.4731\n",
      "Val Loss: 91353.6022\n",
      "MAE: 214.64\n",
      "\n",
      "Epoch 12/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 175/175 [00:12<00:00, 14.37it/s, Loss=36335.6836] \n",
      "Validation: 100%|██████████| 31/31 [00:02<00:00, 14.21it/s, Loss=146467.1719]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 36895.7647\n",
      "Val Loss: 188349.9892\n",
      "MAE: 349.42\n",
      "\n",
      "Epoch 13/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 175/175 [00:12<00:00, 14.44it/s, Loss=26678.4727] \n",
      "Validation: 100%|██████████| 31/31 [00:02<00:00, 14.54it/s, Loss=7983.7964] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 29952.5399\n",
      "Val Loss: 17092.1983\n",
      "MAE: 88.75\n",
      "✓ New best model saved! (Val Loss: 17092.1983)\n",
      "\n",
      "Epoch 14/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 175/175 [00:12<00:00, 14.36it/s, Loss=37150.3750] \n",
      "Validation: 100%|██████████| 31/31 [00:02<00:00, 14.18it/s, Loss=11522.4912] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 31204.4808\n",
      "Val Loss: 33076.2534\n",
      "MAE: 126.24\n",
      "\n",
      "Epoch 15/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 175/175 [00:14<00:00, 12.10it/s, Loss=30233.2207] \n",
      "Validation: 100%|██████████| 31/31 [00:02<00:00, 13.26it/s, Loss=20755.5156] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 32146.3525\n",
      "Val Loss: 43072.0595\n",
      "MAE: 144.58\n",
      "\n",
      "Epoch 16/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 175/175 [00:13<00:00, 13.33it/s, Loss=16361.7314] \n",
      "Validation: 100%|██████████| 31/31 [00:02<00:00, 13.71it/s, Loss=14130.3008]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 31654.8319\n",
      "Val Loss: 14916.1169\n",
      "MAE: 89.77\n",
      "✓ New best model saved! (Val Loss: 14916.1169)\n",
      "\n",
      "Epoch 17/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 175/175 [00:12<00:00, 14.08it/s, Loss=20925.4297] \n",
      "Validation: 100%|██████████| 31/31 [00:02<00:00, 14.32it/s, Loss=12406.3896]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 33182.6127\n",
      "Val Loss: 28242.9974\n",
      "MAE: 114.82\n",
      "\n",
      "Epoch 18/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 175/175 [00:12<00:00, 14.52it/s, Loss=59182.8359]\n",
      "Validation: 100%|██████████| 31/31 [00:02<00:00, 14.44it/s, Loss=9853.0830] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 26671.7349\n",
      "Val Loss: 19636.1633\n",
      "MAE: 107.91\n",
      "\n",
      "Epoch 19/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 175/175 [00:12<00:00, 14.58it/s, Loss=23733.7637]\n",
      "Validation: 100%|██████████| 31/31 [00:02<00:00, 14.45it/s, Loss=9972.2793] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 30601.6517\n",
      "Val Loss: 17461.9334\n",
      "MAE: 97.82\n",
      "\n",
      "Epoch 20/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 175/175 [00:11<00:00, 14.69it/s, Loss=27010.7539] \n",
      "Validation: 100%|██████████| 31/31 [00:02<00:00, 14.31it/s, Loss=17598.0215] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 35336.3669\n",
      "Val Loss: 42270.0524\n",
      "MAE: 144.53\n",
      "\n",
      "Epoch 21/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 175/175 [00:12<00:00, 14.56it/s, Loss=18762.7266]\n",
      "Validation: 100%|██████████| 31/31 [00:02<00:00, 14.12it/s, Loss=11622.4873]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 29740.1647\n",
      "Val Loss: 19684.9223\n",
      "MAE: 103.93\n",
      "\n",
      "Epoch 22/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 175/175 [00:11<00:00, 14.61it/s, Loss=25524.6777] \n",
      "Validation: 100%|██████████| 31/31 [00:02<00:00, 13.61it/s, Loss=5511.2534] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 28751.4916\n",
      "Val Loss: 18673.1548\n",
      "MAE: 89.91\n",
      "\n",
      "Epoch 23/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 175/175 [00:12<00:00, 14.51it/s, Loss=16321.3018] \n",
      "Validation: 100%|██████████| 31/31 [00:02<00:00, 13.60it/s, Loss=36588.4141]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 41094.0699\n",
      "Val Loss: 35407.2401\n",
      "MAE: 134.51\n",
      "\n",
      "Epoch 24/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 175/175 [00:12<00:00, 14.40it/s, Loss=15222.2227] \n",
      "Validation: 100%|██████████| 31/31 [00:02<00:00, 13.49it/s, Loss=28102.8809] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 37452.4287\n",
      "Val Loss: 62971.8583\n",
      "MAE: 176.25\n",
      "\n",
      "Epoch 25/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 175/175 [00:13<00:00, 12.60it/s, Loss=6665.3281] \n",
      "Validation: 100%|██████████| 31/31 [00:02<00:00, 13.27it/s, Loss=6459.4058] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 28815.7880\n",
      "Val Loss: 14860.5781\n",
      "MAE: 89.16\n",
      "✓ New best model saved! (Val Loss: 14860.5781)\n",
      "\n",
      "Epoch 26/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  74%|███████▎  | 129/175 [00:09<00:04, 10.61it/s, Loss=13466.7812] "
     ]
    }
   ],
   "source": [
    "# Run the experiment\n",
    "if __name__ == \"__main__\":\n",
    "    # Set experiment name with timestamp\n",
    "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    experiment_name = f\"exp_inceptionv2_calorie_{timestamp}\"\n",
    "    \n",
    "    # Optional: Run model parameter count check\n",
    "    model = CalorieInceptionV2Model(pretrained=False, dropout_rate=DROPOUT_RATE, fc_dim=FC_DIM)\n",
    "    print(f\"Model parameter count: {model.get_num_parameters():,}\")\n",
    "    del model  # Free up memory\n",
    "    \n",
    "    # Print current configuration\n",
    "    print(\"\\nTraining configuration:\")\n",
    "    print(f\"  Batch size: {BATCH_SIZE}\")\n",
    "    print(f\"  FC dimensions: {FC_DIM}\")\n",
    "    print(f\"  Learning rate: {LEARNING_RATE}\")\n",
    "    print(f\"  Image size: {IMG_SIZE}x{IMG_SIZE}\")\n",
    "    print(f\"  Number of workers: {NUM_WORKERS}\")\n",
    "    \n",
    "    # Ensure we have GPU support if available\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"\\nGPU available: {torch.cuda.get_device_name(0)}\")\n",
    "        print(f\"GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "    else:\n",
    "        print(\"\\nNo GPU available, using CPU\")\n",
    "    \n",
    "    # Run the training\n",
    "    print(\"\\nStarting training...\")\n",
    "    try:\n",
    "        # Train the model\n",
    "        metrics = train_inceptionv2_model(\n",
    "            experiment_name=experiment_name,\n",
    "            batch_size=BATCH_SIZE,\n",
    "            num_epochs=NUM_EPOCHS,\n",
    "            learning_rate=LEARNING_RATE,\n",
    "            momentum=MOMENTUM,\n",
    "            decay=DECAY,\n",
    "            epsilon=EPSILON,\n",
    "            dropout_rate=DROPOUT_RATE,\n",
    "            fc_dim=FC_DIM,\n",
    "            img_size=IMG_SIZE,\n",
    "            num_workers=NUM_WORKERS,\n",
    "            early_stopping_patience=15\n",
    "        )\n",
    "        \n",
    "        # Print final results\n",
    "        print(\"\\nTraining completed successfully!\")\n",
    "        print(f\"Best validation MAE: {metrics['mae']:.2f}\")\n",
    "        print(f\"Best epoch: {metrics['epoch']}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\nTraining failed with error: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "compvis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
