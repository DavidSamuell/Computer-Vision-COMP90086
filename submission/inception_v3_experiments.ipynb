{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "98f4f0d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "from typing import Tuple, Optional\n",
    "from torchvision import models\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import torchvision.transforms as T\n",
    "import torchvision.transforms.functional as TF\n",
    "import random\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(789)\n",
    "np.random.seed(789)\n",
    "random.seed(789)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(789)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdad0586",
   "metadata": {},
   "source": [
    "# 1. Model and Trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b2af3ac",
   "metadata": {},
   "source": [
    "## 1.1 Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "01ec5e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "\n",
    "class InceptionV3Encoder(nn.Module):\n",
    "    \"\"\"InceptionV3 encoder as used in the original Nutrition5k paper\"\"\"\n",
    "    \n",
    "    def __init__(self, pretrained: bool = False, in_channels: int = 3):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Load InceptionV3 model\n",
    "        inception = models.inception_v3(pretrained=pretrained, aux_logits=False)\n",
    "        \n",
    "        # The output of InceptionV3 features is 2048 channels\n",
    "        self.out_channels = 2048\n",
    "        \n",
    "        # Modify first conv if we have different input channels (e.g., 1 for depth)\n",
    "        if in_channels != 3:\n",
    "            self.Conv2d_1a_3x3 = nn.Conv2d(\n",
    "                in_channels, 32, kernel_size=3, stride=2, bias=False\n",
    "            )\n",
    "        else:\n",
    "            self.Conv2d_1a_3x3 = inception.Conv2d_1a_3x3\n",
    "        \n",
    "        # Copy all other layers from InceptionV3\n",
    "        # First block\n",
    "        self.Conv2d_2a_3x3 = inception.Conv2d_2a_3x3\n",
    "        self.Conv2d_2b_3x3 = inception.Conv2d_2b_3x3\n",
    "        self.maxpool1 = inception.maxpool1\n",
    "        \n",
    "        # Second block\n",
    "        self.Conv2d_3b_1x1 = inception.Conv2d_3b_1x1\n",
    "        self.Conv2d_4a_3x3 = inception.Conv2d_4a_3x3\n",
    "        self.maxpool2 = inception.maxpool2\n",
    "        \n",
    "        # Inception blocks\n",
    "        self.Mixed_5b = inception.Mixed_5b\n",
    "        self.Mixed_5c = inception.Mixed_5c\n",
    "        self.Mixed_5d = inception.Mixed_5d\n",
    "        self.Mixed_6a = inception.Mixed_6a\n",
    "        self.Mixed_6b = inception.Mixed_6b\n",
    "        self.Mixed_6c = inception.Mixed_6c\n",
    "        self.Mixed_6d = inception.Mixed_6d\n",
    "        self.Mixed_6e = inception.Mixed_6e\n",
    "        self.Mixed_7a = inception.Mixed_7a\n",
    "        self.Mixed_7b = inception.Mixed_7b\n",
    "        self.Mixed_7c = inception.Mixed_7c\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Input tensor (B, C, H, W)\n",
    "        Returns:\n",
    "            Feature map (B, 2048, H/32, W/32)\n",
    "        \"\"\"\n",
    "        # First block\n",
    "        x = self.Conv2d_1a_3x3(x)\n",
    "        x = self.Conv2d_2a_3x3(x)\n",
    "        x = self.Conv2d_2b_3x3(x)\n",
    "        x = self.maxpool1(x)\n",
    "        \n",
    "        # Second block\n",
    "        x = self.Conv2d_3b_1x1(x)\n",
    "        x = self.Conv2d_4a_3x3(x)\n",
    "        x = self.maxpool2(x)\n",
    "        \n",
    "        # Inception blocks\n",
    "        x = self.Mixed_5b(x)\n",
    "        x = self.Mixed_5c(x)\n",
    "        x = self.Mixed_5d(x)\n",
    "        x = self.Mixed_6a(x)\n",
    "        x = self.Mixed_6b(x)\n",
    "        x = self.Mixed_6c(x)\n",
    "        x = self.Mixed_6d(x)\n",
    "        x = self.Mixed_6e(x)\n",
    "        x = self.Mixed_7a(x)\n",
    "        x = self.Mixed_7b(x)\n",
    "        x = self.Mixed_7c(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "# Early Fusion Module (RGB + Depth fused at input level)\n",
    "class EarlyFusion(nn.Module):\n",
    "    \"\"\"\n",
    "    Early Fusion: Combine RGB and Depth channels at the input level\n",
    "    before processing through the network\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, pretrained: bool = False, fusion_channels: int = 2048, dropout_rate: float = 0.4):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Create a single encoder with 4 input channels (3 RGB + 1 Depth)\n",
    "        self.encoder = InceptionV3Encoder(pretrained=pretrained, in_channels=4)\n",
    "        \n",
    "        # Regression head for calorie prediction\n",
    "        self.regression_head = RegressionHead(\n",
    "            in_channels=self.encoder.out_channels,\n",
    "            dropout_rate=dropout_rate\n",
    "        )\n",
    "    \n",
    "    def forward(self, rgb, depth):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            rgb: RGB images (B, 3, H, W)\n",
    "            depth: Depth images (B, 1, H, W)\n",
    "        \n",
    "        Returns:\n",
    "            Predicted calories (B, 1)\n",
    "        \"\"\"\n",
    "        # Concatenate RGB and depth along channel dimension\n",
    "        x = torch.cat([rgb, depth], dim=1)  # (B, 4, H, W)\n",
    "        \n",
    "        # Process through the encoder\n",
    "        features = self.encoder(x)\n",
    "        \n",
    "        # Predict calories\n",
    "        calories = self.regression_head(features)\n",
    "        \n",
    "        return calories\n",
    "\n",
    "# Late Fusion Module (RGB + Depth processed separately and fused at regression level)\n",
    "class LateFusion(nn.Module):\n",
    "    \"\"\"\n",
    "    Late Fusion: Process RGB and Depth streams independently, then fuse at the regression head level\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, pretrained: bool = False, fusion_channels: int = 2048, dropout_rate: float = 0.4):\n",
    "        super().__init__()\n",
    "        \n",
    "        # RGB and Depth encoders\n",
    "        self.rgb_encoder = InceptionV3Encoder(pretrained=pretrained, in_channels=3)\n",
    "        self.depth_encoder = InceptionV3Encoder(pretrained=pretrained, in_channels=1)\n",
    "        \n",
    "        # Global average pooling\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        \n",
    "        # Fusion at the feature vector level\n",
    "        in_features = self.rgb_encoder.out_channels + self.depth_encoder.out_channels\n",
    "        \n",
    "        # Fully connected layers for regression\n",
    "        self.regression_layers = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(in_features, 512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(256, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, rgb, depth):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            rgb: RGB images (B, 3, H, W)\n",
    "            depth: Depth images (B, 1, H, W)\n",
    "        \n",
    "        Returns:\n",
    "            Predicted calories (B, 1)\n",
    "        \"\"\"\n",
    "        # Extract features from both streams\n",
    "        rgb_features = self.rgb_encoder(rgb)    # (B, 2048, H/32, W/32)\n",
    "        depth_features = self.depth_encoder(depth)  # (B, 2048, H/32, W/32)\n",
    "        \n",
    "        # Apply global average pooling\n",
    "        rgb_features = self.avgpool(rgb_features)    # (B, 2048, 1, 1)\n",
    "        depth_features = self.avgpool(depth_features)  # (B, 2048, 1, 1)\n",
    "        \n",
    "        # Concatenate feature vectors\n",
    "        fused = torch.cat([rgb_features, depth_features], dim=1)  # (B, 4096, 1, 1)\n",
    "        \n",
    "        # Predict calories\n",
    "        calories = self.regression_layers(fused)\n",
    "        \n",
    "        return calories\n",
    "\n",
    "class RegressionHead(nn.Module):\n",
    "    def __init__(self, in_channels: int = 2048, dropout_rate: float = 0.4):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        \n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(in_channels, 512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(256, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.avgpool(x)  # (B, C, 1, 1)\n",
    "        x = self.fc_layers(x)  # (B, 1)\n",
    "        return x\n",
    "\n",
    "\n",
    "class VolumeEstimator(nn.Module):\n",
    "    \"\"\"\n",
    "    Food volume estimation from overhead depth images following the Nutrition5k paper.\n",
    "    \n",
    "    Given:\n",
    "    - Distance between camera and capture plane: 35.9 cm\n",
    "    - Per-pixel surface area at this distance: 5.957 × 10^-3 cm²\n",
    "    \n",
    "    The volume is calculated by:\n",
    "    1. Computing per-pixel volume (depth × surface_area)\n",
    "    2. Summing over all food pixels (using binary threshold segmentation)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 camera_distance: float = 35.9,  # cm\n",
    "                 pixel_surface_area: float = 5.957e-3,  # cm²\n",
    "                 depth_threshold: float = 0.1):  # Threshold for simple segmentation\n",
    "        super().__init__()\n",
    "        \n",
    "        self.camera_distance = camera_distance\n",
    "        self.pixel_surface_area = pixel_surface_area\n",
    "        self.depth_threshold = depth_threshold\n",
    "    \n",
    "    def forward(self, depth_images):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            depth_images: Depth images (B, 1, H, W), normalized to [0, 1] range\n",
    "        \n",
    "        Returns:\n",
    "            volume_estimates: Volume in cm³ for each image (B, 1)\n",
    "        \"\"\"\n",
    "        # Simple threshold-based segmentation for foreground/background\n",
    "        segmentation_mask = (depth_images > self.depth_threshold).float()\n",
    "        \n",
    "        # Convert normalized depth back to actual depth values\n",
    "        # Assuming depth is normalized to [0, 1] and represents distance from camera\n",
    "        # For simplicity, we assume the depth represents actual distance in cm scaled to [0, 1]\n",
    "        depth_cm = depth_images * self.camera_distance\n",
    "        \n",
    "        # Calculate per-pixel volume: depth × surface_area\n",
    "        per_pixel_volume = depth_cm * self.pixel_surface_area  # (B, 1, H, W)\n",
    "        \n",
    "        # Apply segmentation mask to consider only food pixels\n",
    "        masked_volume = per_pixel_volume * segmentation_mask\n",
    "        \n",
    "        # Sum over all pixels to get total volume\n",
    "        volume_estimates = masked_volume.sum(dim=[2, 3])  # (B, 1)\n",
    "        \n",
    "        return volume_estimates\n",
    "\n",
    "\n",
    "class RegressionHeadWithVolume(nn.Module):\n",
    "    \"\"\"\n",
    "    Regression head that concatenates volume estimate to InceptionV3 features.\n",
    "    \n",
    "    According to the paper: \"concatenating the volume estimation value to the output \n",
    "    of the InceptionV3 backbone, before the following two fully connected layers\"\n",
    "    with FC layers of 64 and 1 dimension.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, in_channels: int = 2048, dropout_rate: float = 0.4):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        \n",
    "        # Two FC layers as described in the paper (2048+1 -> 64 -> 1)\n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(in_channels + 1, 64),  # +1 for volume\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, features, volume):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            features: Feature maps from backbone (B, 2048, H, W)\n",
    "            volume: Volume estimates (B, 1)\n",
    "        \n",
    "        Returns:\n",
    "            Predicted calories (B, 1)\n",
    "        \"\"\"\n",
    "        # Global average pooling\n",
    "        x = self.avgpool(features)  # (B, 2048, 1, 1)\n",
    "        x = torch.flatten(x, 1)  # (B, 2048)\n",
    "        \n",
    "        # Concatenate volume estimate\n",
    "        x = torch.cat([x, volume], dim=1)  # (B, 2049)\n",
    "        \n",
    "        # Predict calories\n",
    "        x = self.fc_layers(x)  # (B, 1)\n",
    "        \n",
    "        return x\n",
    "\n",
    "class Nutrition5kModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Implementation of the dual-stream architecture used in the original Nutrition5k paper\n",
    "    Uses InceptionV3 as the backbone and middle fusion\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        fusion: str = 'middle',\n",
    "        fusion_channels: int = 2048,\n",
    "        dropout_rate: float = 0.4,\n",
    "        pretrained: bool = False,\n",
    "        use_volume: bool = False\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.use_volume = use_volume\n",
    "        \n",
    "        if fusion == 'early':\n",
    "            self.model = EarlyFusion(\n",
    "                pretrained=pretrained,\n",
    "                fusion_channels=fusion_channels,\n",
    "                dropout_rate=dropout_rate\n",
    "            )\n",
    "        elif fusion == 'late':\n",
    "            self.model = LateFusion(\n",
    "                pretrained=pretrained,\n",
    "                fusion_channels=fusion_channels,\n",
    "                dropout_rate=dropout_rate\n",
    "            )\n",
    "        elif fusion == 'image_only':\n",
    "            # Image-only variant: only RGB is used\n",
    "            self.rgb_encoder = InceptionV3Encoder(pretrained=pretrained, in_channels=3)\n",
    "            \n",
    "            # Volume estimator (if enabled)\n",
    "            if use_volume:\n",
    "                self.volume_estimator = VolumeEstimator()\n",
    "                self.regression_head = RegressionHeadWithVolume(\n",
    "                    in_channels=self.rgb_encoder.out_channels,\n",
    "                    dropout_rate=dropout_rate\n",
    "                )\n",
    "            else:\n",
    "                self.regression_head = RegressionHead(\n",
    "                    in_channels=self.rgb_encoder.out_channels,\n",
    "                    dropout_rate=dropout_rate\n",
    "                )\n",
    "        elif fusion == 'image_volume':\n",
    "            # Image+Volume variant: RGB encoder + volume as additional signal\n",
    "            self.rgb_encoder = InceptionV3Encoder(pretrained=pretrained, in_channels=3)\n",
    "            self.volume_estimator = VolumeEstimator()\n",
    "            self.regression_head = RegressionHeadWithVolume(\n",
    "                in_channels=self.rgb_encoder.out_channels,\n",
    "                dropout_rate=dropout_rate\n",
    "            )\n",
    "            self.use_volume = True  # Always use volume for this variant\n",
    "        else:  # middle fusion\n",
    "            # RGB and Depth encoders using InceptionV3\n",
    "            self.rgb_encoder = InceptionV3Encoder(pretrained=pretrained, in_channels=3)\n",
    "            self.depth_encoder = InceptionV3Encoder(pretrained=pretrained, in_channels=1)\n",
    "            \n",
    "            # Create middle fusion module\n",
    "            from_channels = self.rgb_encoder.out_channels + self.depth_encoder.out_channels\n",
    "            self.fusion_conv = nn.Sequential(\n",
    "                nn.Conv2d(from_channels, fusion_channels, kernel_size=1, bias=False),\n",
    "                nn.BatchNorm2d(fusion_channels),\n",
    "                nn.ReLU(inplace=True)\n",
    "            )\n",
    "            \n",
    "            # Volume estimator (if enabled)\n",
    "            if use_volume:\n",
    "                self.volume_estimator = VolumeEstimator()\n",
    "                self.regression_head = RegressionHeadWithVolume(\n",
    "                    in_channels=fusion_channels,\n",
    "                    dropout_rate=dropout_rate\n",
    "                )\n",
    "            else:\n",
    "                self.regression_head = RegressionHead(\n",
    "                    in_channels=fusion_channels,\n",
    "                    dropout_rate=dropout_rate\n",
    "                )\n",
    "    \n",
    "    def forward(self, rgb, depth):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            rgb: RGB images (B, 3, H, W)\n",
    "            depth: Depth images (B, 1, H, W)\n",
    "        \n",
    "        Returns:\n",
    "            calorie_pred: Predicted calories (B, 1)\n",
    "        \"\"\"\n",
    "        if hasattr(self, 'model'):\n",
    "            return self.model(rgb, depth)\n",
    "        \n",
    "        # Calculate volume estimate if enabled\n",
    "        volume = None\n",
    "        if self.use_volume and hasattr(self, 'volume_estimator'):\n",
    "            volume = self.volume_estimator(depth)  # (B, 1)\n",
    "        \n",
    "        # Image-only or Image+Volume variant\n",
    "        if hasattr(self, 'rgb_encoder') and not hasattr(self, 'depth_encoder'):\n",
    "            rgb_features = self.rgb_encoder(rgb)  # (B, 2048, H/32, W/32)\n",
    "            \n",
    "            if volume is not None:\n",
    "                calorie_pred = self.regression_head(rgb_features, volume)\n",
    "            else:\n",
    "                calorie_pred = self.regression_head(rgb_features)\n",
    "            \n",
    "            return calorie_pred\n",
    "        \n",
    "        # Extract features from both streams\n",
    "        rgb_features = self.rgb_encoder(rgb)      # (B, 2048, H/32, W/32)\n",
    "        depth_features = self.depth_encoder(depth)  # (B, 2048, H/32, W/32)\n",
    "        \n",
    "        # Middle fusion - concatenate and apply 1x1 conv\n",
    "        fused = torch.cat([rgb_features, depth_features], dim=1)  # (B, 4096, H/32, W/32)\n",
    "        fused = self.fusion_conv(fused)  # (B, 2048, H/32, W/32)\n",
    "        \n",
    "        # Predict calories (with or without volume)\n",
    "        if volume is not None:\n",
    "            calorie_pred = self.regression_head(fused, volume)\n",
    "        else:\n",
    "            calorie_pred = self.regression_head(fused)\n",
    "        \n",
    "        return calorie_pred\n",
    "    \n",
    "    def get_num_parameters(self):\n",
    "        \"\"\"Get total number of trainable parameters\"\"\"\n",
    "        return sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
    "\n",
    "# Factory function to build Nutrition5k models with different fusion types\n",
    "def build_nutrition5k_model(fusion='middle', pretrained=False, dropout_rate=0.4, fusion_channels=2048, \n",
    "                           use_volume=False, **kwargs):\n",
    "    \"\"\"\n",
    "    Factory function to build models using the Nutrition5k paper architecture (InceptionV3 backbone)\n",
    "    \n",
    "    Args:\n",
    "        fusion: Fusion type ('early', 'middle', 'late', 'image_only', or 'image_volume')\n",
    "        pretrained: Whether to use pretrained weights for InceptionV3\n",
    "        dropout_rate: Dropout rate for regression head\n",
    "        fusion_channels: Number of channels after fusion\n",
    "        use_volume: Whether to use volume estimation as additional signal (uses simple threshold-based segmentation)\n",
    "    \n",
    "    Returns:\n",
    "        Nutrition5k model with specified configuration\n",
    "    \"\"\"\n",
    "    return Nutrition5kModel(\n",
    "        fusion=fusion,\n",
    "        fusion_channels=fusion_channels,\n",
    "        dropout_rate=dropout_rate,\n",
    "        pretrained=pretrained,\n",
    "        use_volume=use_volume\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83e59b45",
   "metadata": {},
   "source": [
    "## 1.2 Trainer Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ed6abeca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def get_warmup_cosine_scheduler(optimizer, warmup_steps, total_steps, min_lr_ratio=0.0):\n",
    "    def lr_lambda(current_step):\n",
    "        if current_step < warmup_steps:\n",
    "            return float(current_step) / float(max(1, warmup_steps))\n",
    "        else:\n",
    "            progress = float(current_step - warmup_steps) / float(max(1, total_steps - warmup_steps))\n",
    "            return min_lr_ratio + (1.0 - min_lr_ratio) * 0.5 * (1.0 + math.cos(math.pi * progress))\n",
    "    \n",
    "    return optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n",
    "\n",
    "\n",
    "class EarlyStopping:\n",
    "    \"\"\"Early stopping to stop training when validation loss stops improving\"\"\"\n",
    "    \n",
    "    def __init__(self, patience: int = 10, min_delta: float = 0.0, mode: str = 'min'):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            patience: Number of epochs with no improvement after which training will be stopped\n",
    "            min_delta: Minimum change to qualify as an improvement\n",
    "            mode: 'min' or 'max' - whether lower or higher metric is better\n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.mode = mode\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.best_epoch = 0\n",
    "        \n",
    "    def __call__(self, score, epoch):\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.best_epoch = epoch\n",
    "            return False\n",
    "        \n",
    "        if self.mode == 'min':\n",
    "            improved = score < (self.best_score - self.min_delta)\n",
    "        else:\n",
    "            improved = score > (self.best_score + self.min_delta)\n",
    "        \n",
    "        if improved:\n",
    "            self.best_score = score\n",
    "            self.best_epoch = epoch\n",
    "            self.counter = 0\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "                \n",
    "        return self.early_stop\n",
    "\n",
    "\n",
    "class Trainer:\n",
    "    \"\"\"Training manager for calorie prediction\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        model,\n",
    "        train_loader,\n",
    "        val_loader,\n",
    "        criterion,\n",
    "        optimizer,\n",
    "        scheduler,\n",
    "        device,\n",
    "        output_dir,\n",
    "        early_stopping_patience=15,\n",
    "        scheduler_step_on_batch=False\n",
    "    ):\n",
    "        self.model = model\n",
    "        self.train_loader = train_loader\n",
    "        self.val_loader = val_loader\n",
    "        self.criterion = criterion\n",
    "        self.optimizer = optimizer\n",
    "        self.scheduler = scheduler\n",
    "        self.device = device\n",
    "        self.output_dir = output_dir\n",
    "        self.scheduler_step_on_batch = scheduler_step_on_batch\n",
    "        \n",
    "        # Early stopping\n",
    "        self.early_stopping = EarlyStopping(\n",
    "            patience=early_stopping_patience,\n",
    "            min_delta=0.1,\n",
    "            mode='min'\n",
    "        )\n",
    "        \n",
    "        # Tensorboard\n",
    "        self.writer = SummaryWriter(log_dir=os.path.join(output_dir, 'tensorboard'))\n",
    "        \n",
    "        # Tracking\n",
    "        self.best_val_loss = float('inf')\n",
    "        self.train_losses = []\n",
    "        self.val_losses = []\n",
    "        self.best_metrics = {}\n",
    "    \n",
    "    def train_epoch(self):\n",
    "        \"\"\"Train for one epoch\"\"\"\n",
    "        self.model.train()\n",
    "        total_loss = 0.0\n",
    "        num_batches = 0\n",
    "        \n",
    "        pbar = tqdm(self.train_loader, desc=\"Training\")\n",
    "        for batch_idx, batch in enumerate(pbar):\n",
    "            # Move to device\n",
    "            rgb = batch['rgb'].to(self.device)\n",
    "            depth = batch['depth'].to(self.device)\n",
    "            calories = batch['calorie'].to(self.device)\n",
    "            \n",
    "            # Forward pass\n",
    "            self.optimizer.zero_grad()\n",
    "            calorie_pred = self.model(rgb, depth)\n",
    "            \n",
    "            # Compute loss (MSE for calorie prediction)\n",
    "            loss = self.criterion(calorie_pred.squeeze(), calories)\n",
    "            \n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)\n",
    "            self.optimizer.step()\n",
    "            \n",
    "            # Update learning rate (if step_on_batch)\n",
    "            if self.scheduler_step_on_batch and self.scheduler:\n",
    "                self.scheduler.step()\n",
    "            \n",
    "            # Track metrics\n",
    "            total_loss += loss.item()\n",
    "            num_batches += 1\n",
    "            \n",
    "            # Update progress bar\n",
    "            pbar.set_postfix({'Loss': f'{loss.item():.4f}'})\n",
    "        \n",
    "        return total_loss / num_batches\n",
    "    \n",
    "    def validate_epoch(self):\n",
    "        \"\"\"Validate for one epoch\"\"\"\n",
    "        self.model.eval()\n",
    "        total_loss = 0.0\n",
    "        all_predictions = []\n",
    "        all_targets = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(self.val_loader, desc=\"Validation\"):\n",
    "                # Move to device\n",
    "                rgb = batch['rgb'].to(self.device)\n",
    "                depth = batch['depth'].to(self.device)\n",
    "                calories = batch['calorie'].to(self.device)\n",
    "                \n",
    "                # Forward pass\n",
    "                calorie_pred = self.model(rgb, depth)\n",
    "                \n",
    "                # Compute loss\n",
    "                loss = self.criterion(calorie_pred.squeeze(), calories)\n",
    "                total_loss += loss.item()\n",
    "                \n",
    "                # Store predictions and targets for metrics\n",
    "                all_predictions.extend(calorie_pred.squeeze().cpu().numpy())\n",
    "                all_targets.extend(calories.cpu().numpy())\n",
    "        \n",
    "        # Calculate metrics\n",
    "        avg_loss = total_loss / len(self.val_loader)\n",
    "        predictions = np.array(all_predictions)\n",
    "        targets = np.array(all_targets)\n",
    "        \n",
    "        mae = np.mean(np.abs(predictions - targets))\n",
    "        \n",
    "        return avg_loss, mae\n",
    "    \n",
    "    def train(self, num_epochs):\n",
    "        \"\"\"Full training loop\"\"\"\n",
    "        print(f\"Starting training for {num_epochs} epochs...\")\n",
    "        \n",
    "        for epoch in range(num_epochs):\n",
    "            print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
    "            \n",
    "            # Train\n",
    "            train_loss = self.train_epoch()\n",
    "            \n",
    "            # Validate\n",
    "            val_loss, mae = self.validate_epoch()\n",
    "            \n",
    "            # Update learning rate (if not step_on_batch)\n",
    "            if not self.scheduler_step_on_batch and self.scheduler:\n",
    "                self.scheduler.step(val_loss)\n",
    "            \n",
    "            # Log metrics\n",
    "            self.writer.add_scalar('Loss/Train', train_loss, epoch)\n",
    "            self.writer.add_scalar('Loss/Val', val_loss, epoch)\n",
    "            self.writer.add_scalar('MAE', mae, epoch)\n",
    "            \n",
    "            # Save best model\n",
    "            if val_loss < self.best_val_loss:\n",
    "                self.best_val_loss = val_loss\n",
    "                self.best_metrics = {\n",
    "                    'epoch': epoch + 1,\n",
    "                    'val_loss': val_loss,\n",
    "                    'mae': mae,\n",
    "                }\n",
    "                \n",
    "                # Save model checkpoint\n",
    "                torch.save({\n",
    "                    'epoch': epoch + 1,\n",
    "                    'model_state_dict': self.model.state_dict(),\n",
    "                    'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "                    'val_loss': val_loss,\n",
    "                    'mae': mae,\n",
    "                }, os.path.join(self.output_dir, 'best_model.pth'))\n",
    "            \n",
    "            # Print epoch results\n",
    "            print(f\"Train Loss: {train_loss:.4f}\")\n",
    "            print(f\"Val Loss: {val_loss:.4f}\")\n",
    "            print(f\"MAE: {mae:.2f}\")\n",
    "            \n",
    "            # Early stopping\n",
    "            if self.early_stopping(val_loss, epoch):\n",
    "                print(f\"Early stopping triggered after {epoch+1} epochs\")\n",
    "                print(f\"Best epoch: {self.early_stopping.best_epoch+1}\")\n",
    "                break\n",
    "        \n",
    "        self.writer.close()\n",
    "        print(f\"\\nTraining completed!\")\n",
    "        print(f\"Best validation loss: {self.best_val_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9edad408",
   "metadata": {},
   "source": [
    "# 2. Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8abbd3fa",
   "metadata": {},
   "source": [
    "## 2.1 Dataset Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54abd461",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset Implementation\n",
    "class Nutrition5KDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset class for Nutrition5K with multi-modal inputs (RGB + Depth)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        csv_path: str,\n",
    "        data_root: str,\n",
    "        split: str = 'train',\n",
    "        augment: bool = True,\n",
    "        img_size: int = 224,\n",
    "    ):\n",
    "        self.data_root = data_root\n",
    "        self.split = split\n",
    "        self.augment = augment\n",
    "        self.img_size = img_size\n",
    "        \n",
    "        # Load CSV\n",
    "        self.df = pd.read_csv(csv_path)\n",
    "        if 'Value' in self.df.columns and 'calories' not in self.df.columns:\n",
    "            self.df = self.df.rename(columns={'Value': 'calories'})\n",
    "        if 'calories' not in self.df.columns:\n",
    "            raise ValueError(\"CSV file must contain a 'calories' column or a 'Value' column that can be renamed\")\n",
    "        self.df = self.df[self.df['calories'] < 3000].reset_index(drop=True)\n",
    "                \n",
    "        self.color_dir = os.path.join(data_root, 'color')\n",
    "        self.depth_raw_dir = os.path.join(data_root, 'depth_raw')\n",
    "        \n",
    "        self.valid_indices = self._validate_dataset()\n",
    "        print(f\"Loaded {len(self.valid_indices)} valid samples out of {len(self.df)}\")\n",
    "        \n",
    "        # Color normalization (ImageNet stats as baseline)\n",
    "        self.color_normalize = T.Normalize(\n",
    "            mean=[0.485, 0.456, 0.406],\n",
    "            std=[0.229, 0.224, 0.225]\n",
    "        )\n",
    "        \n",
    "    def _validate_dataset(self):\n",
    "        \"\"\"This method ensure that the code don't break when there are corrupted images.\"\"\"\n",
    "        valid_indices = []\n",
    "        \n",
    "        for idx in range(len(self.df)):\n",
    "            dish_id = self.df.iloc[idx]['ID']\n",
    "            \n",
    "            rgb_path = os.path.join(self.color_dir, dish_id, 'rgb.png')\n",
    "            depth_path = os.path.join(self.depth_raw_dir, dish_id, 'depth_raw.png')\n",
    "            \n",
    "            # Check if files exist\n",
    "            if not os.path.exists(rgb_path):\n",
    "                continue\n",
    "            if not os.path.exists(depth_path):\n",
    "                continue\n",
    "            \n",
    "            # Try to load images to check for corruption\n",
    "            try:\n",
    "                with Image.open(rgb_path) as img:\n",
    "                    img.verify()\n",
    "                with Image.open(depth_path) as img:\n",
    "                    img.verify()\n",
    "                valid_indices.append(idx)\n",
    "            except Exception as e:\n",
    "                continue\n",
    "                \n",
    "        return valid_indices\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.valid_indices)\n",
    "    \n",
    "    def _load_image_safe(self, path: str, mode: str = 'RGB') -> Optional[Image.Image]:\n",
    "        \"\"\"Safely load an image with error handling\"\"\"\n",
    "        try:\n",
    "            with Image.open(path) as img:\n",
    "                return img.convert(mode).copy()\n",
    "        except Exception as e:\n",
    "            return None\n",
    "    \n",
    "    def _apply_augmentation(self, rgb_img, depth_img):\n",
    "        \"\"\"Apply geometric augmentation only (no color changes)\"\"\"\n",
    "        if not self.augment:\n",
    "            return rgb_img, depth_img\n",
    "        \n",
    "        # Convert to tensors first\n",
    "        rgb_tensor = TF.to_tensor(rgb_img)\n",
    "        depth_tensor = TF.to_tensor(depth_img)\n",
    "        \n",
    "        # Random horizontal flip\n",
    "        if random.random() > 0.5:\n",
    "            rgb_tensor = TF.hflip(rgb_tensor)\n",
    "            depth_tensor = TF.hflip(depth_tensor)\n",
    "        \n",
    "        # Random rotation (±15 degrees)\n",
    "        if random.random() > 0.5:\n",
    "            angle = random.uniform(-15, 15)\n",
    "            rgb_tensor = TF.rotate(rgb_tensor, angle)\n",
    "            depth_tensor = TF.rotate(depth_tensor, angle)\n",
    "        \n",
    "        # Convert back to PIL\n",
    "        rgb_img = TF.to_pil_image(rgb_tensor)\n",
    "        depth_img = TF.to_pil_image(depth_tensor)\n",
    "        \n",
    "        return rgb_img, depth_img\n",
    "    \n",
    "    def _resize_and_center_crop(self, img, target_size: int = 256):\n",
    "        \"\"\"\n",
    "        Resize and center crop image to target_size x target_size\n",
    "        Matches the preprocessing in the Nutrition5k paper\n",
    "        \n",
    "        Args:\n",
    "            img: PIL Image\n",
    "            target_size: Target size (default 256x256 as per paper)\n",
    "        \n",
    "        Returns:\n",
    "            Cropped PIL Image\n",
    "        \"\"\"\n",
    "        # Get original dimensions\n",
    "        width, height = img.size\n",
    "        \n",
    "        # Resize so the shorter side is target_size\n",
    "        if width < height:\n",
    "            new_width = target_size\n",
    "            new_height = int(target_size * height / width)\n",
    "        else:\n",
    "            new_height = target_size\n",
    "            new_width = int(target_size * width / height)\n",
    "        \n",
    "        img = img.resize((new_width, new_height), Image.LANCZOS)\n",
    "        \n",
    "        # Center crop to target_size x target_size\n",
    "        left = (new_width - target_size) // 2\n",
    "        top = (new_height - target_size) // 2\n",
    "        right = left + target_size\n",
    "        bottom = top + target_size\n",
    "        \n",
    "        img = img.crop((left, top, right, bottom))\n",
    "        \n",
    "        return img\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"Get a single sample\"\"\"\n",
    "        actual_idx = self.valid_indices[idx]\n",
    "        row = self.df.iloc[actual_idx]\n",
    "        \n",
    "        dish_id = row['ID']\n",
    "        calorie = float(row['calories'])\n",
    "        \n",
    "        # Load images\n",
    "        rgb_path = os.path.join(self.color_dir, dish_id, 'rgb.png')\n",
    "        depth_path = os.path.join(self.depth_raw_dir, dish_id, 'depth_raw.png')\n",
    "        \n",
    "        rgb_img = self._load_image_safe(rgb_path, 'RGB')\n",
    "        depth_img = self._load_image_safe(depth_path, 'L')  # Grayscale for depth\n",
    "        \n",
    "        # Fallback: return a black image\n",
    "        if rgb_img is None or depth_img is None:\n",
    "            rgb_img = Image.new('RGB', (self.img_size, self.img_size), (0, 0, 0))\n",
    "            depth_img = Image.new('L', (self.img_size, self.img_size), 0)\n",
    "        \n",
    "        # Apply augmentation\n",
    "        rgb_img, depth_img = self._apply_augmentation(rgb_img, depth_img)\n",
    "        \n",
    "        # Resize and center crop to match paper preprocessing (256x256)\n",
    "        rgb_img = self._resize_and_center_crop(rgb_img, target_size=self.img_size)\n",
    "        depth_img = self._resize_and_center_crop(depth_img, target_size=self.img_size)\n",
    "        \n",
    "        # Convert to tensors\n",
    "        rgb_tensor = TF.to_tensor(rgb_img)  # (3, H, W)\n",
    "        depth_tensor = TF.to_tensor(depth_img)  # (1, H, W)\n",
    "        \n",
    "        # Normalize RGB\n",
    "        rgb_tensor = self.color_normalize(rgb_tensor)\n",
    "        \n",
    "        # Normalize depth (0-1 range, assuming depth is already in reasonable range)\n",
    "        depth_tensor = depth_tensor / 255.0\n",
    "        \n",
    "        return {\n",
    "            'dish_id': dish_id,\n",
    "            'rgb': rgb_tensor,\n",
    "            'depth': depth_tensor,\n",
    "            'calorie': torch.tensor(calorie, dtype=torch.float32)\n",
    "        }\n",
    "\n",
    "\n",
    "def create_train_val_split(csv_path: str, val_ratio: float = 0.15, random_seed: int = 42):\n",
    "    \"\"\"\n",
    "    Create train/validation split CSV files\n",
    "    \"\"\"\n",
    "    # Read original CSV\n",
    "    df = pd.read_csv(csv_path)    \n",
    "    \n",
    "    # Shuffle with fixed seed\n",
    "    df_shuffled = df.sample(frac=1, random_state=random_seed).reset_index(drop=True)\n",
    "    \n",
    "    # Split\n",
    "    val_size = int(len(df_shuffled) * val_ratio)\n",
    "    train_df = df_shuffled[val_size:]\n",
    "    val_df = df_shuffled[:val_size]\n",
    "    \n",
    "    # Save temporary CSV files\n",
    "    base_dir = os.path.dirname(csv_path)\n",
    "    train_csv = os.path.join(base_dir, 'train_split.csv')\n",
    "    val_csv = os.path.join(base_dir, 'val_split.csv')\n",
    "    \n",
    "    train_df.to_csv(train_csv, index=False)\n",
    "    val_df.to_csv(val_csv, index=False)\n",
    "    \n",
    "    return train_csv, val_csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcf2b5ab",
   "metadata": {},
   "source": [
    "## 2.2 Dataset Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9d0805ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration:\n",
      "  Data root: ./Nutrition5K/Nutrition5K/train\n",
      "  CSV path: ./Nutrition5K/Nutrition5K/nutrition5k_train.csv\n",
      "  Output directory: ./experiments\n",
      "  Batch size: 32\n",
      "  Number of epochs: 40\n",
      "  Image size: 256\n",
      "  Workers: 4\n"
     ]
    }
   ],
   "source": [
    "# Configuration - Update these paths to match your setup\n",
    "DATA_ROOT = './Nutrition5K/Nutrition5K/train'  # Path to training data directory\n",
    "CSV_PATH = './Nutrition5K/Nutrition5K/nutrition5k_train.csv'  # Path to training CSV\n",
    "OUTPUT_DIR = './experiments'  # Directory to save experiment results\n",
    "\n",
    "# Global training hyperparameters (learning rate and weight decay set per experiment)\n",
    "BATCH_SIZE = 32\n",
    "NUM_EPOCHS = 40\n",
    "VAL_RATIO = 0.15\n",
    "IMG_SIZE = 256\n",
    "NUM_WORKERS = 4\n",
    "\n",
    "print(\"Configuration:\")\n",
    "print(f\"  Data root: {DATA_ROOT}\")\n",
    "print(f\"  CSV path: {CSV_PATH}\")\n",
    "print(f\"  Output directory: {OUTPUT_DIR}\")\n",
    "print(f\"  Batch size: {BATCH_SIZE}\")\n",
    "print(f\"  Number of epochs: {NUM_EPOCHS}\")\n",
    "print(f\"  Image size: {IMG_SIZE}\")\n",
    "print(f\"  Workers: {NUM_WORKERS}\")\n",
    "\n",
    "# Create output directory\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1969721f",
   "metadata": {},
   "source": [
    "### Dataset Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2c3f4f6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating train/validation split...\n",
      "Train CSV: ./Nutrition5K/Nutrition5K/train_split.csv\n",
      "Validation CSV: ./Nutrition5K/Nutrition5K/val_split.csv\n",
      "Loaded 2804 valid samples out of 2805\n",
      "\n",
      "Dataset loaded successfully!\n",
      "Training samples: 2804\n",
      "RGB shape: torch.Size([3, 256, 256])\n",
      "Depth shape: torch.Size([1, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "# Create train/validation split\n",
    "print(\"Creating train/validation split...\")\n",
    "train_csv, val_csv = create_train_val_split(\n",
    "    CSV_PATH,\n",
    "    val_ratio=VAL_RATIO,\n",
    "    random_seed=42\n",
    ")\n",
    "\n",
    "print(f\"Train CSV: {train_csv}\")\n",
    "print(f\"Validation CSV: {val_csv}\")\n",
    "\n",
    "# Load a sample to check data\n",
    "sample_dataset = Nutrition5KDataset(\n",
    "    csv_path=train_csv,\n",
    "    data_root=DATA_ROOT,\n",
    "    split='train',\n",
    "    augment=False,  # No augmentation for checking\n",
    "    img_size=IMG_SIZE,\n",
    ")\n",
    "\n",
    "print(f\"\\nDataset loaded successfully!\")\n",
    "print(f\"Training samples: {len(sample_dataset)}\")\n",
    "print(f\"RGB shape: {sample_dataset[0]['rgb'].shape}\")\n",
    "print(f\"Depth shape: {sample_dataset[0]['depth'].shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d718780",
   "metadata": {},
   "source": [
    "# 3. Experiments\n",
    "We'll conduct experiments to compare different fusion strategies using the InceptionV3 architecture.\n",
    "\n",
    "**Architecture**: InceptionV3\n",
    "- **RGB encoder**: InceptionV3 (in_channels=3)\n",
    "- **Depth encoder**: InceptionV3 (in_channels=1) \n",
    "- **Fusion**: Various fusion strategies (early, middle, late)\n",
    "- **Volume estimation**: Optional food volume calculation from depth images\n",
    "\n",
    "**Experiments**:\n",
    "1. **InceptionV3 - Middle Fusion**: RGB and Depth features concatenated at feature map level, then fused with 1×1 conv\n",
    "2. **InceptionV3 - Early Fusion**: RGB and Depth concatenated at input level (4 channels), processed by single encoder\n",
    "3. **InceptionV3 - Late Fusion**: RGB and Depth processed separately, features concatenated after global pooling\n",
    "4. **InceptionV3 - Image+Volume**: RGB encoder only + volume estimate from depth as additional signal\n",
    "5. **InceptionV3 - Middle+Volume**: Middle fusion (RGB+Depth) + volume estimate as additional signal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "480c9c51",
   "metadata": {},
   "source": [
    "## 3.1 InceptionV3 - Middle Fusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dcecae0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "TRAINING: Nutrition5k InceptionV3 + Middle Fusion\n",
      "============================================================\n",
      "Loaded 2804 valid samples out of 2805\n",
      "Loaded 495 valid samples out of 495\n",
      "Model parameters: 53,143,873\n",
      "Training samples: 2804\n",
      "Validation samples: 495\n",
      "Learning rate: 0.0003\n",
      "Weight decay: 1e-06\n",
      "Starting training for 40 epochs...\n",
      "\n",
      "Epoch 1/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:15<00:00,  5.72it/s, Loss=128568.6797]\n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 99179.5700\n",
      "Val Loss: 107412.9351\n",
      "MAE: 240.64\n",
      "\n",
      "Epoch 2/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:14<00:00,  6.18it/s, Loss=14551.3506] \n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 44613.2511\n",
      "Val Loss: 22115.9072\n",
      "MAE: 103.60\n",
      "\n",
      "Epoch 3/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.24it/s, Loss=8529.9629] \n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 14990.2540\n",
      "Val Loss: 15339.2216\n",
      "MAE: 89.52\n",
      "\n",
      "Epoch 4/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.24it/s, Loss=10611.9082]\n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 12844.5238\n",
      "Val Loss: 14614.1202\n",
      "MAE: 86.15\n",
      "\n",
      "Epoch 5/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:14<00:00,  6.19it/s, Loss=10989.2109]\n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 12333.9567\n",
      "Val Loss: 13903.3691\n",
      "MAE: 84.82\n",
      "\n",
      "Epoch 6/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.34it/s, Loss=7037.2222] \n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 13701.8433\n",
      "Val Loss: 15968.9327\n",
      "MAE: 87.62\n",
      "\n",
      "Epoch 7/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.43it/s, Loss=9634.6934] \n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 9885.7110\n",
      "Val Loss: 13036.7188\n",
      "MAE: 78.05\n",
      "\n",
      "Epoch 8/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.32it/s, Loss=15831.9697]\n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 13061.2367\n",
      "Val Loss: 37848.2272\n",
      "MAE: 150.94\n",
      "\n",
      "Epoch 9/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.28it/s, Loss=15873.6582]\n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 13630.2193\n",
      "Val Loss: 26709.6769\n",
      "MAE: 118.80\n",
      "\n",
      "Epoch 10/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.29it/s, Loss=12883.2695]\n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 11182.7567\n",
      "Val Loss: 16951.6092\n",
      "MAE: 94.59\n",
      "\n",
      "Epoch 11/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.38it/s, Loss=8178.9229] \n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 8048.6062\n",
      "Val Loss: 12620.6396\n",
      "MAE: 74.06\n",
      "\n",
      "Epoch 12/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.34it/s, Loss=7492.9854] \n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 10334.1747\n",
      "Val Loss: 15044.2647\n",
      "MAE: 86.57\n",
      "\n",
      "Epoch 13/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.30it/s, Loss=3736.9170] \n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 7712.2835\n",
      "Val Loss: 10110.7316\n",
      "MAE: 70.29\n",
      "\n",
      "Epoch 14/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.35it/s, Loss=12397.4609]\n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 4862.7000\n",
      "Val Loss: 9211.0626\n",
      "MAE: 63.99\n",
      "\n",
      "Epoch 15/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.34it/s, Loss=4223.6538] \n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 4543.4875\n",
      "Val Loss: 9900.7881\n",
      "MAE: 65.54\n",
      "\n",
      "Epoch 16/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.34it/s, Loss=2259.6072] \n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 4426.4736\n",
      "Val Loss: 9328.8389\n",
      "MAE: 66.59\n",
      "\n",
      "Epoch 17/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.31it/s, Loss=1298.2256] \n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 3814.8753\n",
      "Val Loss: 8683.5598\n",
      "MAE: 63.18\n",
      "\n",
      "Epoch 18/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:14<00:00,  6.18it/s, Loss=4756.7666] \n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 4460.9755\n",
      "Val Loss: 9520.8630\n",
      "MAE: 66.56\n",
      "\n",
      "Epoch 19/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.25it/s, Loss=3241.6943] \n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 4638.1852\n",
      "Val Loss: 8618.1980\n",
      "MAE: 63.57\n",
      "\n",
      "Epoch 20/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.23it/s, Loss=2759.9041] \n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 4441.7072\n",
      "Val Loss: 10380.2209\n",
      "MAE: 66.44\n",
      "\n",
      "Epoch 21/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.29it/s, Loss=8066.5176] \n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 4397.2711\n",
      "Val Loss: 9014.0540\n",
      "MAE: 64.05\n",
      "\n",
      "Epoch 22/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.23it/s, Loss=2299.1699] \n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 3708.5228\n",
      "Val Loss: 8564.3510\n",
      "MAE: 61.92\n",
      "\n",
      "Epoch 23/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.30it/s, Loss=3101.3701] \n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 3872.7702\n",
      "Val Loss: 9195.6286\n",
      "MAE: 63.51\n",
      "\n",
      "Epoch 24/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.28it/s, Loss=1211.8209] \n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 3619.2267\n",
      "Val Loss: 8286.5531\n",
      "MAE: 59.58\n",
      "\n",
      "Epoch 25/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.31it/s, Loss=4860.5879] \n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 4642.6053\n",
      "Val Loss: 11539.1972\n",
      "MAE: 76.86\n",
      "\n",
      "Epoch 26/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.31it/s, Loss=5091.5918] \n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 5310.7747\n",
      "Val Loss: 9601.1501\n",
      "MAE: 65.47\n",
      "\n",
      "Epoch 27/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.28it/s, Loss=3598.9797]\n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 4402.4465\n",
      "Val Loss: 8586.3208\n",
      "MAE: 60.98\n",
      "\n",
      "Epoch 28/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.28it/s, Loss=3286.5671] \n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 4379.7765\n",
      "Val Loss: 9463.2135\n",
      "MAE: 64.45\n",
      "\n",
      "Epoch 29/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.28it/s, Loss=3635.2405] \n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 3820.1549\n",
      "Val Loss: 7934.8172\n",
      "MAE: 57.35\n",
      "\n",
      "Epoch 30/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.23it/s, Loss=9945.4971] \n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 5754.6514\n",
      "Val Loss: 10848.7680\n",
      "MAE: 67.77\n",
      "\n",
      "Epoch 31/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:14<00:00,  6.12it/s, Loss=2592.0410] \n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 4085.6081\n",
      "Val Loss: 8705.7453\n",
      "MAE: 62.18\n",
      "\n",
      "Epoch 32/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.28it/s, Loss=1845.0674] \n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 3637.0483\n",
      "Val Loss: 8562.5231\n",
      "MAE: 62.51\n",
      "\n",
      "Epoch 33/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.27it/s, Loss=5132.6309] \n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 3630.7866\n",
      "Val Loss: 9332.6526\n",
      "MAE: 62.17\n",
      "\n",
      "Epoch 34/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.27it/s, Loss=1935.2677] \n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 3029.1384\n",
      "Val Loss: 7578.5881\n",
      "MAE: 56.28\n",
      "\n",
      "Epoch 35/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.31it/s, Loss=5809.5571] \n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 4695.0795\n",
      "Val Loss: 9082.4254\n",
      "MAE: 66.55\n",
      "\n",
      "Epoch 36/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.28it/s, Loss=2360.2610] \n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 4250.1567\n",
      "Val Loss: 8774.8670\n",
      "MAE: 62.02\n",
      "\n",
      "Epoch 37/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.41it/s, Loss=5048.2212]\n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 3479.9186\n",
      "Val Loss: 9305.3121\n",
      "MAE: 62.28\n",
      "\n",
      "Epoch 38/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.33it/s, Loss=1282.8358] \n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 3230.0179\n",
      "Val Loss: 7757.0370\n",
      "MAE: 56.59\n",
      "\n",
      "Epoch 39/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.33it/s, Loss=3625.0686] \n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 4668.2630\n",
      "Val Loss: 12126.1959\n",
      "MAE: 82.30\n",
      "\n",
      "Epoch 40/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.33it/s, Loss=8918.3311] \n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 5625.5605\n",
      "Val Loss: 14431.2035\n",
      "MAE: 85.84\n",
      "\n",
      "Training completed!\n",
      "Best validation loss: 7578.5881\n",
      "\n",
      "Experiment completed! Results saved to: ../experiments/nutrition5k_experiments/inceptionv3_middle_fusion_20251024_130923\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Define experiment hyperparamers\n",
    "# BATCH_SIZE = 32\n",
    "NUM_EPOCHS = 40\n",
    "DROPOUT_RATE = 0.4\n",
    "LEARNING_RATE = 3e-4\n",
    "WEIGHT_DECAY = 1e-6\n",
    "EARLY_STOPPING_PATIENCE = 15\n",
    "WARMUP_RATIO = 0.1\n",
    "MIN_LR_RATIO = 0.05\n",
    "FUSION_CHANNELS = 2048\n",
    "\n",
    "def train_nutrition5k_model(fusion_type='middle'):\n",
    "    \"\"\"Train the Nutrition5k model with InceptionV3 and specified fusion type\"\"\"\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "    print(f\"TRAINING: Nutrition5k InceptionV3 + {fusion_type.capitalize()} Fusion\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Create datasets\n",
    "    train_dataset = Nutrition5KDataset(\n",
    "        csv_path=train_csv,\n",
    "        data_root=DATA_ROOT,\n",
    "        split='train',\n",
    "        augment=False,\n",
    "        img_size=IMG_SIZE,\n",
    "    )\n",
    "    \n",
    "    val_dataset = Nutrition5KDataset(\n",
    "        csv_path=val_csv,\n",
    "        data_root=DATA_ROOT,\n",
    "        split='val',\n",
    "        augment=False,\n",
    "        img_size=IMG_SIZE,\n",
    "    )\n",
    "    \n",
    "    # Create data loaders\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=True,\n",
    "        num_workers=NUM_WORKERS,\n",
    "        pin_memory=True if torch.cuda.is_available() else False,\n",
    "        drop_last=True\n",
    "    )\n",
    "    \n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=False,\n",
    "        num_workers=NUM_WORKERS,\n",
    "        pin_memory=True if torch.cuda.is_available() else False\n",
    "    )\n",
    "    \n",
    "    # Build model with specified fusion type\n",
    "    model = build_nutrition5k_model(\n",
    "        fusion=fusion_type,\n",
    "        pretrained=False,\n",
    "        dropout_rate=DROPOUT_RATE,\n",
    "        fusion_channels=FUSION_CHANNELS\n",
    "    )\n",
    "    model = model.to(device)\n",
    "    \n",
    "    print(f\"Model parameters: {model.get_num_parameters():,}\")\n",
    "    print(f\"Training samples: {len(train_dataset)}\")\n",
    "    print(f\"Validation samples: {len(val_dataset)}\")\n",
    "    \n",
    "    # Loss function\n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    # Optimizer\n",
    "    optimizer = optim.AdamW(\n",
    "        model.parameters(),\n",
    "        lr=LEARNING_RATE,\n",
    "        weight_decay=WEIGHT_DECAY\n",
    "    )\n",
    "    \n",
    "    print(f\"Learning rate: {LEARNING_RATE}\")\n",
    "    print(f\"Weight decay: {WEIGHT_DECAY}\")\n",
    "    \n",
    "    # Learning rate scheduler\n",
    "    steps_per_epoch = len(train_loader)\n",
    "    total_steps = NUM_EPOCHS * steps_per_epoch\n",
    "    warmup_steps = int(total_steps * WARMUP_RATIO)\n",
    "    \n",
    "    scheduler = get_warmup_cosine_scheduler(\n",
    "        optimizer, \n",
    "        warmup_steps=warmup_steps, \n",
    "        total_steps=total_steps,\n",
    "        min_lr_ratio=MIN_LR_RATIO\n",
    "    )\n",
    "    \n",
    "    # Create experiment directory\n",
    "    exp_name = f\"inceptionv3_{fusion_type}_fusion_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "    exp_dir = os.path.join(OUTPUT_DIR, 'nutrition5k_experiments', exp_name)\n",
    "    os.makedirs(exp_dir, exist_ok=True)\n",
    "    \n",
    "    # Save experiment configuration\n",
    "    config = {\n",
    "        'fusion': fusion_type,\n",
    "        'pretrained': False,\n",
    "        'dropout_rate': DROPOUT_RATE,\n",
    "        'fusion_channels': FUSION_CHANNELS,\n",
    "        'learning_rate': LEARNING_RATE,\n",
    "        'weight_decay': WEIGHT_DECAY,\n",
    "        'batch_size': BATCH_SIZE,\n",
    "        'img_size': IMG_SIZE,\n",
    "        'num_epochs': NUM_EPOCHS\n",
    "    }\n",
    "    \n",
    "    with open(os.path.join(exp_dir, 'config.json'), 'w') as f:\n",
    "        json.dump(config, f, indent=4)\n",
    "    \n",
    "    # Create trainer\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        criterion=criterion,\n",
    "        optimizer=optimizer,\n",
    "        scheduler=scheduler,\n",
    "        device=device,\n",
    "        output_dir=exp_dir,\n",
    "        early_stopping_patience=EARLY_STOPPING_PATIENCE,\n",
    "        scheduler_step_on_batch=False\n",
    "    )\n",
    "    \n",
    "    # Train the model\n",
    "    trainer.train(NUM_EPOCHS)\n",
    "    \n",
    "    print(f\"\\nExperiment completed! Results saved to: {exp_dir}\")\n",
    "    return trainer.best_metrics\n",
    "\n",
    "# Run an experiment with middle fusion\n",
    "middle_fusion_results = train_nutrition5k_model(fusion_type='middle')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eee7ad5",
   "metadata": {},
   "source": [
    "## 3.2 InceptionV3 - Early Fusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2cd5dc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "TRAINING: Nutrition5k InceptionV3 + Early Fusion\n",
      "============================================================\n",
      "Loaded 2804 valid samples out of 2805\n",
      "Loaded 495 valid samples out of 495\n",
      "Model parameters: 22,966,465\n",
      "Training samples: 2804\n",
      "Validation samples: 495\n",
      "Learning rate: 0.0003\n",
      "Weight decay: 1e-06\n",
      "Starting training for 40 epochs...\n",
      "\n",
      "Epoch 1/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.49it/s, Loss=66747.9062] \n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 99325.6495\n",
      "Val Loss: 107376.2798\n",
      "MAE: 240.56\n",
      "\n",
      "Epoch 2/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.57it/s, Loss=22843.2305] \n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 42928.3323\n",
      "Val Loss: 19980.8986\n",
      "MAE: 103.48\n",
      "\n",
      "Epoch 3/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.55it/s, Loss=10708.5781]\n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 17106.7717\n",
      "Val Loss: 19836.2162\n",
      "MAE: 95.29\n",
      "\n",
      "Epoch 4/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.48it/s, Loss=12754.4180]\n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 17037.9287\n",
      "Val Loss: 29315.9156\n",
      "MAE: 112.77\n",
      "\n",
      "Epoch 5/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.52it/s, Loss=11109.4961]\n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 12419.4974\n",
      "Val Loss: 12350.2427\n",
      "MAE: 78.81\n",
      "\n",
      "Epoch 6/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.60it/s, Loss=8752.6113] \n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 13488.8264\n",
      "Val Loss: 12729.3925\n",
      "MAE: 76.98\n",
      "\n",
      "Epoch 7/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.46it/s, Loss=8419.4541] \n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 12929.6196\n",
      "Val Loss: 21976.7961\n",
      "MAE: 100.01\n",
      "\n",
      "Epoch 8/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.56it/s, Loss=4679.3589] \n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 10146.2328\n",
      "Val Loss: 10747.8688\n",
      "MAE: 71.96\n",
      "\n",
      "Epoch 9/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.50it/s, Loss=5635.6797] \n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 8792.4451\n",
      "Val Loss: 10783.5305\n",
      "MAE: 71.93\n",
      "\n",
      "Epoch 10/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.62it/s, Loss=5006.7964] \n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 7525.5879\n",
      "Val Loss: 11707.3728\n",
      "MAE: 71.07\n",
      "\n",
      "Epoch 11/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.63it/s, Loss=4415.2920] \n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 8775.2512\n",
      "Val Loss: 11203.9194\n",
      "MAE: 70.39\n",
      "\n",
      "Epoch 12/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.59it/s, Loss=8206.9492] \n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 7488.8060\n",
      "Val Loss: 11448.6037\n",
      "MAE: 70.75\n",
      "\n",
      "Epoch 13/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.58it/s, Loss=20127.1152]\n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 7698.7578\n",
      "Val Loss: 19493.1765\n",
      "MAE: 90.35\n",
      "\n",
      "Epoch 14/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.55it/s, Loss=12658.3789]\n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 8682.6980\n",
      "Val Loss: 18340.8633\n",
      "MAE: 85.01\n",
      "\n",
      "Epoch 15/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.58it/s, Loss=5315.3403] \n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 7846.1895\n",
      "Val Loss: 13579.7655\n",
      "MAE: 80.14\n",
      "\n",
      "Epoch 16/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.62it/s, Loss=7562.2861] \n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 8193.6605\n",
      "Val Loss: 10145.4527\n",
      "MAE: 69.51\n",
      "\n",
      "Epoch 17/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.41it/s, Loss=2269.3503] \n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 5541.4729\n",
      "Val Loss: 7903.5556\n",
      "MAE: 58.91\n",
      "\n",
      "Epoch 18/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.57it/s, Loss=8156.2046] \n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 6543.4155\n",
      "Val Loss: 8652.4755\n",
      "MAE: 62.92\n",
      "\n",
      "Epoch 19/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.58it/s, Loss=2912.4287] \n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 4890.6906\n",
      "Val Loss: 8625.3224\n",
      "MAE: 62.41\n",
      "\n",
      "Epoch 20/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.47it/s, Loss=3772.5403] \n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 5196.3465\n",
      "Val Loss: 10165.2951\n",
      "MAE: 63.64\n",
      "\n",
      "Epoch 21/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.46it/s, Loss=1960.0081] \n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 3562.4957\n",
      "Val Loss: 7652.7168\n",
      "MAE: 54.76\n",
      "\n",
      "Epoch 22/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.45it/s, Loss=17051.5156]\n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 5509.5268\n",
      "Val Loss: 12240.1665\n",
      "MAE: 74.17\n",
      "\n",
      "Epoch 23/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.50it/s, Loss=3583.2676] \n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 6604.5801\n",
      "Val Loss: 12108.0193\n",
      "MAE: 69.45\n",
      "\n",
      "Epoch 24/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.45it/s, Loss=7632.9282] \n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 6506.6554\n",
      "Val Loss: 14720.9593\n",
      "MAE: 77.30\n",
      "\n",
      "Epoch 25/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.45it/s, Loss=2055.4907] \n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 4572.6020\n",
      "Val Loss: 9063.0943\n",
      "MAE: 59.88\n",
      "\n",
      "Epoch 26/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.50it/s, Loss=1919.8328] \n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 3421.9659\n",
      "Val Loss: 7674.3759\n",
      "MAE: 56.12\n",
      "\n",
      "Epoch 27/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.42it/s, Loss=1848.4658] \n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 4576.4738\n",
      "Val Loss: 9714.7449\n",
      "MAE: 61.45\n",
      "\n",
      "Epoch 28/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.40it/s, Loss=2453.5884] \n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 3644.3707\n",
      "Val Loss: 7877.2721\n",
      "MAE: 55.54\n",
      "\n",
      "Epoch 29/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.46it/s, Loss=4258.2334] \n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 4365.7270\n",
      "Val Loss: 11066.9588\n",
      "MAE: 69.82\n",
      "\n",
      "Epoch 30/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.44it/s, Loss=7192.0464]\n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 3430.3935\n",
      "Val Loss: 7680.3667\n",
      "MAE: 57.24\n",
      "\n",
      "Epoch 31/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.50it/s, Loss=3328.4053] \n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 4283.5907\n",
      "Val Loss: 8796.7368\n",
      "MAE: 64.82\n",
      "\n",
      "Epoch 32/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.56it/s, Loss=2363.3750] \n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 3478.3367\n",
      "Val Loss: 7675.7208\n",
      "MAE: 58.12\n",
      "\n",
      "Epoch 33/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.48it/s, Loss=5590.2734] \n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 3638.4839\n",
      "Val Loss: 9331.1797\n",
      "MAE: 65.83\n",
      "\n",
      "Epoch 34/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.28it/s, Loss=4025.5542] \n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 3395.1593\n",
      "Val Loss: 7289.9086\n",
      "MAE: 54.76\n",
      "\n",
      "Epoch 35/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.47it/s, Loss=1952.4146] \n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 4409.1664\n",
      "Val Loss: 10487.2523\n",
      "MAE: 68.48\n",
      "\n",
      "Epoch 36/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.56it/s, Loss=5168.2134] \n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 3580.7155\n",
      "Val Loss: 7592.5793\n",
      "MAE: 55.65\n",
      "\n",
      "Epoch 37/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.44it/s, Loss=5320.6187] \n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 4219.9357\n",
      "Val Loss: 9092.4391\n",
      "MAE: 63.40\n",
      "\n",
      "Epoch 38/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.43it/s, Loss=2338.5122]\n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 2817.4882\n",
      "Val Loss: 8004.2764\n",
      "MAE: 54.92\n",
      "\n",
      "Epoch 39/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.47it/s, Loss=2916.2383]\n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 3088.7458\n",
      "Val Loss: 8371.1610\n",
      "MAE: 58.62\n",
      "\n",
      "Epoch 40/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.51it/s, Loss=2650.1313] \n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 2973.2887\n",
      "Val Loss: 9626.2067\n",
      "MAE: 60.45\n",
      "\n",
      "Training completed!\n",
      "Best validation loss: 7289.9086\n",
      "\n",
      "Experiment completed! Results saved to: ../experiments/nutrition5k_experiments/inceptionv3_early_fusion_20251024_132025\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Define experiment hyperparamers\n",
    "BATCH_SIZE = 32\n",
    "NUM_EPOCHS = 40\n",
    "DROPOUT_RATE = 0.4\n",
    "LEARNING_RATE = 3e-4\n",
    "WEIGHT_DECAY = 1e-6\n",
    "EARLY_STOPPING_PATIENCE = 15\n",
    "WARMUP_RATIO = 0.1\n",
    "MIN_LR_RATIO = 0.05\n",
    "FUSION_CHANNELS = 2048\n",
    "\n",
    "def train_nutrition5k_model(fusion_type='middle'):\n",
    "    \"\"\"Train the Nutrition5k model with InceptionV3 and specified fusion type\"\"\"\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "    print(f\"TRAINING: Nutrition5k InceptionV3 + {fusion_type.capitalize()} Fusion\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Create datasets\n",
    "    train_dataset = Nutrition5KDataset(\n",
    "        csv_path=train_csv,\n",
    "        data_root=DATA_ROOT,\n",
    "        split='train',\n",
    "        augment=False,\n",
    "        img_size=IMG_SIZE,\n",
    "    )\n",
    "    \n",
    "    val_dataset = Nutrition5KDataset(\n",
    "        csv_path=val_csv,\n",
    "        data_root=DATA_ROOT,\n",
    "        split='val',\n",
    "        augment=False,\n",
    "        img_size=IMG_SIZE,\n",
    "    )\n",
    "    \n",
    "    # Create data loaders\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=True,\n",
    "        num_workers=NUM_WORKERS,\n",
    "        pin_memory=True if torch.cuda.is_available() else False,\n",
    "        drop_last=True\n",
    "    )\n",
    "    \n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=False,\n",
    "        num_workers=NUM_WORKERS,\n",
    "        pin_memory=True if torch.cuda.is_available() else False\n",
    "    )\n",
    "    \n",
    "    # Build model with specified fusion type\n",
    "    model = build_nutrition5k_model(\n",
    "        fusion=fusion_type,\n",
    "        pretrained=False,\n",
    "        dropout_rate=DROPOUT_RATE,\n",
    "        fusion_channels=FUSION_CHANNELS\n",
    "    )\n",
    "    model = model.to(device)\n",
    "    \n",
    "    print(f\"Model parameters: {model.get_num_parameters():,}\")\n",
    "    print(f\"Training samples: {len(train_dataset)}\")\n",
    "    print(f\"Validation samples: {len(val_dataset)}\")\n",
    "    \n",
    "    # Loss function\n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    # Optimizer\n",
    "    optimizer = optim.AdamW(\n",
    "        model.parameters(),\n",
    "        lr=LEARNING_RATE,\n",
    "        weight_decay=WEIGHT_DECAY\n",
    "    )\n",
    "    \n",
    "    print(f\"Learning rate: {LEARNING_RATE}\")\n",
    "    print(f\"Weight decay: {WEIGHT_DECAY}\")\n",
    "    \n",
    "    # Learning rate scheduler\n",
    "    steps_per_epoch = len(train_loader)\n",
    "    total_steps = NUM_EPOCHS * steps_per_epoch\n",
    "    warmup_steps = int(total_steps * WARMUP_RATIO)\n",
    "    \n",
    "    scheduler = get_warmup_cosine_scheduler(\n",
    "        optimizer, \n",
    "        warmup_steps=warmup_steps, \n",
    "        total_steps=total_steps,\n",
    "        min_lr_ratio=MIN_LR_RATIO\n",
    "    )\n",
    "    \n",
    "    # Create experiment directory\n",
    "    exp_name = f\"inceptionv3_{fusion_type}_fusion_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "    exp_dir = os.path.join(OUTPUT_DIR, 'nutrition5k_experiments', exp_name)\n",
    "    os.makedirs(exp_dir, exist_ok=True)\n",
    "    \n",
    "    # Save experiment configuration\n",
    "    config = {\n",
    "        'fusion': fusion_type,\n",
    "        'pretrained': False,\n",
    "        'dropout_rate': DROPOUT_RATE,\n",
    "        'fusion_channels': FUSION_CHANNELS,\n",
    "        'learning_rate': LEARNING_RATE,\n",
    "        'weight_decay': WEIGHT_DECAY,\n",
    "        'batch_size': BATCH_SIZE,\n",
    "        'img_size': IMG_SIZE,\n",
    "        'num_epochs': NUM_EPOCHS\n",
    "    }\n",
    "    \n",
    "    with open(os.path.join(exp_dir, 'config.json'), 'w') as f:\n",
    "        json.dump(config, f, indent=4)\n",
    "    \n",
    "    # Create trainer\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        criterion=criterion,\n",
    "        optimizer=optimizer,\n",
    "        scheduler=scheduler,\n",
    "        device=device,\n",
    "        output_dir=exp_dir,\n",
    "        early_stopping_patience=EARLY_STOPPING_PATIENCE,\n",
    "        scheduler_step_on_batch=False\n",
    "    )\n",
    "    \n",
    "    # Train the model\n",
    "    trainer.train(NUM_EPOCHS)\n",
    "    \n",
    "    print(f\"\\nExperiment completed! Results saved to: {exp_dir}\")\n",
    "    return trainer.best_metrics\n",
    "\n",
    "# Run an experiment with middle fusion\n",
    "early_fusion_results = train_nutrition5k_model(fusion_type='early')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe967232",
   "metadata": {},
   "source": [
    "## 3.3 InceptionV3 - Late Fusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b791dc23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "TRAINING: Nutrition5k InceptionV3 + Late Fusion\n",
      "============================================================\n",
      "Loaded 2804 valid samples out of 2805\n",
      "Loaded 495 valid samples out of 495\n",
      "Model parameters: 45,799,745\n",
      "Training samples: 2804\n",
      "Validation samples: 495\n",
      "Learning rate: 0.0003\n",
      "Weight decay: 1e-06\n",
      "Starting training for 40 epochs...\n",
      "\n",
      "Epoch 1/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.40it/s, Loss=106439.0938]\n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 98389.8398\n",
      "Val Loss: 107367.1013\n",
      "MAE: 240.54\n",
      "\n",
      "Epoch 2/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.25it/s, Loss=28412.5645]\n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 41690.2808\n",
      "Val Loss: 31997.4932\n",
      "MAE: 123.44\n",
      "\n",
      "Epoch 3/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.32it/s, Loss=11473.9180]\n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 17350.7568\n",
      "Val Loss: 19154.0181\n",
      "MAE: 99.54\n",
      "\n",
      "Epoch 4/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.25it/s, Loss=10099.2783]\n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 14514.1249\n",
      "Val Loss: 18284.8318\n",
      "MAE: 101.39\n",
      "\n",
      "Epoch 5/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.37it/s, Loss=8358.5098] \n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 13459.6902\n",
      "Val Loss: 13000.7582\n",
      "MAE: 83.92\n",
      "\n",
      "Epoch 6/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:14<00:00,  6.19it/s, Loss=11738.3379]\n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 11558.1775\n",
      "Val Loss: 14764.5721\n",
      "MAE: 93.13\n",
      "\n",
      "Epoch 7/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.37it/s, Loss=5837.0171] \n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 7952.4247\n",
      "Val Loss: 10476.4639\n",
      "MAE: 72.17\n",
      "\n",
      "Epoch 8/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.38it/s, Loss=5708.1650] \n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 6212.3241\n",
      "Val Loss: 10303.3420\n",
      "MAE: 67.74\n",
      "\n",
      "Epoch 9/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.28it/s, Loss=7737.3613] \n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 5673.2633\n",
      "Val Loss: 8837.4902\n",
      "MAE: 63.97\n",
      "\n",
      "Epoch 10/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.35it/s, Loss=9406.4307] \n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 5494.6568\n",
      "Val Loss: 9756.0814\n",
      "MAE: 69.59\n",
      "\n",
      "Epoch 11/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.27it/s, Loss=4610.4297] \n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 5146.7232\n",
      "Val Loss: 8753.4821\n",
      "MAE: 63.10\n",
      "\n",
      "Epoch 12/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.35it/s, Loss=5909.7256] \n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 4907.9320\n",
      "Val Loss: 9570.8997\n",
      "MAE: 65.66\n",
      "\n",
      "Epoch 13/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.37it/s, Loss=4963.5718] \n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 4037.6672\n",
      "Val Loss: 8444.3814\n",
      "MAE: 61.17\n",
      "\n",
      "Epoch 14/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.28it/s, Loss=7843.6504] \n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 4759.5694\n",
      "Val Loss: 9681.8932\n",
      "MAE: 66.88\n",
      "\n",
      "Epoch 15/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.27it/s, Loss=9072.1475] \n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 4229.9857\n",
      "Val Loss: 8599.4233\n",
      "MAE: 62.60\n",
      "\n",
      "Epoch 16/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.22it/s, Loss=2737.9417] \n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 4172.8494\n",
      "Val Loss: 10331.9440\n",
      "MAE: 70.47\n",
      "\n",
      "Epoch 17/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.33it/s, Loss=3295.8223] \n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 4371.7250\n",
      "Val Loss: 8629.5308\n",
      "MAE: 65.55\n",
      "\n",
      "Epoch 18/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.30it/s, Loss=4097.1982] \n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 4424.5678\n",
      "Val Loss: 8790.1743\n",
      "MAE: 65.26\n",
      "\n",
      "Epoch 19/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.38it/s, Loss=2633.3958] \n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 4350.4276\n",
      "Val Loss: 8730.1980\n",
      "MAE: 62.17\n",
      "\n",
      "Epoch 20/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.25it/s, Loss=4055.3335] \n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 4177.5882\n",
      "Val Loss: 14836.8188\n",
      "MAE: 79.11\n",
      "\n",
      "Epoch 21/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.23it/s, Loss=2228.0410] \n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 4229.4994\n",
      "Val Loss: 9068.2230\n",
      "MAE: 66.28\n",
      "\n",
      "Epoch 22/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.30it/s, Loss=1770.4275] \n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 3978.3809\n",
      "Val Loss: 8781.0372\n",
      "MAE: 61.05\n",
      "\n",
      "Epoch 23/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.32it/s, Loss=3093.6758] \n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 3067.3215\n",
      "Val Loss: 8732.8710\n",
      "MAE: 64.11\n",
      "\n",
      "Epoch 24/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.42it/s, Loss=3645.4475] \n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 3630.3558\n",
      "Val Loss: 8726.5923\n",
      "MAE: 62.54\n",
      "\n",
      "Epoch 25/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.28it/s, Loss=1093.2750] \n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 3741.8051\n",
      "Val Loss: 9379.7649\n",
      "MAE: 62.13\n",
      "\n",
      "Epoch 26/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.33it/s, Loss=1971.1019] \n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 3466.8906\n",
      "Val Loss: 8442.0509\n",
      "MAE: 58.94\n",
      "\n",
      "Epoch 27/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.32it/s, Loss=4339.6250] \n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 4190.1686\n",
      "Val Loss: 8390.8783\n",
      "MAE: 63.37\n",
      "\n",
      "Epoch 28/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.33it/s, Loss=2052.4866] \n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 3881.3878\n",
      "Val Loss: 9256.8487\n",
      "MAE: 65.92\n",
      "\n",
      "Epoch 29/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.30it/s, Loss=12281.3994]\n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 3442.6623\n",
      "Val Loss: 7819.5536\n",
      "MAE: 57.46\n",
      "\n",
      "Epoch 30/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.38it/s, Loss=4406.4668] \n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 5363.2380\n",
      "Val Loss: 13414.7968\n",
      "MAE: 77.36\n",
      "\n",
      "Epoch 31/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.43it/s, Loss=4942.8740] \n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 6517.6254\n",
      "Val Loss: 24391.3788\n",
      "MAE: 103.60\n",
      "\n",
      "Epoch 32/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.48it/s, Loss=4491.8784] \n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 5878.0402\n",
      "Val Loss: 10430.6566\n",
      "MAE: 65.04\n",
      "\n",
      "Epoch 33/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.23it/s, Loss=2186.6104] \n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 4494.0169\n",
      "Val Loss: 8213.3772\n",
      "MAE: 59.08\n",
      "\n",
      "Epoch 34/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.37it/s, Loss=7051.7349] \n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 3761.5112\n",
      "Val Loss: 10694.6189\n",
      "MAE: 66.91\n",
      "\n",
      "Epoch 35/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.29it/s, Loss=2668.6448]\n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 3711.7728\n",
      "Val Loss: 7766.9089\n",
      "MAE: 59.69\n",
      "\n",
      "Epoch 36/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.28it/s, Loss=3103.6143] \n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 4424.6941\n",
      "Val Loss: 12087.1532\n",
      "MAE: 74.20\n",
      "\n",
      "Epoch 37/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.33it/s, Loss=3766.8242] \n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 5107.7292\n",
      "Val Loss: 16110.6986\n",
      "MAE: 81.93\n",
      "\n",
      "Epoch 38/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.27it/s, Loss=6010.6538] \n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 4091.8209\n",
      "Val Loss: 8399.1342\n",
      "MAE: 57.81\n",
      "\n",
      "Epoch 39/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.40it/s, Loss=2086.4343] \n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 4500.5079\n",
      "Val Loss: 9125.9881\n",
      "MAE: 62.44\n",
      "\n",
      "Epoch 40/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.41it/s, Loss=1422.8608]\n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 3323.3737\n",
      "Val Loss: 8567.3797\n",
      "MAE: 60.29\n",
      "\n",
      "Training completed!\n",
      "Best validation loss: 7766.9089\n",
      "\n",
      "Experiment completed! Results saved to: ../experiments/nutrition5k_experiments/inceptionv3_late_fusion_20251024_133101\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Define experiment hyperparamers\n",
    "BATCH_SIZE = 32\n",
    "NUM_EPOCHS = 40\n",
    "DROPOUT_RATE = 0.4\n",
    "LEARNING_RATE = 3e-4\n",
    "WEIGHT_DECAY = 1e-6\n",
    "EARLY_STOPPING_PATIENCE = 15\n",
    "WARMUP_RATIO = 0.1\n",
    "MIN_LR_RATIO = 0.05\n",
    "FUSION_CHANNELS = 2048\n",
    "\n",
    "def train_nutrition5k_model(fusion_type='middle'):\n",
    "    \"\"\"Train the Nutrition5k model with InceptionV3 and specified fusion type\"\"\"\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "    print(f\"TRAINING: Nutrition5k InceptionV3 + {fusion_type.capitalize()} Fusion\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Create datasets\n",
    "    train_dataset = Nutrition5KDataset(\n",
    "        csv_path=train_csv,\n",
    "        data_root=DATA_ROOT,\n",
    "        split='train',\n",
    "        augment=False,\n",
    "        img_size=IMG_SIZE,\n",
    "    )\n",
    "    \n",
    "    val_dataset = Nutrition5KDataset(\n",
    "        csv_path=val_csv,\n",
    "        data_root=DATA_ROOT,\n",
    "        split='val',\n",
    "        augment=False,\n",
    "        img_size=IMG_SIZE,\n",
    "    )\n",
    "    \n",
    "    # Create data loaders\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=True,\n",
    "        num_workers=NUM_WORKERS,\n",
    "        pin_memory=True if torch.cuda.is_available() else False,\n",
    "        drop_last=True\n",
    "    )\n",
    "    \n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=False,\n",
    "        num_workers=NUM_WORKERS,\n",
    "        pin_memory=True if torch.cuda.is_available() else False\n",
    "    )\n",
    "    \n",
    "    # Build model with specified fusion type\n",
    "    model = build_nutrition5k_model(\n",
    "        fusion=fusion_type,\n",
    "        pretrained=False,\n",
    "        dropout_rate=DROPOUT_RATE,\n",
    "        fusion_channels=FUSION_CHANNELS\n",
    "    )\n",
    "    model = model.to(device)\n",
    "    \n",
    "    print(f\"Model parameters: {model.get_num_parameters():,}\")\n",
    "    print(f\"Training samples: {len(train_dataset)}\")\n",
    "    print(f\"Validation samples: {len(val_dataset)}\")\n",
    "    \n",
    "    # Loss function\n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    # Optimizer\n",
    "    optimizer = optim.AdamW(\n",
    "        model.parameters(),\n",
    "        lr=LEARNING_RATE,\n",
    "        weight_decay=WEIGHT_DECAY\n",
    "    )\n",
    "    \n",
    "    print(f\"Learning rate: {LEARNING_RATE}\")\n",
    "    print(f\"Weight decay: {WEIGHT_DECAY}\")\n",
    "    \n",
    "    # Learning rate scheduler\n",
    "    steps_per_epoch = len(train_loader)\n",
    "    total_steps = NUM_EPOCHS * steps_per_epoch\n",
    "    warmup_steps = int(total_steps * WARMUP_RATIO)\n",
    "    \n",
    "    scheduler = get_warmup_cosine_scheduler(\n",
    "        optimizer, \n",
    "        warmup_steps=warmup_steps, \n",
    "        total_steps=total_steps,\n",
    "        min_lr_ratio=MIN_LR_RATIO\n",
    "    )\n",
    "    \n",
    "    # Create experiment directory\n",
    "    exp_name = f\"inceptionv3_{fusion_type}_fusion_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "    exp_dir = os.path.join(OUTPUT_DIR, 'nutrition5k_experiments', exp_name)\n",
    "    os.makedirs(exp_dir, exist_ok=True)\n",
    "    \n",
    "    # Save experiment configuration\n",
    "    config = {\n",
    "        'fusion': fusion_type,\n",
    "        'pretrained': False,\n",
    "        'dropout_rate': DROPOUT_RATE,\n",
    "        'fusion_channels': FUSION_CHANNELS,\n",
    "        'learning_rate': LEARNING_RATE,\n",
    "        'weight_decay': WEIGHT_DECAY,\n",
    "        'batch_size': BATCH_SIZE,\n",
    "        'img_size': IMG_SIZE,\n",
    "        'num_epochs': NUM_EPOCHS\n",
    "    }\n",
    "    \n",
    "    with open(os.path.join(exp_dir, 'config.json'), 'w') as f:\n",
    "        json.dump(config, f, indent=4)\n",
    "    \n",
    "    # Create trainer\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        criterion=criterion,\n",
    "        optimizer=optimizer,\n",
    "        scheduler=scheduler,\n",
    "        device=device,\n",
    "        output_dir=exp_dir,\n",
    "        early_stopping_patience=EARLY_STOPPING_PATIENCE,\n",
    "        scheduler_step_on_batch=False\n",
    "    )\n",
    "    \n",
    "    # Train the model\n",
    "    trainer.train(NUM_EPOCHS)\n",
    "    \n",
    "    print(f\"\\nExperiment completed! Results saved to: {exp_dir}\")\n",
    "    return trainer.best_metrics\n",
    "\n",
    "# Run an experiment with middle fusion\n",
    "late_fusion_results = train_nutrition5k_model(fusion_type='late')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b743bb8",
   "metadata": {},
   "source": [
    "## 3.4 InceptionV3 - Image + Volume"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52dfc49d",
   "metadata": {},
   "source": [
    "This section implements the food volume estimation method as described in the Nutrition5k paper. The method:\n",
    "\n",
    "1. **Estimates food volume from overhead depth images** using:\n",
    "   - Camera distance: 35.9 cm\n",
    "   - Per-pixel surface area: 5.957 × 10⁻³ cm²\n",
    "   \n",
    "2. **Uses binary foreground/background segmentation** to identify food pixels\n",
    "\n",
    "3. **Calculates volume** by summing per-pixel volumes (depth × surface_area) over all food pixels\n",
    "\n",
    "4. **Concatenates volume estimate** to the InceptionV3 backbone output before FC layers\n",
    "\n",
    "We implement three variants:\n",
    "- **Image+Volume**: RGB + volume estimate as additional signal  \n",
    "- **Middle+Volume**: RGB + Depth fusion + volume estimate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ba819d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "TRAINING: Nutrition5k InceptionV3 + IMAGE_VOLUME\n",
      "============================================================\n",
      "Loaded 2804 valid samples out of 2805\n",
      "Loaded 495 valid samples out of 495\n",
      "Model parameters: 21,916,833\n",
      "Training samples: 2804\n",
      "Validation samples: 495\n",
      "Learning rate: 0.0005\n",
      "Weight decay: 1e-06\n",
      "Starting training for 40 epochs...\n",
      "\n",
      "Epoch 1/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.43it/s, Loss=108259.2578]\n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 98651.2712\n",
      "Val Loss: 107381.6948\n",
      "MAE: 240.57\n",
      "\n",
      "Epoch 2/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.54it/s, Loss=16013.5312] \n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 65919.9727\n",
      "Val Loss: 29319.6537\n",
      "MAE: 118.82\n",
      "\n",
      "Epoch 3/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.54it/s, Loss=21910.8262]\n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 26365.2399\n",
      "Val Loss: 25831.0439\n",
      "MAE: 110.35\n",
      "\n",
      "Epoch 4/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.54it/s, Loss=28824.6973]\n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 19369.3317\n",
      "Val Loss: 26086.2035\n",
      "MAE: 106.78\n",
      "\n",
      "Epoch 5/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.52it/s, Loss=12186.3164]\n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 16064.0011\n",
      "Val Loss: 22472.0131\n",
      "MAE: 101.50\n",
      "\n",
      "Epoch 6/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.43it/s, Loss=7965.6699] \n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 14435.9392\n",
      "Val Loss: 12856.3794\n",
      "MAE: 76.76\n",
      "\n",
      "Epoch 7/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.66it/s, Loss=11353.6094]\n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 14900.7881\n",
      "Val Loss: 16189.5482\n",
      "MAE: 85.86\n",
      "\n",
      "Epoch 8/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.52it/s, Loss=11691.4023]\n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 13149.4884\n",
      "Val Loss: 11781.9545\n",
      "MAE: 76.72\n",
      "\n",
      "Epoch 9/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.55it/s, Loss=11914.6602]\n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 12836.9574\n",
      "Val Loss: 12081.9814\n",
      "MAE: 75.09\n",
      "\n",
      "Epoch 10/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.65it/s, Loss=5833.5947] \n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 12259.6983\n",
      "Val Loss: 34023.9789\n",
      "MAE: 135.16\n",
      "\n",
      "Epoch 11/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.51it/s, Loss=7867.6704] \n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 10627.7787\n",
      "Val Loss: 8857.1537\n",
      "MAE: 64.40\n",
      "\n",
      "Epoch 12/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.45it/s, Loss=8124.0371] \n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 8637.2702\n",
      "Val Loss: 12734.0982\n",
      "MAE: 73.18\n",
      "\n",
      "Epoch 13/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.48it/s, Loss=5287.5591] \n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 12329.5391\n",
      "Val Loss: 11926.0453\n",
      "MAE: 73.37\n",
      "\n",
      "Epoch 14/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.53it/s, Loss=4309.2856] \n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 10885.5472\n",
      "Val Loss: 13142.9994\n",
      "MAE: 78.21\n",
      "\n",
      "Epoch 15/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.51it/s, Loss=20139.8066]\n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 10598.2898\n",
      "Val Loss: 14689.3955\n",
      "MAE: 81.68\n",
      "\n",
      "Epoch 16/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.41it/s, Loss=7086.2871] \n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 8293.4044\n",
      "Val Loss: 10231.8171\n",
      "MAE: 66.58\n",
      "\n",
      "Epoch 17/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.57it/s, Loss=12543.9473]\n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 6399.1024\n",
      "Val Loss: 9016.7161\n",
      "MAE: 60.06\n",
      "\n",
      "Epoch 18/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.55it/s, Loss=5665.5361] \n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 5826.5098\n",
      "Val Loss: 8145.8707\n",
      "MAE: 57.01\n",
      "\n",
      "Epoch 19/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.39it/s, Loss=5068.1343] \n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 7035.3614\n",
      "Val Loss: 9229.9055\n",
      "MAE: 61.73\n",
      "\n",
      "Epoch 20/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.56it/s, Loss=5287.8306] \n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 6057.8679\n",
      "Val Loss: 7616.8182\n",
      "MAE: 56.79\n",
      "\n",
      "Epoch 21/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.43it/s, Loss=4746.9902] \n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 7072.8501\n",
      "Val Loss: 24474.4791\n",
      "MAE: 99.65\n",
      "\n",
      "Epoch 22/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.53it/s, Loss=13119.5117]\n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 7839.2704\n",
      "Val Loss: 10457.4959\n",
      "MAE: 65.65\n",
      "\n",
      "Epoch 23/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.62it/s, Loss=3085.6042] \n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 6021.8229\n",
      "Val Loss: 7191.0435\n",
      "MAE: 55.98\n",
      "\n",
      "Epoch 24/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.61it/s, Loss=6064.8857] \n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 7473.2339\n",
      "Val Loss: 9309.3742\n",
      "MAE: 63.62\n",
      "\n",
      "Epoch 25/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.55it/s, Loss=3918.4976] \n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 6366.5307\n",
      "Val Loss: 7951.3516\n",
      "MAE: 57.45\n",
      "\n",
      "Epoch 26/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.59it/s, Loss=3958.7363] \n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 5714.5977\n",
      "Val Loss: 8676.6589\n",
      "MAE: 61.97\n",
      "\n",
      "Epoch 27/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.59it/s, Loss=4025.3110] \n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 6011.6670\n",
      "Val Loss: 8741.7768\n",
      "MAE: 62.08\n",
      "\n",
      "Epoch 28/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.63it/s, Loss=3402.2810] \n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 5449.7801\n",
      "Val Loss: 9306.1301\n",
      "MAE: 59.73\n",
      "\n",
      "Epoch 29/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.50it/s, Loss=3335.1790] \n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 4545.7973\n",
      "Val Loss: 7175.3438\n",
      "MAE: 54.02\n",
      "\n",
      "Epoch 30/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.53it/s, Loss=10892.3789]\n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 7070.0363\n",
      "Val Loss: 13045.5674\n",
      "MAE: 77.76\n",
      "\n",
      "Epoch 31/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.51it/s, Loss=5397.4434] \n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 6838.4158\n",
      "Val Loss: 10400.0666\n",
      "MAE: 64.40\n",
      "\n",
      "Epoch 32/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.44it/s, Loss=4656.6797] \n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 5807.6003\n",
      "Val Loss: 8157.4585\n",
      "MAE: 59.51\n",
      "\n",
      "Epoch 33/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.52it/s, Loss=1558.7113] \n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 5879.0825\n",
      "Val Loss: 8046.5217\n",
      "MAE: 57.06\n",
      "\n",
      "Epoch 34/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.62it/s, Loss=7381.2354] \n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 6066.1867\n",
      "Val Loss: 9070.3390\n",
      "MAE: 60.64\n",
      "\n",
      "Epoch 35/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.55it/s, Loss=3927.4146] \n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 4413.8461\n",
      "Val Loss: 7681.4189\n",
      "MAE: 54.37\n",
      "\n",
      "Epoch 36/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.51it/s, Loss=9086.3516] \n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 5761.2919\n",
      "Val Loss: 9406.7413\n",
      "MAE: 65.93\n",
      "\n",
      "Epoch 37/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.50it/s, Loss=6872.7788] \n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 4781.9159\n",
      "Val Loss: 7606.9587\n",
      "MAE: 55.86\n",
      "\n",
      "Epoch 38/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.54it/s, Loss=6143.3491] \n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 5480.3070\n",
      "Val Loss: 10385.1789\n",
      "MAE: 63.28\n",
      "\n",
      "Epoch 39/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.49it/s, Loss=3758.9685] \n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 4822.6640\n",
      "Val Loss: 7814.0743\n",
      "MAE: 54.46\n",
      "\n",
      "Epoch 40/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.51it/s, Loss=2849.0405] \n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 5745.1829\n",
      "Val Loss: 9136.1897\n",
      "MAE: 59.73\n",
      "\n",
      "Training completed!\n",
      "Best validation loss: 7175.3438\n",
      "\n",
      "Experiment completed! Results saved to: ../experiments/nutrition5k_experiments/inceptionv3_image_volume_20251024_140149\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Define experiment hyperparamers\n",
    "BATCH_SIZE = 32\n",
    "NUM_EPOCHS = 40\n",
    "DROPOUT_RATE = 0.4\n",
    "LEARNING_RATE = 5e-4\n",
    "WEIGHT_DECAY = 1e-6\n",
    "EARLY_STOPPING_PATIENCE = 15\n",
    "WARMUP_RATIO = 0.1\n",
    "MIN_LR_RATIO = 0.05\n",
    "FUSION_CHANNELS = 2048\n",
    "\n",
    "# Training function with volume estimation support\n",
    "def train_nutrition5k_with_volume(fusion_type='image_volume'):\n",
    "    \"\"\"\n",
    "    Train the Nutrition5k model with volume estimation\n",
    "    \n",
    "    Args:\n",
    "        fusion_type: 'image_only', 'image_volume', 'middle', etc.\n",
    "        use_segmentation: Whether to use learned segmentation for volume estimation\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "    print(f\"TRAINING: Nutrition5k InceptionV3 + {fusion_type.upper()}\")\n",
    "\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Create datasets\n",
    "    train_dataset = Nutrition5KDataset(\n",
    "        csv_path=train_csv,\n",
    "        data_root=DATA_ROOT,\n",
    "        split='train',\n",
    "        augment=False,\n",
    "        img_size=IMG_SIZE,\n",
    "    )\n",
    "    \n",
    "    val_dataset = Nutrition5KDataset(\n",
    "        csv_path=val_csv,\n",
    "        data_root=DATA_ROOT,\n",
    "        split='val',\n",
    "        augment=False,\n",
    "        img_size=IMG_SIZE,\n",
    "    )\n",
    "    \n",
    "    # Create data loaders\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=True,\n",
    "        num_workers=NUM_WORKERS,\n",
    "        pin_memory=True if torch.cuda.is_available() else False,\n",
    "        drop_last=True\n",
    "    )\n",
    "    \n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=False,\n",
    "        num_workers=NUM_WORKERS,\n",
    "        pin_memory=True if torch.cuda.is_available() else False\n",
    "    )\n",
    "    \n",
    "\n",
    "   # Use Volume    \n",
    "    model = build_nutrition5k_model(\n",
    "        fusion=fusion_type,\n",
    "        pretrained=False,\n",
    "        dropout_rate=DROPOUT_RATE,\n",
    "        fusion_channels=FUSION_CHANNELS,\n",
    "        use_volume=True,\n",
    "    )\n",
    "    model = model.to(device)\n",
    "    \n",
    "    print(f\"Model parameters: {model.get_num_parameters():,}\")\n",
    "    print(f\"Training samples: {len(train_dataset)}\")\n",
    "    print(f\"Validation samples: {len(val_dataset)}\")\n",
    "    \n",
    "    # Loss function\n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    # Optimizer\n",
    "    optimizer = optim.AdamW(\n",
    "        model.parameters(),\n",
    "        lr=LEARNING_RATE,\n",
    "        weight_decay=WEIGHT_DECAY\n",
    "    )\n",
    "    \n",
    "    print(f\"Learning rate: {LEARNING_RATE}\")\n",
    "    print(f\"Weight decay: {WEIGHT_DECAY}\")\n",
    "    \n",
    "    # Learning rate scheduler\n",
    "    steps_per_epoch = len(train_loader)\n",
    "    total_steps = NUM_EPOCHS * steps_per_epoch\n",
    "    warmup_steps = int(total_steps * WARMUP_RATIO)\n",
    "    \n",
    "    scheduler = get_warmup_cosine_scheduler(\n",
    "        optimizer, \n",
    "        warmup_steps=warmup_steps, \n",
    "        total_steps=total_steps,\n",
    "        min_lr_ratio=MIN_LR_RATIO\n",
    "    )\n",
    "    \n",
    "    # Create experiment directory\n",
    "    exp_name = f\"inceptionv3_{fusion_type}_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "    exp_dir = os.path.join(OUTPUT_DIR, 'nutrition5k_experiments', exp_name)\n",
    "    os.makedirs(exp_dir, exist_ok=True)\n",
    "    \n",
    "    # Save experiment configuration\n",
    "    config = {\n",
    "        'fusion': fusion_type,\n",
    "        'use_volume': True,\n",
    "        'pretrained': False,\n",
    "        'dropout_rate': DROPOUT_RATE,\n",
    "        'fusion_channels': FUSION_CHANNELS,\n",
    "        'learning_rate': LEARNING_RATE,\n",
    "        'weight_decay': WEIGHT_DECAY,\n",
    "        'batch_size': BATCH_SIZE,\n",
    "        'img_size': IMG_SIZE,\n",
    "        'num_epochs': NUM_EPOCHS\n",
    "    }\n",
    "    \n",
    "    with open(os.path.join(exp_dir, 'config.json'), 'w') as f:\n",
    "        json.dump(config, f, indent=4)\n",
    "    \n",
    "    # Create trainer\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        criterion=criterion,\n",
    "        optimizer=optimizer,\n",
    "        scheduler=scheduler,\n",
    "        device=device,\n",
    "        output_dir=exp_dir,\n",
    "        early_stopping_patience=EARLY_STOPPING_PATIENCE,\n",
    "        scheduler_step_on_batch=False\n",
    "    )\n",
    "    \n",
    "    # Train the model\n",
    "    trainer.train(NUM_EPOCHS)\n",
    "    \n",
    "    print(f\"\\nExperiment completed! Results saved to: {exp_dir}\")\n",
    "    return trainer.best_metrics\n",
    "\n",
    "\n",
    "image_volume_result = train_nutrition5k_with_volume(fusion_type='image_volume')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d575c0df",
   "metadata": {},
   "source": [
    "## 3.5 InceptionV3 - Image + Depth + Volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "881890a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "TRAINING: Nutrition5k InceptionV3 + MIDDLE\n",
      "============================================================\n",
      "Loaded 2804 valid samples out of 2805\n",
      "Loaded 495 valid samples out of 495\n",
      "Model parameters: 52,094,465\n",
      "Training samples: 2804\n",
      "Validation samples: 495\n",
      "Learning rate: 0.0005\n",
      "Weight decay: 1e-06\n",
      "Starting training for 40 epochs...\n",
      "\n",
      "Epoch 1/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.41it/s, Loss=80694.2109] \n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 98966.1618\n",
      "Val Loss: 107401.5198\n",
      "MAE: 240.62\n",
      "\n",
      "Epoch 2/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.39it/s, Loss=32316.2949] \n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 69272.0114\n",
      "Val Loss: 24299.0708\n",
      "MAE: 107.20\n",
      "\n",
      "Epoch 3/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.40it/s, Loss=14151.7119]\n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 23787.9891\n",
      "Val Loss: 16633.3892\n",
      "MAE: 88.82\n",
      "\n",
      "Epoch 4/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.36it/s, Loss=15007.1816]\n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 14682.7154\n",
      "Val Loss: 12146.5886\n",
      "MAE: 75.05\n",
      "\n",
      "Epoch 5/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.33it/s, Loss=19958.5156]\n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 15840.9862\n",
      "Val Loss: 17202.8272\n",
      "MAE: 94.55\n",
      "\n",
      "Epoch 6/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.49it/s, Loss=16198.6484]\n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 13342.0799\n",
      "Val Loss: 15637.7800\n",
      "MAE: 82.39\n",
      "\n",
      "Epoch 7/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.39it/s, Loss=13739.9629]\n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 10382.3783\n",
      "Val Loss: 10612.7254\n",
      "MAE: 68.76\n",
      "\n",
      "Epoch 8/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.38it/s, Loss=4878.6045] \n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 9570.5377\n",
      "Val Loss: 10028.6673\n",
      "MAE: 66.16\n",
      "\n",
      "Epoch 9/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.37it/s, Loss=8596.0879] \n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 8189.9623\n",
      "Val Loss: 9066.1782\n",
      "MAE: 64.52\n",
      "\n",
      "Epoch 10/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.46it/s, Loss=9611.9727] \n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 7434.2731\n",
      "Val Loss: 9510.0068\n",
      "MAE: 64.08\n",
      "\n",
      "Epoch 11/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.41it/s, Loss=8335.9004] \n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 7191.1866\n",
      "Val Loss: 9110.0953\n",
      "MAE: 62.14\n",
      "\n",
      "Epoch 12/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.39it/s, Loss=9409.6133] \n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 6747.8446\n",
      "Val Loss: 10531.1671\n",
      "MAE: 65.82\n",
      "\n",
      "Epoch 13/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.30it/s, Loss=9208.3613] \n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 7328.7460\n",
      "Val Loss: 11288.8193\n",
      "MAE: 73.27\n",
      "\n",
      "Epoch 14/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.40it/s, Loss=8933.9141] \n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 9884.4828\n",
      "Val Loss: 11184.1066\n",
      "MAE: 71.21\n",
      "\n",
      "Epoch 15/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.43it/s, Loss=5830.4062] \n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 9719.4533\n",
      "Val Loss: 12809.6439\n",
      "MAE: 74.93\n",
      "\n",
      "Epoch 16/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.37it/s, Loss=6700.2168] \n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 12226.5881\n",
      "Val Loss: 14135.2347\n",
      "MAE: 77.15\n",
      "\n",
      "Epoch 17/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.27it/s, Loss=8846.7324] \n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 10628.8874\n",
      "Val Loss: 17854.5973\n",
      "MAE: 85.47\n",
      "\n",
      "Epoch 18/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.48it/s, Loss=6236.5356] \n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 9605.7054\n",
      "Val Loss: 11989.8474\n",
      "MAE: 75.97\n",
      "\n",
      "Epoch 19/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.37it/s, Loss=5089.1396] \n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 10650.8845\n",
      "Val Loss: 12936.2934\n",
      "MAE: 78.96\n",
      "\n",
      "Epoch 20/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.36it/s, Loss=10002.7715]\n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 10585.4611\n",
      "Val Loss: 20954.0295\n",
      "MAE: 93.90\n",
      "\n",
      "Epoch 21/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.41it/s, Loss=9147.0654] \n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 7982.7874\n",
      "Val Loss: 12198.2485\n",
      "MAE: 69.25\n",
      "\n",
      "Epoch 22/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.37it/s, Loss=9361.1641] \n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 8563.8640\n",
      "Val Loss: 12346.0579\n",
      "MAE: 82.81\n",
      "\n",
      "Epoch 23/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.33it/s, Loss=13987.2080]\n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 9314.2981\n",
      "Val Loss: 15242.2944\n",
      "MAE: 78.82\n",
      "\n",
      "Epoch 24/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.41it/s, Loss=3405.4075] \n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 7241.1247\n",
      "Val Loss: 8901.5778\n",
      "MAE: 59.49\n",
      "\n",
      "Epoch 25/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.34it/s, Loss=5384.7852] \n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 5968.9408\n",
      "Val Loss: 8102.0835\n",
      "MAE: 57.21\n",
      "\n",
      "Epoch 26/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.42it/s, Loss=6591.0449] \n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 6917.0128\n",
      "Val Loss: 12186.5141\n",
      "MAE: 68.78\n",
      "\n",
      "Epoch 27/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.45it/s, Loss=8429.6357] \n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 8189.9398\n",
      "Val Loss: 9175.6458\n",
      "MAE: 66.36\n",
      "\n",
      "Epoch 28/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.37it/s, Loss=14604.1523]\n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 6946.9857\n",
      "Val Loss: 8362.9894\n",
      "MAE: 59.92\n",
      "\n",
      "Epoch 29/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.41it/s, Loss=2387.0117] \n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 6338.9732\n",
      "Val Loss: 10491.9875\n",
      "MAE: 64.81\n",
      "\n",
      "Epoch 30/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.33it/s, Loss=5547.8535] \n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 6336.7661\n",
      "Val Loss: 7986.1679\n",
      "MAE: 57.02\n",
      "\n",
      "Epoch 31/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.41it/s, Loss=9401.0254] \n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 6179.5254\n",
      "Val Loss: 10851.4927\n",
      "MAE: 73.47\n",
      "\n",
      "Epoch 32/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.39it/s, Loss=6818.7988] \n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 6252.9587\n",
      "Val Loss: 8419.3591\n",
      "MAE: 61.60\n",
      "\n",
      "Epoch 33/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.37it/s, Loss=2115.7651] \n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 5743.9070\n",
      "Val Loss: 7800.9672\n",
      "MAE: 58.19\n",
      "\n",
      "Epoch 34/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.44it/s, Loss=5256.0479] \n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 6294.0270\n",
      "Val Loss: 8742.9595\n",
      "MAE: 63.41\n",
      "\n",
      "Epoch 35/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.43it/s, Loss=3182.2451] \n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 6482.3907\n",
      "Val Loss: 8548.3289\n",
      "MAE: 61.67\n",
      "\n",
      "Epoch 36/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.51it/s, Loss=4070.2322] \n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 4683.8792\n",
      "Val Loss: 8047.7720\n",
      "MAE: 60.68\n",
      "\n",
      "Epoch 37/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.40it/s, Loss=4029.4165] \n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 6090.3191\n",
      "Val Loss: 9532.8971\n",
      "MAE: 65.66\n",
      "\n",
      "Epoch 38/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.43it/s, Loss=3800.9351] \n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 5001.6133\n",
      "Val Loss: 8364.4256\n",
      "MAE: 57.44\n",
      "\n",
      "Epoch 39/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.45it/s, Loss=3780.6094] \n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 5878.9752\n",
      "Val Loss: 8378.5119\n",
      "MAE: 59.38\n",
      "\n",
      "Epoch 40/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:13<00:00,  6.44it/s, Loss=6472.6729] \n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  6.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 5337.3297\n",
      "Val Loss: 9411.3538\n",
      "MAE: 60.59\n",
      "\n",
      "Training completed!\n",
      "Best validation loss: 7800.9672\n",
      "\n",
      "Experiment completed! Results saved to: ./experiments/nutrition5k_experiments/inceptionv3_middle_volume_20251025_171018\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Configure experiment settings\n",
    "BATCH_SIZE = 32\n",
    "NUM_EPOCHS = 40\n",
    "DROPOUT_RATE = 0.4\n",
    "LEARNING_RATE = 5e-4\n",
    "WEIGHT_DECAY = 1e-6\n",
    "EARLY_STOPPING_PATIENCE = 15\n",
    "WARMUP_RATIO = 0.1\n",
    "MIN_LR_RATIO = 0.05\n",
    "FUSION_CHANNELS = 2048  # InceptionV3 output channels\n",
    "\n",
    "# Training function with volume estimation support\n",
    "def train_nutrition5k_with_volume(fusion_type='image_volume'):\n",
    "    \"\"\"\n",
    "    Train the Nutrition5k model with volume estimation\n",
    "    \n",
    "    Args:\n",
    "        fusion_type: 'image_only', 'image_volume', 'middle', etc.\n",
    "        use_segmentation: Whether to use learned segmentation for volume estimation\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "    print(f\"TRAINING: Nutrition5k InceptionV3 + {fusion_type.upper()}\")\n",
    "\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Create datasets\n",
    "    train_dataset = Nutrition5KDataset(\n",
    "        csv_path=train_csv,\n",
    "        data_root=DATA_ROOT,\n",
    "        split='train',\n",
    "        augment=False,\n",
    "        img_size=IMG_SIZE,\n",
    "    )\n",
    "    \n",
    "    val_dataset = Nutrition5KDataset(\n",
    "        csv_path=val_csv,\n",
    "        data_root=DATA_ROOT,\n",
    "        split='val',\n",
    "        augment=False,\n",
    "        img_size=IMG_SIZE,\n",
    "    )\n",
    "    \n",
    "    # Create data loaders\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=True,\n",
    "        num_workers=NUM_WORKERS,\n",
    "        pin_memory=True if torch.cuda.is_available() else False,\n",
    "        drop_last=True\n",
    "    )\n",
    "    \n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=False,\n",
    "        num_workers=NUM_WORKERS,\n",
    "        pin_memory=True if torch.cuda.is_available() else False\n",
    "    )\n",
    "    \n",
    "    # Use Volume  \n",
    "    model = build_nutrition5k_model(\n",
    "        fusion=fusion_type,\n",
    "        pretrained=False,\n",
    "        dropout_rate=DROPOUT_RATE,\n",
    "        fusion_channels=FUSION_CHANNELS,\n",
    "        use_volume=True,\n",
    "    )\n",
    "    model = model.to(device)\n",
    "    \n",
    "    print(f\"Model parameters: {model.get_num_parameters():,}\")\n",
    "    print(f\"Training samples: {len(train_dataset)}\")\n",
    "    print(f\"Validation samples: {len(val_dataset)}\")\n",
    "    \n",
    "    # Loss function\n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    # Optimizer\n",
    "    optimizer = optim.AdamW(\n",
    "        model.parameters(),\n",
    "        lr=LEARNING_RATE,\n",
    "        weight_decay=WEIGHT_DECAY\n",
    "    )\n",
    "    \n",
    "    print(f\"Learning rate: {LEARNING_RATE}\")\n",
    "    print(f\"Weight decay: {WEIGHT_DECAY}\")\n",
    "    \n",
    "    # Learning rate scheduler\n",
    "    steps_per_epoch = len(train_loader)\n",
    "    total_steps = NUM_EPOCHS * steps_per_epoch\n",
    "    warmup_steps = int(total_steps * WARMUP_RATIO)\n",
    "    \n",
    "    scheduler = get_warmup_cosine_scheduler(\n",
    "        optimizer, \n",
    "        warmup_steps=warmup_steps, \n",
    "        total_steps=total_steps,\n",
    "        min_lr_ratio=MIN_LR_RATIO\n",
    "    )\n",
    "    \n",
    "    # Create experiment directory\n",
    "    exp_name = f\"inceptionv3_{fusion_type}_volume_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "    exp_dir = os.path.join(OUTPUT_DIR, 'nutrition5k_experiments', exp_name)\n",
    "    os.makedirs(exp_dir, exist_ok=True)\n",
    "    \n",
    "    # Save experiment configuration\n",
    "    config = {\n",
    "        'fusion': fusion_type,\n",
    "        'use_volume': True,\n",
    "        'pretrained': False,\n",
    "        'dropout_rate': DROPOUT_RATE,\n",
    "        'fusion_channels': FUSION_CHANNELS,\n",
    "        'learning_rate': LEARNING_RATE,\n",
    "        'weight_decay': WEIGHT_DECAY,\n",
    "        'batch_size': BATCH_SIZE,\n",
    "        'img_size': IMG_SIZE,\n",
    "        'num_epochs': NUM_EPOCHS\n",
    "    }\n",
    "    \n",
    "    with open(os.path.join(exp_dir, 'config.json'), 'w') as f:\n",
    "        json.dump(config, f, indent=4)\n",
    "    \n",
    "    # Create trainer\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        criterion=criterion,\n",
    "        optimizer=optimizer,\n",
    "        scheduler=scheduler,\n",
    "        device=device,\n",
    "        output_dir=exp_dir,\n",
    "        early_stopping_patience=EARLY_STOPPING_PATIENCE,\n",
    "        scheduler_step_on_batch=False\n",
    "    )\n",
    "    \n",
    "    # Train the model\n",
    "    trainer.train(NUM_EPOCHS)\n",
    "    \n",
    "    print(f\"\\nExperiment completed! Results saved to: {exp_dir}\")\n",
    "    return trainer.best_metrics\n",
    "\n",
    "\n",
    "volume_with_middle_fusion_result = train_nutrition5k_with_volume(fusion_type='middle')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e4aa802",
   "metadata": {},
   "source": [
    "# Test Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f9fb6fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestDataset(Dataset):\n",
    "    \"\"\"Dataset class for test set inference\"\"\"\n",
    "    \n",
    "    def __init__(self, test_root, img_size=256):\n",
    "        self.test_root = test_root\n",
    "        self.img_size = img_size\n",
    "        \n",
    "        # Paths to subdirectories\n",
    "        self.color_dir = os.path.join(test_root, 'color')\n",
    "        self.depth_raw_dir = os.path.join(test_root, 'depth_raw')\n",
    "        \n",
    "        # Get all dish IDs from color directory\n",
    "        self.dish_ids = sorted([d for d in os.listdir(self.color_dir) \n",
    "                              if os.path.isdir(os.path.join(self.color_dir, d))])\n",
    "        \n",
    "        print(f\"Found {len(self.dish_ids)} test samples\")\n",
    "        \n",
    "        # Color normalization (same as training)\n",
    "        self.color_normalize = T.Normalize(\n",
    "            mean=[0.485, 0.456, 0.406],\n",
    "            std=[0.229, 0.224, 0.225]\n",
    "        )\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dish_ids)\n",
    "    \n",
    "    def _resize_and_center_crop(self, img, target_size=256):\n",
    "        \"\"\"Resize and center crop (same as training)\"\"\"\n",
    "        width, height = img.size\n",
    "        \n",
    "        if width < height:\n",
    "            new_width = target_size\n",
    "            new_height = int(target_size * height / width)\n",
    "        else:\n",
    "            new_height = target_size\n",
    "            new_width = int(target_size * width / height)\n",
    "        \n",
    "        img = img.resize((new_width, new_height), Image.LANCZOS)\n",
    "        \n",
    "        left = (new_width - target_size) // 2\n",
    "        top = (new_height - target_size) // 2\n",
    "        right = left + target_size\n",
    "        bottom = top + target_size\n",
    "        \n",
    "        return img.crop((left, top, right, bottom))\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        dish_id = self.dish_ids[idx]\n",
    "        \n",
    "        # Load images\n",
    "        rgb_path = os.path.join(self.color_dir, dish_id, 'rgb.png')\n",
    "        depth_path = os.path.join(self.depth_raw_dir, dish_id, 'depth_raw.png')\n",
    "        \n",
    "        rgb_img = Image.open(rgb_path).convert('RGB')\n",
    "        depth_img = Image.open(depth_path).convert('L')\n",
    "        \n",
    "        # Resize and center crop\n",
    "        rgb_img = self._resize_and_center_crop(rgb_img, target_size=self.img_size)\n",
    "        depth_img = self._resize_and_center_crop(depth_img, target_size=self.img_size)\n",
    "        \n",
    "        # Convert to tensors\n",
    "        rgb_tensor = TF.to_tensor(rgb_img)\n",
    "        depth_tensor = TF.to_tensor(depth_img)\n",
    "        \n",
    "        # Normalize\n",
    "        rgb_tensor = self.color_normalize(rgb_tensor)\n",
    "        depth_tensor = depth_tensor / 255.0\n",
    "        \n",
    "        return {\n",
    "            'rgb': rgb_tensor,\n",
    "            'depth': depth_tensor,\n",
    "            'dish_id': dish_id\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5326f049",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_from_checkpoint(checkpoint_path, device='cuda'):\n",
    "    \"\"\"Load trained model from checkpoint\"\"\"\n",
    "    \n",
    "    # Load checkpoint\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=device, weights_only=False)\n",
    "    \n",
    "    # Get the directory to look for config\n",
    "    model_dir = os.path.dirname(checkpoint_path)\n",
    "    config_path = os.path.join(model_dir, 'config.json')\n",
    "    \n",
    "    # Load config\n",
    "    if os.path.exists(config_path):\n",
    "        with open(config_path, 'r') as f:\n",
    "            config = json.load(f)\n",
    "        print(f\"Loaded config: {config}\")\n",
    "    else:\n",
    "        raise FileNotFoundError(f\"Config file not found: {config_path}\")\n",
    "    \n",
    "    # Build model with the same configuration\n",
    "    model = build_nutrition5k_model(\n",
    "        fusion=config.get('fusion', 'middle'),\n",
    "        pretrained=False,\n",
    "        dropout_rate=config.get('dropout_rate', 0.4),\n",
    "        fusion_channels=config.get('fusion_channels', 2048),\n",
    "        use_volume=config.get('use_volume', False)\n",
    "    )\n",
    "    \n",
    "    # Load state dict\n",
    "    if 'model_state_dict' in checkpoint:\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        print(f\"Loaded model from epoch {checkpoint.get('epoch', 'unknown')}\")\n",
    "        if 'val_loss' in checkpoint:\n",
    "            print(f\"Validation loss: {checkpoint['val_loss']:.4f}\")\n",
    "            print(f\"MAE: {checkpoint.get('mae', 'N/A')}\")\n",
    "    else:\n",
    "        model.load_state_dict(checkpoint)\n",
    "    \n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1ba481d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def run_inference(model, test_dataset, batch_size=32, device='cuda'):\n",
    "    \"\"\"Run inference on test set\"\"\"\n",
    "    \n",
    "    test_loader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=NUM_WORKERS,\n",
    "        pin_memory=True if torch.cuda.is_available() else False\n",
    "    )\n",
    "    \n",
    "    predictions = {}\n",
    "    \n",
    "    print(\"Running inference...\")\n",
    "    for batch in tqdm(test_loader):\n",
    "        rgb = batch['rgb'].to(device)\n",
    "        depth = batch['depth'].to(device)\n",
    "        dish_ids = batch['dish_id']\n",
    "        \n",
    "        # Forward pass\n",
    "        calorie_pred = model(rgb, depth)\n",
    "        \n",
    "        # Store predictions\n",
    "        calorie_pred = calorie_pred.cpu().numpy().flatten()\n",
    "        for i, dish_id in enumerate(dish_ids):\n",
    "            predictions[dish_id] = float(calorie_pred[i])\n",
    "    \n",
    "    return predictions\n",
    "\n",
    "def create_submission(predictions, output_path='submission.csv'):\n",
    "    \"\"\"Create submission CSV file\"\"\"\n",
    "    \n",
    "    submission_data = []\n",
    "    for dish_id in sorted(predictions.keys()):\n",
    "        submission_data.append({\n",
    "            'ID': dish_id,\n",
    "            'Value': predictions[dish_id]\n",
    "        })\n",
    "    \n",
    "    submission_df = pd.DataFrame(submission_data)\n",
    "    submission_df.to_csv(output_path, index=False)\n",
    "    \n",
    "    print(f\"Submission saved to: {output_path}\")\n",
    "    print(f\"Total predictions: {len(submission_data)}\")\n",
    "    print(f\"\\nSample predictions:\")\n",
    "    print(submission_df.head(10))\n",
    "    \n",
    "    # Statistics\n",
    "    values = submission_df['Value'].values\n",
    "    print(f\"\\nPrediction Statistics:\")\n",
    "    print(f\"  Min: {values.min():.2f}\")\n",
    "    print(f\"  Max: {values.max():.2f}\")\n",
    "    print(f\"  Mean: {values.mean():.2f}\")\n",
    "    print(f\"  Median: {np.median(values):.2f}\")\n",
    "    \n",
    "    return submission_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f876c65e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 189 test samples\n",
      "Loaded config: {'fusion': 'middle', 'use_volume': True, 'pretrained': False, 'dropout_rate': 0.4, 'fusion_channels': 2048, 'learning_rate': 0.0005, 'weight_decay': 1e-06, 'batch_size': 32, 'img_size': 256, 'num_epochs': 40}\n",
      "Loaded model from epoch 17\n",
      "Validation loss: 6987.1615\n",
      "MAE: 54.757686614990234\n",
      "Running inference...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:01<00:00,  4.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submission saved to: ./submission.csv\n",
      "Total predictions: 189\n",
      "\n",
      "Sample predictions:\n",
      "          ID       Value\n",
      "0  dish_3301  948.794556\n",
      "1  dish_3302   16.694901\n",
      "2  dish_3303   27.170441\n",
      "3  dish_3304  222.738480\n",
      "4  dish_3305  466.522308\n",
      "5  dish_3306    3.652470\n",
      "6  dish_3307  472.011383\n",
      "7  dish_3308   44.091991\n",
      "8  dish_3309  544.475403\n",
      "9  dish_3310  253.851929\n",
      "\n",
      "Prediction Statistics:\n",
      "  Min: 2.25\n",
      "  Max: 948.79\n",
      "  Mean: 248.84\n",
      "  Median: 206.76\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "TEST_ROOT = './Nutrition5K/Nutrition5K/test'  # Path to test data\n",
    "MODEL_PATH = './experiments/nutrition5k_experiments/inceptionv3_middle_volume_20251025_163021/best_model.pth'  # Your best model\n",
    "OUTPUT_PATH = './submission.csv'\n",
    "\n",
    "# Load test dataset\n",
    "test_dataset = TestDataset(\n",
    "    test_root=TEST_ROOT,\n",
    "    img_size=IMG_SIZE\n",
    ")\n",
    "\n",
    "# Load model\n",
    "model = load_model_from_checkpoint(MODEL_PATH, device=device)\n",
    "\n",
    "# Run inference\n",
    "predictions = run_inference(\n",
    "    model=model,\n",
    "    test_dataset=test_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "# Create submission\n",
    "submission_df = create_submission(predictions, output_path=OUTPUT_PATH)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "compvis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
