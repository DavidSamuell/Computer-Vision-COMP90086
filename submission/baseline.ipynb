{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "70b9a0fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import torchvision.transforms as T\n",
    "import torchvision.transforms.functional as TF\n",
    "import random\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from PIL import Image\n",
    "from typing import Tuple, Optional\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(789)\n",
    "np.random.seed(789)\n",
    "random.seed(789)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(789)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3806c735",
   "metadata": {},
   "source": [
    "# 1. Model and Trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d20b7ee",
   "metadata": {},
   "source": [
    "## 1.1 Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "009adc76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Implementation - Dual-stream CNN with Middle Fusion\n",
    "\n",
    "class SimpleCNN(nn.Module):\n",
    "    \"\"\"\n",
    "    Simple 2-layer CNN encoder\n",
    "    \n",
    "    Based on the architecture in CNN.ipynb, converted to PyTorch\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_channels=3, width1=32, width2=64, dropout_rate=0.25):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input_channels: Number of input channels (3 for RGB, 1 for depth)\n",
    "            width1: Number of filters in first conv layer\n",
    "            width2: Number of filters in second conv layer\n",
    "            dropout_rate: Dropout rate\n",
    "        \"\"\"\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        \n",
    "        # First convolutional block\n",
    "        self.conv1 = nn.Conv2d(input_channels, width1, kernel_size=3, padding=0)\n",
    "        self.bn1 = nn.BatchNorm2d(width1)\n",
    "        self.dropout1 = nn.Dropout(dropout_rate)\n",
    "        \n",
    "        # Second convolutional block\n",
    "        self.conv2 = nn.Conv2d(width1, width2, kernel_size=3, padding=0)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.bn2 = nn.BatchNorm2d(width2)\n",
    "        self.dropout2 = nn.Dropout(dropout_rate)\n",
    "        \n",
    "        self.out_channels = width2\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Input tensor (B, C, H, W) - expects normalized images [0, 1]\n",
    "        \n",
    "        Returns:\n",
    "            Feature map (B, width2, H', W')\n",
    "        \"\"\"\n",
    "        # First convolutional block\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.dropout1(x)\n",
    "        \n",
    "        # Second convolutional block\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.pool(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.dropout2(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "class MiddleFusionModule(nn.Module):\n",
    "    \"\"\"Middle fusion: Concatenate RGB and Depth features, then merge with 1x1 conv\"\"\"\n",
    "    \n",
    "    def __init__(self, rgb_channels: int = 64, depth_channels: int = 64, output_channels: int = 64):\n",
    "        super().__init__()\n",
    "        \n",
    "        # 1x1 convolution to merge features\n",
    "        self.fusion_conv = nn.Conv2d(\n",
    "            rgb_channels + depth_channels,\n",
    "            output_channels,\n",
    "            kernel_size=1,\n",
    "            bias=False\n",
    "        )\n",
    "        self.bn = nn.BatchNorm2d(output_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "    \n",
    "    def forward(self, rgb_features, depth_features):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            rgb_features: (B, rgb_channels, H, W)\n",
    "            depth_features: (B, depth_channels, H, W)\n",
    "        Returns:\n",
    "            Fused features: (B, output_channels, H, W)\n",
    "        \"\"\"\n",
    "        # Concatenate along channel dimension\n",
    "        fused = torch.cat([rgb_features, depth_features], dim=1)\n",
    "        \n",
    "        # Apply 1x1 conv to reduce channels\n",
    "        fused = self.fusion_conv(fused)\n",
    "        fused = self.bn(fused)\n",
    "        fused = self.relu(fused)\n",
    "        \n",
    "        return fused\n",
    "\n",
    "\n",
    "class RegressionHead(nn.Module):\n",
    "    \"\"\"Regression head for calorie prediction (matches ResNet experiments architecture)\"\"\"\n",
    "    \n",
    "    def __init__(self, in_channels: int = 64, dropout_rate: float = 0.4):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Adaptive average pooling to handle any feature map size\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        \n",
    "        # FC layers matching ResNet experiments: in_channels -> 256 -> 128 -> 1\n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(in_channels, 256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(128, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.avgpool(x)  # (B, C, 1, 1)\n",
    "        x = self.fc_layers(x)  # (B, 1)\n",
    "        return x\n",
    "\n",
    "\n",
    "class DualStreamCNN(nn.Module):\n",
    "    \"\"\"\n",
    "    Dual-stream CNN for calorie prediction using RGB and Depth images with middle fusion\n",
    "    Architecture: Simple CNN encoders + Middle Fusion + Regression head\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, width1=32, width2=64, dropout_rate=0.25):\n",
    "        super().__init__()\n",
    "        \n",
    "        # RGB and Depth encoders\n",
    "        self.rgb_encoder = SimpleCNN(input_channels=3, width1=width1, width2=width2, dropout_rate=dropout_rate)\n",
    "        self.depth_encoder = SimpleCNN(input_channels=1, width1=width1, width2=width2, dropout_rate=dropout_rate)\n",
    "        \n",
    "        # Middle fusion module\n",
    "        self.fusion = MiddleFusionModule(\n",
    "            rgb_channels=width2,\n",
    "            depth_channels=width2,\n",
    "            output_channels=width2\n",
    "        )\n",
    "        \n",
    "        # Regression head for calorie prediction\n",
    "        self.regression_head = RegressionHead(\n",
    "            in_channels=width2,\n",
    "            dropout_rate=dropout_rate\n",
    "        )\n",
    "    \n",
    "    def forward(self, rgb, depth):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            rgb: RGB images (B, 3, H, W)\n",
    "            depth: Depth images (B, 1, H, W)\n",
    "        \n",
    "        Returns:\n",
    "            calorie_pred: Predicted calories (B, 1)\n",
    "        \"\"\"\n",
    "        # Extract features from both streams\n",
    "        rgb_features = self.rgb_encoder(rgb)\n",
    "        depth_features = self.depth_encoder(depth)\n",
    "        \n",
    "        # Fuse features\n",
    "        fused_features = self.fusion(rgb_features, depth_features)\n",
    "        \n",
    "        # Predict calories\n",
    "        calorie_pred = self.regression_head(fused_features)\n",
    "        \n",
    "        return calorie_pred\n",
    "    \n",
    "    def get_num_parameters(self):\n",
    "        \"\"\"Get total number of trainable parameters\"\"\"\n",
    "        return sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
    "\n",
    "\n",
    "def build_model(width1=32, width2=64, dropout_rate=0.25, **kwargs):\n",
    "    \"\"\"\n",
    "    Factory function to build the dual-stream CNN model with middle fusion\n",
    "    \n",
    "    Args:\n",
    "        width1: Number of filters in first conv layer\n",
    "        width2: Number of filters in second conv layer\n",
    "        dropout_rate: Dropout rate\n",
    "    \n",
    "    Returns:\n",
    "        DualStreamCNN model\n",
    "    \"\"\"\n",
    "    return DualStreamCNN(\n",
    "        width1=width1,\n",
    "        width2=width2,\n",
    "        dropout_rate=dropout_rate\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5952d666",
   "metadata": {},
   "source": [
    "## 1.2 Trainer Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b60d167d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import math\n",
    "def get_warmup_cosine_scheduler(optimizer, warmup_steps, total_steps, min_lr_ratio=0.0):\n",
    "    def lr_lambda(current_step):\n",
    "        if current_step < warmup_steps:\n",
    "            return float(current_step) / float(max(1, warmup_steps))\n",
    "        else:\n",
    "            progress = float(current_step - warmup_steps) / float(max(1, total_steps - warmup_steps))\n",
    "            return min_lr_ratio + (1.0 - min_lr_ratio) * 0.5 * (1.0 + math.cos(math.pi * progress))\n",
    "    \n",
    "    return optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n",
    "\n",
    "class EarlyStopping:\n",
    "    \"\"\"Early stopping to stop training when validation loss stops improving\"\"\"\n",
    "    \n",
    "    def __init__(self, patience: int = 10, min_delta: float = 0.0, mode: str = 'min'):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            patience: Number of epochs with no improvement after which training will be stopped\n",
    "            min_delta: Minimum change to qualify as an improvement\n",
    "            mode: 'min' or 'max' - whether lower or higher metric is better\n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.mode = mode\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.best_epoch = 0\n",
    "        \n",
    "    def __call__(self, score, epoch):\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.best_epoch = epoch\n",
    "            return False\n",
    "        \n",
    "        if self.mode == 'min':\n",
    "            improved = score < (self.best_score - self.min_delta)\n",
    "        else:\n",
    "            improved = score > (self.best_score + self.min_delta)\n",
    "        \n",
    "        if improved:\n",
    "            self.best_score = score\n",
    "            self.best_epoch = epoch\n",
    "            self.counter = 0\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "                \n",
    "        return self.early_stop\n",
    "\n",
    "class Trainer:\n",
    "    \"\"\"Training manager for calorie prediction\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        model,\n",
    "        train_loader,\n",
    "        val_loader,\n",
    "        criterion,\n",
    "        optimizer,\n",
    "        scheduler,\n",
    "        device,\n",
    "        output_dir,\n",
    "        early_stopping_patience=15\n",
    "    ):\n",
    "        self.model = model\n",
    "        self.train_loader = train_loader\n",
    "        self.val_loader = val_loader\n",
    "        self.criterion = criterion\n",
    "        self.optimizer = optimizer\n",
    "        self.scheduler = scheduler\n",
    "        self.device = device\n",
    "        self.output_dir = output_dir\n",
    "        \n",
    "        # Early stopping\n",
    "        self.early_stopping = EarlyStopping(\n",
    "            patience=early_stopping_patience,\n",
    "            min_delta=0.1,\n",
    "            mode='min'\n",
    "        )\n",
    "        \n",
    "        # Tensorboard\n",
    "        self.writer = SummaryWriter(log_dir=os.path.join(output_dir, 'tensorboard'))\n",
    "        \n",
    "        # Tracking\n",
    "        self.best_val_loss = float('inf')\n",
    "        self.train_losses = []\n",
    "        self.val_losses = []\n",
    "        self.best_metrics = {}\n",
    "    \n",
    "    def train_epoch(self):\n",
    "        \"\"\"Train for one epoch\"\"\"\n",
    "        self.model.train()\n",
    "        total_loss = 0.0\n",
    "        num_batches = 0\n",
    "        \n",
    "        pbar = tqdm(self.train_loader, desc=\"Training\")\n",
    "        for batch_idx, batch in enumerate(pbar):\n",
    "            # Move to device (use both RGB and depth for fusion)\n",
    "            rgb = batch['rgb'].to(self.device)\n",
    "            depth = batch['depth'].to(self.device)\n",
    "            calories = batch['calorie'].to(self.device)\n",
    "            \n",
    "            # Forward pass\n",
    "            self.optimizer.zero_grad()\n",
    "            calorie_pred = self.model(rgb, depth)\n",
    "            \n",
    "            # Compute loss (MSE for calorie prediction)\n",
    "            loss = self.criterion(calorie_pred.squeeze(), calories)\n",
    "            \n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)\n",
    "            self.optimizer.step()\n",
    "            \n",
    "            # Track metrics\n",
    "            total_loss += loss.item()\n",
    "            num_batches += 1\n",
    "            \n",
    "            # Update progress bar\n",
    "            pbar.set_postfix({'Loss': f'{loss.item():.4f}'})\n",
    "        \n",
    "        return total_loss / num_batches\n",
    "    \n",
    "    def validate_epoch(self):\n",
    "        \"\"\"Validate for one epoch\"\"\"\n",
    "        self.model.eval()\n",
    "        total_loss = 0.0\n",
    "        all_predictions = []\n",
    "        all_targets = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(self.val_loader, desc=\"Validation\"):\n",
    "                # Move to device (use both RGB and depth for fusion)\n",
    "                rgb = batch['rgb'].to(self.device)\n",
    "                depth = batch['depth'].to(self.device)\n",
    "                calories = batch['calorie'].to(self.device)\n",
    "                \n",
    "                # Forward pass\n",
    "                calorie_pred = self.model(rgb, depth)\n",
    "                \n",
    "                # Compute loss\n",
    "                loss = self.criterion(calorie_pred.squeeze(), calories)\n",
    "                total_loss += loss.item()\n",
    "                \n",
    "                # Store predictions and targets for metrics\n",
    "                all_predictions.extend(calorie_pred.squeeze().cpu().numpy())\n",
    "                all_targets.extend(calories.cpu().numpy())\n",
    "        \n",
    "        # Calculate metrics\n",
    "        avg_loss = total_loss / len(self.val_loader)\n",
    "        predictions = np.array(all_predictions)\n",
    "        targets = np.array(all_targets)\n",
    "        \n",
    "        mae = np.mean(np.abs(predictions - targets))\n",
    "        \n",
    "        return avg_loss, mae\n",
    "    \n",
    "    def train(self, num_epochs):\n",
    "        \"\"\"Full training loop\"\"\"\n",
    "        print(f\"Starting training for {num_epochs} epochs...\")\n",
    "        \n",
    "        for epoch in range(num_epochs):\n",
    "            print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
    "            \n",
    "            # Train\n",
    "            train_loss = self.train_epoch()\n",
    "            \n",
    "            # Validate\n",
    "            val_loss, mae = self.validate_epoch()\n",
    "            \n",
    "            # Update learning rate\n",
    "            if self.scheduler:\n",
    "                if isinstance(self.scheduler, optim.lr_scheduler.ReduceLROnPlateau):\n",
    "                    self.scheduler.step(val_loss)\n",
    "                else:\n",
    "                    self.scheduler.step()\n",
    "            \n",
    "            # Log metrics\n",
    "            self.writer.add_scalar('Loss/Train', train_loss, epoch)\n",
    "            self.writer.add_scalar('Loss/Val', val_loss, epoch)\n",
    "            self.writer.add_scalar('MAE', mae, epoch)\n",
    "            \n",
    "            # Save best model\n",
    "            if val_loss < self.best_val_loss:\n",
    "                self.best_val_loss = val_loss\n",
    "                self.best_metrics = {\n",
    "                    'epoch': epoch + 1,\n",
    "                    'val_loss': val_loss,\n",
    "                    'mae': mae,\n",
    "                }\n",
    "                \n",
    "                # Save model checkpoint\n",
    "                torch.save({\n",
    "                    'epoch': epoch + 1,\n",
    "                    'model_state_dict': self.model.state_dict(),\n",
    "                    'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "                    'val_loss': val_loss,\n",
    "                    'mae': mae,\n",
    "                }, os.path.join(self.output_dir, 'best_model.pth'))\n",
    "            \n",
    "            # Print epoch results\n",
    "            print(f\"Train Loss: {train_loss:.4f}\")\n",
    "            print(f\"Val Loss: {val_loss:.4f}\")\n",
    "            print(f\"MAE: {mae:.2f}\")\n",
    "            \n",
    "            # Early stopping\n",
    "            if self.early_stopping(val_loss, epoch):\n",
    "                print(f\"Early stopping triggered after {epoch+1} epochs\")\n",
    "                print(f\"Best epoch: {self.early_stopping.best_epoch+1}\")\n",
    "                break\n",
    "        \n",
    "        self.writer.close()\n",
    "        print(f\"\\nTraining completed!\")\n",
    "        print(f\"Best validation loss: {self.best_val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47c7df1a",
   "metadata": {},
   "source": [
    "# 2. Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e823036",
   "metadata": {},
   "source": [
    "## 2.1 Dataset Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba675ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset Implementation\n",
    "class Nutrition5KDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset class for Nutrition5K with multi-modal inputs (RGB + Depth)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        csv_path: str,\n",
    "        data_root: str,\n",
    "        split: str = 'train',\n",
    "        augment: bool = True,\n",
    "        img_size: int = 224,\n",
    "    ):\n",
    "        self.data_root = data_root\n",
    "        self.split = split\n",
    "        self.augment = augment\n",
    "        self.img_size = img_size\n",
    "        \n",
    "        # Load CSV\n",
    "        self.df = pd.read_csv(csv_path)\n",
    "        if 'Value' in self.df.columns and 'calories' not in self.df.columns:\n",
    "            self.df = self.df.rename(columns={'Value': 'calories'})\n",
    "        if 'calories' not in self.df.columns:\n",
    "            raise ValueError(\"CSV file must contain a 'calories' column or a 'Value' column that can be renamed\")\n",
    "        self.df = self.df[self.df['calories'] < 3000].reset_index(drop=True)\n",
    "                \n",
    "        self.color_dir = os.path.join(data_root, 'color')\n",
    "        self.depth_raw_dir = os.path.join(data_root, 'depth_raw')\n",
    "        \n",
    "        self.valid_indices = self._validate_dataset()\n",
    "        print(f\"Loaded {len(self.valid_indices)} valid samples out of {len(self.df)}\")\n",
    "        \n",
    "        # Color normalization (ImageNet stats as baseline)\n",
    "        self.color_normalize = T.Normalize(\n",
    "            mean=[0.485, 0.456, 0.406],\n",
    "            std=[0.229, 0.224, 0.225]\n",
    "        )\n",
    "        \n",
    "    def _validate_dataset(self):\n",
    "        \"\"\"This method ensure that the code don't break when there are corrupted images.\"\"\"\n",
    "        valid_indices = []\n",
    "        \n",
    "        for idx in range(len(self.df)):\n",
    "            dish_id = self.df.iloc[idx]['ID']\n",
    "            \n",
    "            rgb_path = os.path.join(self.color_dir, dish_id, 'rgb.png')\n",
    "            depth_path = os.path.join(self.depth_raw_dir, dish_id, 'depth_raw.png')\n",
    "            \n",
    "            # Check if files exist\n",
    "            if not os.path.exists(rgb_path):\n",
    "                continue\n",
    "            if not os.path.exists(depth_path):\n",
    "                continue\n",
    "            \n",
    "            # Try to load images to check for corruption\n",
    "            try:\n",
    "                with Image.open(rgb_path) as img:\n",
    "                    img.verify()\n",
    "                with Image.open(depth_path) as img:\n",
    "                    img.verify()\n",
    "                valid_indices.append(idx)\n",
    "            except Exception as e:\n",
    "                continue\n",
    "                \n",
    "        return valid_indices\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.valid_indices)\n",
    "    \n",
    "    def _load_image_safe(self, path: str, mode: str = 'RGB') -> Optional[Image.Image]:\n",
    "        \"\"\"Safely load an image with error handling\"\"\"\n",
    "        try:\n",
    "            with Image.open(path) as img:\n",
    "                return img.convert(mode).copy()\n",
    "        except Exception as e:\n",
    "            return None\n",
    "    \n",
    "    def _apply_augmentation(self, rgb_img, depth_img):\n",
    "        \"\"\"Apply geometric augmentation only (no color changes)\"\"\"\n",
    "        if not self.augment:\n",
    "            return rgb_img, depth_img\n",
    "        \n",
    "        # Convert to tensors first\n",
    "        rgb_tensor = TF.to_tensor(rgb_img)\n",
    "        depth_tensor = TF.to_tensor(depth_img)\n",
    "        \n",
    "        # Random horizontal flip\n",
    "        if random.random() > 0.5:\n",
    "            rgb_tensor = TF.hflip(rgb_tensor)\n",
    "            depth_tensor = TF.hflip(depth_tensor)\n",
    "        \n",
    "        # Random rotation (±15 degrees)\n",
    "        if random.random() > 0.5:\n",
    "            angle = random.uniform(-15, 15)\n",
    "            rgb_tensor = TF.rotate(rgb_tensor, angle)\n",
    "            depth_tensor = TF.rotate(depth_tensor, angle)\n",
    "        \n",
    "        # Convert back to PIL\n",
    "        rgb_img = TF.to_pil_image(rgb_tensor)\n",
    "        depth_img = TF.to_pil_image(depth_tensor)\n",
    "        \n",
    "        return rgb_img, depth_img\n",
    "    \n",
    "    def _resize_and_center_crop(self, img, target_size: int = 256):\n",
    "        \"\"\"\n",
    "        Resize and center crop image to target_size x target_size\n",
    "        Matches the preprocessing in the Nutrition5k paper\n",
    "        \n",
    "        Args:\n",
    "            img: PIL Image\n",
    "            target_size: Target size (default 256x256 as per paper)\n",
    "        \n",
    "        Returns:\n",
    "            Cropped PIL Image\n",
    "        \"\"\"\n",
    "        # Get original dimensions\n",
    "        width, height = img.size\n",
    "        \n",
    "        # Resize so the shorter side is target_size\n",
    "        if width < height:\n",
    "            new_width = target_size\n",
    "            new_height = int(target_size * height / width)\n",
    "        else:\n",
    "            new_height = target_size\n",
    "            new_width = int(target_size * width / height)\n",
    "        \n",
    "        img = img.resize((new_width, new_height), Image.LANCZOS)\n",
    "        \n",
    "        # Center crop to target_size x target_size\n",
    "        left = (new_width - target_size) // 2\n",
    "        top = (new_height - target_size) // 2\n",
    "        right = left + target_size\n",
    "        bottom = top + target_size\n",
    "        \n",
    "        img = img.crop((left, top, right, bottom))\n",
    "        \n",
    "        return img\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"Get a single sample\"\"\"\n",
    "        actual_idx = self.valid_indices[idx]\n",
    "        row = self.df.iloc[actual_idx]\n",
    "        \n",
    "        dish_id = row['ID']\n",
    "        calorie = float(row['calories'])\n",
    "        \n",
    "        # Load images\n",
    "        rgb_path = os.path.join(self.color_dir, dish_id, 'rgb.png')\n",
    "        depth_path = os.path.join(self.depth_raw_dir, dish_id, 'depth_raw.png')\n",
    "        \n",
    "        rgb_img = self._load_image_safe(rgb_path, 'RGB')\n",
    "        depth_img = self._load_image_safe(depth_path, 'L')  # Grayscale for depth\n",
    "        \n",
    "        # Fallback: return a black image\n",
    "        if rgb_img is None or depth_img is None:\n",
    "            rgb_img = Image.new('RGB', (self.img_size, self.img_size), (0, 0, 0))\n",
    "            depth_img = Image.new('L', (self.img_size, self.img_size), 0)\n",
    "        \n",
    "        # Apply augmentation\n",
    "        rgb_img, depth_img = self._apply_augmentation(rgb_img, depth_img)\n",
    "        \n",
    "        # Resize and center crop to match paper preprocessing (256x256)\n",
    "        rgb_img = self._resize_and_center_crop(rgb_img, target_size=self.img_size)\n",
    "        depth_img = self._resize_and_center_crop(depth_img, target_size=self.img_size)\n",
    "        \n",
    "        # Convert to tensors\n",
    "        rgb_tensor = TF.to_tensor(rgb_img)  # (3, H, W)\n",
    "        depth_tensor = TF.to_tensor(depth_img)  # (1, H, W)\n",
    "        \n",
    "        # Normalize RGB\n",
    "        rgb_tensor = self.color_normalize(rgb_tensor)\n",
    "        \n",
    "        # Normalize depth (0-1 range, assuming depth is already in reasonable range)\n",
    "        depth_tensor = depth_tensor / 255.0\n",
    "        \n",
    "        return {\n",
    "            'dish_id': dish_id,\n",
    "            'rgb': rgb_tensor,\n",
    "            'depth': depth_tensor,\n",
    "            'calorie': torch.tensor(calorie, dtype=torch.float32)\n",
    "        }\n",
    "\n",
    "\n",
    "def create_train_val_split(csv_path: str, val_ratio: float = 0.15, random_seed: int = 42):\n",
    "    \"\"\"\n",
    "    Create train/validation split CSV files\n",
    "    \"\"\"\n",
    "    # Read original CSV\n",
    "    df = pd.read_csv(csv_path)    \n",
    "    \n",
    "    # Shuffle with fixed seed\n",
    "    df_shuffled = df.sample(frac=1, random_state=random_seed).reset_index(drop=True)\n",
    "    \n",
    "    # Split\n",
    "    val_size = int(len(df_shuffled) * val_ratio)\n",
    "    train_df = df_shuffled[val_size:]\n",
    "    val_df = df_shuffled[:val_size]\n",
    "    \n",
    "    # Save temporary CSV files\n",
    "    base_dir = os.path.dirname(csv_path)\n",
    "    train_csv = os.path.join(base_dir, 'train_split.csv')\n",
    "    val_csv = os.path.join(base_dir, 'val_split.csv')\n",
    "    \n",
    "    train_df.to_csv(train_csv, index=False)\n",
    "    val_df.to_csv(val_csv, index=False)\n",
    "    \n",
    "    return train_csv, val_csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55947277",
   "metadata": {},
   "source": [
    "## 2.2 Dataset Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2d845f4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration:\n",
      "  Data root: ./Nutrition5K/Nutrition5K/train\n",
      "  CSV path: ./Nutrition5K/Nutrition5K/nutrition5k_train.csv\n",
      "  Output directory: ./experiments\n",
      "  Batch size: 32\n",
      "  Number of epochs: 40\n",
      "  Image size: 128\n",
      "  Dropout rate: 0.25\n",
      "  Learning rate: 0.001\n",
      "  Workers: 4\n"
     ]
    }
   ],
   "source": [
    "# Configuration - Update these paths to match your setup\n",
    "DATA_ROOT = './Nutrition5K/Nutrition5K/train'  # Path to training data directory\n",
    "CSV_PATH = './Nutrition5K/Nutrition5K/nutrition5k_train.csv'  # Path to training CSV\n",
    "OUTPUT_DIR = './experiments'  # Directory to save experiment results\n",
    "\n",
    "# Global training hyperparameters (matching CNN.ipynb architecture)\n",
    "BATCH_SIZE = 32\n",
    "NUM_EPOCHS = 40\n",
    "VAL_RATIO = 0.15\n",
    "IMG_SIZE = 128  # Changed to match CNN.ipynb\n",
    "NUM_WORKERS = 4\n",
    "DROPOUT_RATE = 0.25  # Match CNN.ipynb\n",
    "LEARNING_RATE = 0.001\n",
    "EARLY_STOPPING_PATIENCE = 5\n",
    "\n",
    "print(\"Configuration:\")\n",
    "print(f\"  Data root: {DATA_ROOT}\")\n",
    "print(f\"  CSV path: {CSV_PATH}\")\n",
    "print(f\"  Output directory: {OUTPUT_DIR}\")\n",
    "print(f\"  Batch size: {BATCH_SIZE}\")\n",
    "print(f\"  Number of epochs: {NUM_EPOCHS}\")\n",
    "print(f\"  Image size: {IMG_SIZE}\")\n",
    "print(f\"  Dropout rate: {DROPOUT_RATE}\")\n",
    "print(f\"  Learning rate: {LEARNING_RATE}\")\n",
    "print(f\"  Workers: {NUM_WORKERS}\")\n",
    "\n",
    "# Create output directory\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "31b0e9f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating train/validation split...\n",
      "Train CSV: ./Nutrition5K/Nutrition5K/train_split.csv\n",
      "Validation CSV: ./Nutrition5K/Nutrition5K/val_split.csv\n",
      "Loaded 2804 valid samples out of 2805\n",
      "\n",
      "Dataset loaded successfully!\n",
      "Training samples: 2804\n",
      "RGB shape: torch.Size([3, 128, 128])\n",
      "Calorie value: 88.54999542236328\n"
     ]
    }
   ],
   "source": [
    "# Create train/validation split\n",
    "print(\"Creating train/validation split...\")\n",
    "train_csv, val_csv = create_train_val_split(\n",
    "    CSV_PATH,\n",
    "    val_ratio=VAL_RATIO,\n",
    "    random_seed=42\n",
    ")\n",
    "\n",
    "print(f\"Train CSV: {train_csv}\")\n",
    "print(f\"Validation CSV: {val_csv}\")\n",
    "\n",
    "# Load a sample to check data\n",
    "sample_dataset = Nutrition5KDataset(\n",
    "    csv_path=train_csv,\n",
    "    data_root=DATA_ROOT,\n",
    "    split='train',\n",
    "    augment=False,  # No augmentation for checking\n",
    "    img_size=IMG_SIZE,\n",
    ")\n",
    "\n",
    "print(f\"\\nDataset loaded successfully!\")\n",
    "print(f\"Training samples: {len(sample_dataset)}\")\n",
    "print(f\"RGB shape: {sample_dataset[0]['rgb'].shape}\")\n",
    "print(f\"Calorie value: {sample_dataset[0]['calorie']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "190cfc60",
   "metadata": {},
   "source": [
    "# 3. Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ff898d1",
   "metadata": {},
   "source": [
    "## 3.1 Baseline - No Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "41d32c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Define Hyperparameteres\n",
    "DROPOUT_RATE = 0.3\n",
    "LEARNING_RATE = 2e-2\n",
    "WEIGHT_DECAY = 1e-6\n",
    "EARLY_STOPPING_PATIENCE = 7\n",
    "WARMUP_RATIO = 0.1\n",
    "MIN_LR_RATIO = 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7de96ccb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "TRAINING: Simple 2-Layer CNN (Baseline)\n",
      "============================================================\n",
      "Loaded 2804 valid samples out of 2805\n",
      "Loaded 495 valid samples out of 495\n",
      "Model parameters: 96,577\n",
      "Training samples: 2804\n",
      "Validation samples: 495\n",
      "Learning rate: 0.02\n",
      "Starting training for 40 epochs...\n",
      "\n",
      "Epoch 1/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:11<00:00,  7.69it/s, Loss=108093.2422]\n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  7.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 98864.8370\n",
      "Val Loss: 107355.1853\n",
      "MAE: 240.52\n",
      "\n",
      "Epoch 2/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:11<00:00,  7.79it/s, Loss=69902.6562] \n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  7.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 99038.0821\n",
      "Val Loss: 107041.9934\n",
      "MAE: 239.99\n",
      "\n",
      "Epoch 3/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:11<00:00,  7.78it/s, Loss=85103.7344] \n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  7.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 97338.5387\n",
      "Val Loss: 103530.2859\n",
      "MAE: 234.57\n",
      "\n",
      "Epoch 4/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:11<00:00,  7.80it/s, Loss=76401.5781] \n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  7.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 88918.2979\n",
      "Val Loss: 91305.2539\n",
      "MAE: 215.52\n",
      "\n",
      "Epoch 5/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:11<00:00,  7.74it/s, Loss=28038.8555] \n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  7.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 67648.6363\n",
      "Val Loss: 61558.1018\n",
      "MAE: 171.46\n",
      "\n",
      "Epoch 6/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:11<00:00,  7.71it/s, Loss=41210.7539]\n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  7.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 38201.2138\n",
      "Val Loss: 31379.9893\n",
      "MAE: 125.35\n",
      "\n",
      "Epoch 7/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:10<00:00,  7.92it/s, Loss=29742.9531]\n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  7.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 24311.5955\n",
      "Val Loss: 35111.7456\n",
      "MAE: 128.30\n",
      "\n",
      "Epoch 8/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:11<00:00,  7.77it/s, Loss=16946.3359]\n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  7.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 22953.1951\n",
      "Val Loss: 22969.6235\n",
      "MAE: 117.02\n",
      "\n",
      "Epoch 9/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:11<00:00,  7.81it/s, Loss=9095.9141] \n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  7.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 21169.2973\n",
      "Val Loss: 37002.9525\n",
      "MAE: 129.90\n",
      "\n",
      "Epoch 10/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:11<00:00,  7.82it/s, Loss=14162.1094]\n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  7.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 20939.9783\n",
      "Val Loss: 25076.7343\n",
      "MAE: 126.04\n",
      "\n",
      "Epoch 11/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:11<00:00,  7.87it/s, Loss=34422.9297]\n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  7.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 18997.1927\n",
      "Val Loss: 25395.6118\n",
      "MAE: 110.69\n",
      "\n",
      "Epoch 12/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:11<00:00,  7.70it/s, Loss=17347.0664]\n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  7.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 19351.6856\n",
      "Val Loss: 24114.6584\n",
      "MAE: 107.19\n",
      "\n",
      "Epoch 13/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:11<00:00,  7.82it/s, Loss=16328.1729]\n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  7.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 18469.6020\n",
      "Val Loss: 24396.3007\n",
      "MAE: 114.10\n",
      "\n",
      "Epoch 14/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:10<00:00,  7.92it/s, Loss=17174.9844]\n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  7.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 19699.8777\n",
      "Val Loss: 24575.5194\n",
      "MAE: 118.80\n",
      "\n",
      "Epoch 15/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:11<00:00,  7.86it/s, Loss=14422.1514]\n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  7.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 19199.9997\n",
      "Val Loss: 24363.2964\n",
      "MAE: 115.57\n",
      "Early stopping triggered after 15 epochs\n",
      "Best epoch: 8\n",
      "\n",
      "Training completed!\n",
      "Best validation loss: 22969.6235\n",
      "\n",
      "Experiment completed! Results saved to: ./experiments/baseline_simple_cnn_20251025_224927\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Configuration for Simple CNN baseline\n",
    "def train_simple_cnn():\n",
    "    \"\"\"Train simple 2-layer CNN for calorie prediction (matching CNN.ipynb architecture)\"\"\"\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "    print(\"TRAINING: Simple 2-Layer CNN (Baseline)\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Create datasets (no augmentation for baseline)\n",
    "    train_dataset = Nutrition5KDataset(\n",
    "        csv_path=train_csv,\n",
    "        data_root=DATA_ROOT,\n",
    "        split='train',\n",
    "        augment=False,  # No augmentation for baseline\n",
    "        img_size=IMG_SIZE,\n",
    "    )\n",
    "    \n",
    "    val_dataset = Nutrition5KDataset(\n",
    "        csv_path=val_csv,\n",
    "        data_root=DATA_ROOT,\n",
    "        split='val',\n",
    "        augment=False,  # Never augment validation\n",
    "        img_size=IMG_SIZE,\n",
    "    )\n",
    "    \n",
    "    # Create data loaders\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=True,\n",
    "        num_workers=NUM_WORKERS,\n",
    "        pin_memory=True if torch.cuda.is_available() else False,\n",
    "        drop_last=True\n",
    "    )\n",
    "    \n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=False,\n",
    "        num_workers=NUM_WORKERS,\n",
    "        pin_memory=True if torch.cuda.is_available() else False\n",
    "    )\n",
    "    \n",
    "    model = build_model(\n",
    "        width1=32,\n",
    "        width2=64,\n",
    "        dropout_rate=DROPOUT_RATE\n",
    "    )\n",
    "    model = model.to(device)\n",
    "    \n",
    "    print(f\"Model parameters: {model.get_num_parameters():,}\")\n",
    "    print(f\"Training samples: {len(train_dataset)}\")\n",
    "    print(f\"Validation samples: {len(val_dataset)}\")\n",
    "    \n",
    "    # Loss function (MSE for calorie prediction)\n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    learning_rate = LEARNING_RATE\n",
    "    weight_decay = WEIGHT_DECAY\n",
    "    \n",
    "    optimizer = optim.AdamW(\n",
    "        model.parameters(),\n",
    "        lr=learning_rate,\n",
    "        weight_decay=weight_decay\n",
    "    )\n",
    "    \n",
    "    print(f\"Learning rate: {learning_rate}\")\n",
    "    \n",
    "    # Learning rate scheduler\n",
    "    steps_per_epoch = len(train_loader)\n",
    "    total_steps = NUM_EPOCHS * steps_per_epoch\n",
    "    warmup_steps = int(total_steps * WARMUP_RATIO)\n",
    "\n",
    "    # Learning rate scheduler: Warmup + Linear Decay\n",
    "    scheduler = get_warmup_cosine_scheduler(\n",
    "        optimizer, \n",
    "        warmup_steps=warmup_steps, \n",
    "        total_steps=total_steps,\n",
    "        min_lr_ratio=MIN_LR_RATIO\n",
    "    )\n",
    "    # Create experiment directory\n",
    "    exp_name = f\"baseline_simple_cnn_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "    exp_dir = os.path.join(OUTPUT_DIR, exp_name)\n",
    "    os.makedirs(exp_dir, exist_ok=True)\n",
    "    \n",
    "    # Create trainer\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        criterion=criterion,\n",
    "        optimizer=optimizer,\n",
    "        scheduler=scheduler,\n",
    "        device=device,\n",
    "        output_dir=exp_dir,\n",
    "        early_stopping_patience=EARLY_STOPPING_PATIENCE\n",
    "    )\n",
    "    \n",
    "    # Train the model\n",
    "    trainer.train(NUM_EPOCHS)\n",
    "    \n",
    "    print(f\"\\nExperiment completed! Results saved to: {exp_dir}\")\n",
    "    return trainer.best_metrics\n",
    "\n",
    "# Run the experiment\n",
    "simple_cnn_results = train_simple_cnn()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84e070e8",
   "metadata": {},
   "source": [
    "## 3.1 Baseline - Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0dcad4cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "TRAINING: Simple 2-Layer CNN (Baseline)\n",
      "============================================================\n",
      "Loaded 2804 valid samples out of 2805\n",
      "Loaded 495 valid samples out of 495\n",
      "Model parameters: 96,577\n",
      "Training samples: 2804\n",
      "Validation samples: 495\n",
      "Learning rate: 0.02\n",
      "Starting training for 40 epochs...\n",
      "\n",
      "Epoch 1/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:16<00:00,  5.31it/s, Loss=152621.5625]\n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  7.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 98987.3481\n",
      "Val Loss: 107362.6895\n",
      "MAE: 240.53\n",
      "\n",
      "Epoch 2/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:16<00:00,  5.25it/s, Loss=102144.8828]\n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  7.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 98487.1005\n",
      "Val Loss: 106996.3491\n",
      "MAE: 239.92\n",
      "\n",
      "Epoch 3/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:16<00:00,  5.25it/s, Loss=94819.3750] \n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  7.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 96606.0635\n",
      "Val Loss: 103642.0425\n",
      "MAE: 234.85\n",
      "\n",
      "Epoch 4/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:16<00:00,  5.28it/s, Loss=124572.1406]\n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  7.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 88913.4773\n",
      "Val Loss: 90574.2195\n",
      "MAE: 214.43\n",
      "\n",
      "Epoch 5/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:16<00:00,  5.12it/s, Loss=21435.7871] \n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  7.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 66065.7899\n",
      "Val Loss: 60459.3843\n",
      "MAE: 170.51\n",
      "\n",
      "Epoch 6/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:16<00:00,  5.14it/s, Loss=20445.1836]\n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  7.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 36037.4168\n",
      "Val Loss: 34607.9143\n",
      "MAE: 129.54\n",
      "\n",
      "Epoch 7/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:16<00:00,  5.26it/s, Loss=25664.3711]\n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  7.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 24656.3940\n",
      "Val Loss: 27873.2155\n",
      "MAE: 117.64\n",
      "\n",
      "Epoch 8/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:16<00:00,  5.25it/s, Loss=20396.9473]\n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  7.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 22242.6033\n",
      "Val Loss: 24023.6109\n",
      "MAE: 110.84\n",
      "\n",
      "Epoch 9/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:16<00:00,  5.25it/s, Loss=10939.0234]\n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  7.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 20808.6422\n",
      "Val Loss: 28673.0519\n",
      "MAE: 116.72\n",
      "\n",
      "Epoch 10/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:16<00:00,  5.26it/s, Loss=18166.1328]\n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  7.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 19530.7289\n",
      "Val Loss: 21901.0102\n",
      "MAE: 109.43\n",
      "\n",
      "Epoch 11/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:16<00:00,  5.24it/s, Loss=28875.2598]\n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  7.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 19255.6032\n",
      "Val Loss: 29375.0599\n",
      "MAE: 121.47\n",
      "\n",
      "Epoch 12/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:16<00:00,  5.19it/s, Loss=19532.2031]\n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  7.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 19012.9062\n",
      "Val Loss: 24327.9830\n",
      "MAE: 107.79\n",
      "\n",
      "Epoch 13/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:17<00:00,  5.10it/s, Loss=19986.4277]\n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  7.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 18139.0050\n",
      "Val Loss: 27254.1710\n",
      "MAE: 112.20\n",
      "\n",
      "Epoch 14/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:17<00:00,  5.12it/s, Loss=17413.3535]\n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  7.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 17981.1191\n",
      "Val Loss: 110377.8042\n",
      "MAE: 310.07\n",
      "\n",
      "Epoch 15/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:16<00:00,  5.24it/s, Loss=24426.4199]\n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  7.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 18253.1599\n",
      "Val Loss: 35338.6985\n",
      "MAE: 127.01\n",
      "\n",
      "Epoch 16/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:16<00:00,  5.23it/s, Loss=23556.9941]\n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  7.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 17829.3046\n",
      "Val Loss: 19898.2485\n",
      "MAE: 104.10\n",
      "\n",
      "Epoch 17/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:16<00:00,  5.27it/s, Loss=16335.4248]\n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  7.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 18050.1026\n",
      "Val Loss: 20530.7458\n",
      "MAE: 99.48\n",
      "\n",
      "Epoch 18/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:16<00:00,  5.26it/s, Loss=22480.8574]\n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  7.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 18220.5709\n",
      "Val Loss: 8734556.9062\n",
      "MAE: 2949.48\n",
      "\n",
      "Epoch 19/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:16<00:00,  5.27it/s, Loss=19522.8848]\n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  7.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 16990.2763\n",
      "Val Loss: 672721.3125\n",
      "MAE: 803.39\n",
      "\n",
      "Epoch 20/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:16<00:00,  5.23it/s, Loss=26279.8477]\n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  7.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 17453.1802\n",
      "Val Loss: 125554.6660\n",
      "MAE: 333.12\n",
      "\n",
      "Epoch 21/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:16<00:00,  5.16it/s, Loss=12423.6445]\n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  7.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 16656.5246\n",
      "Val Loss: 36625.5835\n",
      "MAE: 133.98\n",
      "\n",
      "Epoch 22/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:16<00:00,  5.24it/s, Loss=13079.5986]\n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  7.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 16675.4579\n",
      "Val Loss: 1967012.3203\n",
      "MAE: 1395.40\n",
      "\n",
      "Epoch 23/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 87/87 [00:16<00:00,  5.24it/s, Loss=12739.0791]\n",
      "Validation: 100%|██████████| 16/16 [00:02<00:00,  7.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 15551.5802\n",
      "Val Loss: 25948.6469\n",
      "MAE: 120.35\n",
      "Early stopping triggered after 23 epochs\n",
      "Best epoch: 16\n",
      "\n",
      "Training completed!\n",
      "Best validation loss: 19898.2485\n",
      "\n",
      "Experiment completed! Results saved to: ./experiments/baseline_simple_cnn_20251025_225247\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Configuration for Simple CNN baseline\n",
    "def train_simple_cnn():\n",
    "    \"\"\"Train simple 2-layer CNN for calorie prediction (matching CNN.ipynb architecture)\"\"\"\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "    print(\"TRAINING: Simple 2-Layer CNN (Baseline)\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Create datasets (no augmentation for baseline)\n",
    "    train_dataset = Nutrition5KDataset(\n",
    "        csv_path=train_csv,\n",
    "        data_root=DATA_ROOT,\n",
    "        split='train',\n",
    "        augment=True,  # No augmentation for baseline\n",
    "        img_size=IMG_SIZE,\n",
    "    )\n",
    "    \n",
    "    val_dataset = Nutrition5KDataset(\n",
    "        csv_path=val_csv,\n",
    "        data_root=DATA_ROOT,\n",
    "        split='val',\n",
    "        augment=False,  # Never augment validation\n",
    "        img_size=IMG_SIZE,\n",
    "    )\n",
    "    \n",
    "    # Create data loaders\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=True,\n",
    "        num_workers=NUM_WORKERS,\n",
    "        pin_memory=True if torch.cuda.is_available() else False,\n",
    "        drop_last=True\n",
    "    )\n",
    "    \n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=False,\n",
    "        num_workers=NUM_WORKERS,\n",
    "        pin_memory=True if torch.cuda.is_available() else False\n",
    "    )\n",
    "    \n",
    "    model = build_model(\n",
    "        width1=32,\n",
    "        width2=64,\n",
    "        dropout_rate=DROPOUT_RATE\n",
    "    )\n",
    "    model = model.to(device)\n",
    "    \n",
    "    print(f\"Model parameters: {model.get_num_parameters():,}\")\n",
    "    print(f\"Training samples: {len(train_dataset)}\")\n",
    "    print(f\"Validation samples: {len(val_dataset)}\")\n",
    "    \n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    learning_rate = LEARNING_RATE\n",
    "    weight_decay = WEIGHT_DECAY\n",
    "    \n",
    "    optimizer = optim.AdamW(\n",
    "        model.parameters(),\n",
    "        lr=learning_rate,\n",
    "        weight_decay=weight_decay\n",
    "    )\n",
    "    \n",
    "    print(f\"Learning rate: {learning_rate}\")\n",
    "    \n",
    "    # Learning rate scheduler\n",
    "    steps_per_epoch = len(train_loader)\n",
    "    total_steps = NUM_EPOCHS * steps_per_epoch\n",
    "    warmup_steps = int(total_steps * WARMUP_RATIO)\n",
    "\n",
    "    # Learning rate scheduler: Warmup + Linear Decay\n",
    "    scheduler = get_warmup_cosine_scheduler(\n",
    "        optimizer, \n",
    "        warmup_steps=warmup_steps, \n",
    "        total_steps=total_steps,\n",
    "        min_lr_ratio=MIN_LR_RATIO\n",
    "    )\n",
    "    # Create experiment directory\n",
    "    exp_name = f\"baseline_simple_cnn_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "    exp_dir = os.path.join(OUTPUT_DIR, exp_name)\n",
    "    os.makedirs(exp_dir, exist_ok=True)\n",
    "    \n",
    "    # Create trainer\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        criterion=criterion,\n",
    "        optimizer=optimizer,\n",
    "        scheduler=scheduler,\n",
    "        device=device,\n",
    "        output_dir=exp_dir,\n",
    "        early_stopping_patience=EARLY_STOPPING_PATIENCE\n",
    "    )\n",
    "    \n",
    "    # Train the model\n",
    "    trainer.train(NUM_EPOCHS)\n",
    "    \n",
    "    print(f\"\\nExperiment completed! Results saved to: {exp_dir}\")\n",
    "    return trainer.best_metrics\n",
    "\n",
    "# Run the experiment\n",
    "simple_cnn_results = train_simple_cnn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0966a39c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "compvis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
